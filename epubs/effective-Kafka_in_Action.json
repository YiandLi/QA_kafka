{
  "Part 1  Getting started": [
    "Dylan Scott",
    "Viktor Gamov Dave Klein",
    "Foreword by Jun Rao"
  ],
  "Part 1  Getting started<sec><sec><sec>What is Kafka?": [
    "n part 1 of this book, we\u2019ll look at introducing you to Apache Kafka and start to look at real use cases where Kafka might be a good fit to try out:",
    "In chapter 1, we give a detailed description of why you would want to use Kafka, and we dispel some myths you might have heard about Kafka in relation to Hadoop.",
    "In chapter 2, we focus on learning about the high-level architecture of Kafka as well as the various other parts that make up the Kafka ecosystem: Kafka Streams, Connect, and ksqlDB.",
    "When you\u2019re finished with this part, you\u2019ll be ready to get started reading and writing messages to and from Kafka. Hopefully, you\u2019ll have picked up some key terminology as well.",
    "As many developers are facing a world full of data produced from every angle, they are often presented with the fact that legacy systems might not be the best option moving forward. One of the foundational pieces of new data infrastructures that has taken over the IT landscape is Apache Kafka\u00ae.1 Kafka is changing the standards for data platforms. It is leading the way to move from extract, transform, load (ETL) and batch workflows (in which work was often held and processed in bulk at one predefined time) to near-real-time data feeds [1]. Batch processing, which was once the standard workhorse of enterprise data processing, might not be some- thing to turn back to after seeing the powerful feature set that Kafka provides. In",
    "1 Apache, Apache Kafka, and Kafka are trademarks of the Apache Software Foundation.",
    "fact, you might not be able to handle the growing snowball of data rolling toward enterprises of all sizes unless something new is approached.",
    "With so much data, systems can get easily overloaded. Legacy systems might be faced with nightly processing windows that run into the next day. To keep up with this ever constant stream of data or evolving data, processing this information as it hap- pens is a way to stay up to date and current on the system\u2019s state.",
    "Kafka touches many of the newest and the most practical trends in today\u2019s IT fields and makes its easier for daily work. For example, Kafka has already made its way into microservice designs and the Internet of Things (IoT). As a de facto technology for more and more companies, Kafka is not only for super geeks or alpha-chasers. Let\u2019s start by looking at Kafka\u2019s features, introducing Kafka itself, and understanding more about the face of modern-day streaming platforms."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka usage": [
    "The Apache Kafka site (http://kafka.apache.org/intro) defines Kafka as a distributed streaming platform. It has three main capabilities:",
    "Reading and writing records like a message queue",
    "Storing records with fault tolerance",
    "Processing streams as they occur [2]",
    "Readers who are not as familiar with queues or message brokers in their daily work might need help when discussing the general purpose and flow of such a system. As a generalization, a core piece of Kafka can be thought of as providing the IT equivalent of a receiver that sits in a home entertainment system. Figure 1.1 shows the data flow between receivers and end users.",
    "As figure 1.1 shows, digital satellite, cable, and Blu-ray\u2122 players can connect to a central receiver. You can think of those individual pieces as regularly sending data in a format that they know about. That flow of data can be thought of as nearly constant while a movie or CD is playing. The receiver deals with this constant stream of data and converts it into a usable format for the external devices attached to the other end (the receiver sends the video to your television and the audio to a decoder as well as to the speakers). So what does this have to do with Kafka exactly? Let\u2019s look at the same relationship from Kafka\u2019s perspective in figure 1.2.",
    "Kafka includes clients to interface with other systems. One such client type is called a producer, which sends multiple data streams to the Kafka brokers. The brokers serve a similar function as the receiver in figure 1.1. Kafka also includes consumers, clients that can read data from the brokers and process it. Data does not have to be limited to only a single destination. The producers and consumers are completely decoupled, allow- ing each client to work independently. We\u2019ll dig into the details of how this is done in later chapters.",
    "As do other messaging platforms, Kafka acts (in reductionist terms) like a middle- man for data coming into the system (from producers) and out of the system (for con- sumers or end users). The loose coupling can be achieved by allowing this separation",
    "Producers and sources of data",
    "End consumers of the data",
    "Controls and handles data",
    "Figure 1.1 Producers, receivers, and data flow overview",
    "The source of data like the satellite, DVD, or Blu-ray player are called producers in Kafka.",
    "The receiver is where Kafka manages the message data and allows producers and consumers to act in a decoupled manner.",
    "The TV and stereos that use the data are called consumers in Kafka.",
    "Figure 1.2  Kafka\u2019s flow from producers to consumers",
    "between the producer and the end user of the message. The producer can send what- ever message it wants and still have no clue about if anyone is subscribed. Further, Kafka has various ways that it can deliver messages to fit your business case. Kafka\u2019s message delivery can take at least the following three delivery methods [3]:",
    "At-least-once semantics\u2014A message is sent as needed until it is acknowledged.",
    "At-most-once semantics \u2014A message is only sent once and not resent on failure.",
    "Exactly-once semantics\u2014A message is only seen once by the consumer of the message.",
    "Let\u2019s dig into what those messaging options mean. Let\u2019s look at at-least-once semantics (figure 1.3). In this case, Kafka can be configured to allow a producer of messages to send the same message more than once and have it written to the brokers. If a mes- sage does not receive a guarantee that it was written to the broker, the producer can resend the message [3]. For those cases where you can\u2019t miss a message, say that some- one has paid an invoice, this guarantee might take some filtering on the consumer end, but it is one of the safest delivery methods.",
    "The broker sees two messages at least once (or only one if there is a failure).",
    "If a message from a producer has a failure or is not acknowledged, the producer resends the message.",
    "Consumers get as many messages as the broker receives. Consumers might see duplicate messages.",
    "Figure 1.3  At-least-once message flow",
    "At-most-once semantics (figure 1.4) is when a producer of messages might send a mes- sage once and never retry. In the event of a failure, the producer moves on and doesn\u2019t attempt to send it again [3]. Why would someone be okay with losing a mes- sage? If a popular website is tracking page views for visitors, it might be okay with miss- ing a few page view events out of the millions it processes each day. Keeping the system performing and not waiting on acknowledgments might outweigh any cost of lost data.",
    "Kafka added the exactly-once semantics, also known as EOS, to its feature set in ver- sion 0.11.0. EOS generated a lot of mixed discussion with its release [3]. On the one hand, exactly-once semantics (figure 1.5) are ideal for a lot of use cases. This seemed like a logical guarantee for removing duplicate messages, making them a thing of the past. But most developers appreciate sending one message and receiving that same message on the consuming side as well.",
    "The broker sees one message at most (or zero if there is a failure).",
    "If a message from a producer",
    "has a failure or is not acknowledged, the producer does not resend",
    "the message.",
    "Consumers see the messages that the broker receives. If there is a failure, the consumer never sees that message.",
    "Figure 1.4  At-most-once message flow",
    "Another discussion that followed the release of EOS was a debate on if exactly once was even possible. Although this goes into deeper computer science theory, it is help- ful to be aware of how Kafka defines their EOS feature [4]. If a producer sends a mes- sage more than once, it will still be delivered only once to the end consumer. EOS has touchpoints at all Kafka layers\u2014producers, topics, brokers, and consumers\u2014and will be briefly tackled later in this book as we move along in our discussion for now.",
    "Besides various delivery options, another common message broker benefit is that if the consuming application is down due to errors or maintenance, the producer does",
    "The broker only allows one message.",
    "If a message from a producer",
    "fails or is not acknowledged, the producer resends the message.",
    "Consumers only",
    "see the message once.",
    "Figure 1.5  Exactly-once message flow",
    "not need to wait on the consumer to handle the message. When consumers start to come back online and process data, they should be able to pick up where they left off and not drop any messages."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka usage<sec><sec>Kafka for the developer": [
    "With many traditional companies facing the challenges of becoming more and more technical and software driven, one question is foremost: how will they be prepared for the future? One possible answer is Kafka. Kafka is noted for being a high-performance, message-delivery workhorse that features replication and fault tolerance as a default.",
    "With Kafka, enormous data processing needs are handled with Kafka in produc- tion [5]. All this with a tool that was not at its 1.0 version release until 2017! However, besides these headline-grabbing facts, why would users want to start looking at Kafka? Let\u2019s look at that answer next."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka usage<sec><sec>Explaining Kafka to your manager": [
    "Why would a software developer be interested in Kafka? Kafka usage is exploding, and the developer demand isn\u2019t being met [6]. A shift in our traditional data processing way of thinking is needed. Various shared experiences or past pain points can help devel- opers see why Kafka could be an appealing step forward in their data architectures.",
    "One of the various on-ramps for newer developers to Kafka is to apply things they know to help them with the unknown. For example, Java\u00ae developers can use Spring\u00ae concepts, and Dependency Injection (DI) Spring for Kafka (https://projects.spring.io/ spring-kafka) has already been through a couple of major release versions. Supporting projects, as well as Kafka itself, have a growing tool ecosystem all their own.",
    "As a common developer, most programmers have likely confronted the challenges of coupling. For example, you want to make a change to one application, but you might have many other applications directly tied to it. Or, you start to unit test and see a large number of mocks you have to create. Kafka, when applied thoughtfully, can help in these situations.",
    "Take, for example, an HR system that employees would use to submit paid vacation leaves. If you are used to a create, read, update, and delete (CRUD) system, the submis- sion of time off would likely be processed by not only payroll but also project burndown charts for forecasting work. Do you tie the two applications together? What if the pay- roll system goes down? Should that impact the availability of the forecasting tooling?",
    "With Kafka, we will see the benefits of being able to decouple some of the applica- tions that we have tied together in older designs. (We will look more in-depth at maturing our data model in chapter 11.) Kafka, however, can be put into the middle of the workflow [7]. Your interface to data becomes Kafka instead of numerous APIs and databases.",
    "Some say that there are better and simpler solutions. What about using ETL to at least load the data into databases for each application? That would only be one inter- face per application and easy, right? But what if the initial source of data is corrupted",
    "or outdated? How often do you look for updates and allow for lag or consistency? And do those copies ever get out of date or diverge so far from the source that it would be hard to run that flow again and get the same results? What is the source of truth? Kafka can help avoid these issues.",
    "Another interesting topic that might add credibility to the use of Kafka is how much it \u201cdogfoods\u201d itself. For example, when we dig into consumers in chapter 5, we will see how Kafka uses topics internally to manage consumers\u2019 offsets. After the release of v0.11, exactly-once semantics for Kafka also uses internal topics. The ability to have many data consumers using the same message allows many possible outcomes. Another developer question might be, why not learn Kafka Streams, ksqlDB, Apache Spark\u2122 Streaming, or other platforms and skip learning about core Kafka? The number of applications that use Kafka internally is indeed impressive. Although abstraction layers are often nice to have (and sometimes close to being required with",
    "so many moving parts), we believe that Kafka itself is worth learning.",
    "There is a difference in knowing that Kafka is a channel option for Apache Flume\u2122 and understanding what all of the config options mean. Although Kafka Streams can simplify examples you might see in this book, it is interesting to note how successful Kafka was before Kafka Streams was even introduced. Kafka\u2019s base is fundamental and will, hopefully, help you see why it is used in some applications and what happens internally. If you want to become an expert in streaming, it is import- ant to know the underlying distributed parts of your applications and all the knobs you can turn to fine-tune your applications. From a purely technical viewpoint, there are exciting computer science topics applied in practical ways. Perhaps the most talked about is the notion of distributed commit logs, which we will discuss in depth in chapter 2, and a personal favorite, hierarchical timing wheels [8]. These examples show you how Kafka handles an issue of scale by applying an interesting data struc- ture to solve a practical problem.",
    "We would also note that the fact that it\u2019s open source is a positive for digging into the source code and having documentation and examples just by searching the inter- net. Resources are not just limited to internal knowledge based solely on a specific workplace."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka myths": [
    "As is often the case, sometimes members of the C-suite will hear the word Kafka and be more confused by the name than care about what it does. It might be nice to explain the value found in this product. Also, it is good to step back and look at the larger picture of what the real added value is for this tool.",
    "One of Kafka\u2019s most important features is the ability to take volumes of data and make it available for use by various business units. Such a data backbone that makes information coming into the enterprise available to all the multiple business areas allows for flexibility and openness on a company-wide scale. Nothing is prescribed, but increased access to data is a potential outcome. Most executives also know that",
    "with more data than ever flooding in, the company wants insights as fast as possible. Rather than pay for data to get old on disk, its value can be derived as it arrives. Kafka is one way to move away from a daily batch job that limits how quickly that data can be turned into value. Fast data seems to be the newer term, hinting that real value focuses on something different from the promises of big data alone.",
    "Running on a Java virtual machine JVM\u00ae should be a familiar and comfortable place for many enterprise development shops. The ability to run on premises is a cru- cial driver for some whose data requires on-site oversight. And the cloud and man- aged platforms are options as well. Kafka can scale horizontally, and not depend on vertical scaling alone, which might eventually reach an expensive peak.",
    "Maybe one of the most important reasons to learn about Kafka is to see how start- ups and others in their industry can overcome the once prohibitive cost of computing power. Instead of relying on a bigger and beefier server or a mainframe that can cost millions of dollars, distributed applications and architectures put competitors quickly within reach with, hopefully, less financial outlay."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka myths<sec><sec>Kafka only works with Hadoop\u00ae": [
    "When you start to learn any new technology, it is often natural to try to map existing knowledge to new concepts. Although that technique can be used in learning Kafka, we wanted to note some of the most common misconceptions that we have run into in our work so far. We\u2019ll cover those in the next sections."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka myths<sec><sec>Kafka is the same as other message brokers": [
    "As mentioned, Kafka is a powerful tool that is often used in various situations. How- ever, it seemed to appear on radars when used in the Hadoop ecosystem and might have first appeared for users as a tool as part of a Cloudera\u2122 or Hortonworks\u2122 suite. It isn\u2019t uncommon to hear the myth that Kafka only works with Hadoop. What could cause this confusion? One of the causes is likely the various tools that use Kafka as part of their products. Spark Streaming and Flume are examples of tools that use Kafka (or did at one point) and could be used with Hadoop as well. The dependency (depend- ing on the version of Kafka) on Apache ZooKeeper\u2122 is also a tool that is often found in Hadoop clusters and might tie Kafka further to this myth.",
    "One other fundamental myth that often appears is that Kafka requires the Hadoop Distributed Filesystem (HDFS). That is not the case. Once we start to dig into how Kafka works, we will see that Kafka\u2019s speed and techniques used to process events would likely be much slower with a NodeManager in the middle of the process. Also, the block replication, usually a part of HDFS, is not done in the same way. One such example is that in Kafka, replicas are not recovered by default. Whereas both products use replication in different ways, the durability that is marketed for Kafka might be easy to group under the Hadoop theme of expecting failure as a default (and thus planning for overcoming it) and is a similar overall goal between Hadoop and Kafka."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka in the real world": [
    "Another big myth is that Kafka is just another message broker. Direct comparisons of the features of various tools (such as Pivotal\u2019s RabbitMQ\u2122 or IBM\u2019s MQSeries\u00ae) to Kafka often have asterisks (or fine print) attached and are not always fair to the best use cases of each. Some tools over time have gained or will gain new features just as Kafka has added the exactly-once semantics. And default configurations can be changed to mirror features closer to other tools in the same space. In general, the following lists some of the most exciting and standout features that we will dig into in a bit:",
    "The ability to replay messages by default",
    "Parallel processing of data",
    "Kafka was designed to have multiple consumers. What that means is that one applica- tion reading a message from the message broker doesn\u2019t remove it from other applica- tions that might want to consume it as well. One effect of this is that a consumer who has already seen the message can again choose to read it (and other messages as well). With some architecture models like lambda (discussed in chapter 8), programmer mis- takes are expected just as much as hardware failures. Imagine consuming millions of messages, and you forget to use a specific field from the original message. In some queues, that message is removed or sent to a duplicate or replay location. However, Kafka provides a way for consumers to seek specific points and read messages again (with a few constraints) by just seeking an earlier position on the topic.",
    "As touched on briefly, Kafka allows for parallel processing of data and can have multiple consumers on the same topic. Kafka also has the concept of consumers being part of a consumer group, which will be covered in depth in chapter 5. Membership in a group determines which consumers get which messages and what work has been done across that group of consumers. Consumer groups act independently of any other group and allow for multiple applications to consume messages at their own pace with as many consumers as they require working together. Processing can hap- pen in various ways: consumption by many consumers working on one application or consumption by many applications. No matter what other message brokers support, let\u2019s now focus on the robust use cases that have made Kafka one of the options devel- opers turn to for getting work done."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka in the real world<sec><sec>Early examples": [
    "Applying Kafka to practical use is the core aim of this book. One of the things to note about Kafka is that it\u2019s hard to say it does one specific function well; it excels in many specific uses. Although we have some basic ideas to grasp first, it might be helpful to discuss at a high level some of the cases that Kafka has already been noted for in real- world use cases. The Apache Kafka site lists general areas where Kafka is used in the real world that we explore in the book. [9]."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka in the real world<sec><sec>Later examples": [
    "Some users\u2019 first experience with Kafka (as was mine) was using it as a messaging tool. Personally, after years of using other tools like IBM\u00ae WebSphere\u00ae MQ (formerly MQ Series), Kafka (which was around version 0.8.3 at the time) seemed simple to use to get messages from point A to point B. Kafka forgoes using popular protocols and stan- dards\u2014like the Extensible Messaging and Presence Protocol (XMPP), Java Message Service (JMS) API (now part of Jakarta EE), or the OASIS\u00ae Advanced Message Queu- ing Protocol (AMQP)\u2014in favor of a custom TCP binary protocol. We will dig in and see some complex uses later.",
    "For an end user developing with a Kafka client, most of the details are in the con- figuration, and the logic becomes relatively straightforward (for example, \u201cI want to place a message on this topic\u201d). Having a durable channel for sending messages is also why Kafka is used.",
    "Oftentimes, memory storage of data in RAM will not be enough to protect your data; if that server dies, the messages are not persisted across a reboot. High availabil- ity and persistent storage are built into Kafka from the start. Apache Flume provides a Kafka channel option because the replication and availability allow Flume events to be made immediately available to other sinks if a Flume agent (or the server it is running on) crashes [10]. Kafka enables robust applications to be built and helps handle the expected failures that distributed applications are bound to run into at some point.",
    "Log aggregation (figure 1.6) is useful in many situations, including when trying to gather application events that were written in distributed applications. In the figure,",
    "Kafka acts as a logical central point for all of the server logs and stores that information on the brokers.",
    "Various server logs are gathered into Kafka.",
    "Kafka serves the aggregate view",
    "to each application (assuming each application is part of its own group).",
    "Figure 1.6  Kafka log aggregation",
    "the log files are sent as messages into Kafka, and then different applications have a sin- gle logical topic to consume that information. With Kafka\u2019s ability to handle large amounts of data, collecting events from various servers or sources is a key feature. Depending on the contents of the log event itself, some organizations use it for audit- ing and failure-detection trending. Kafka is also used in various logging tools (or as an input option).",
    "How do all of these log file entries allow Kafka to maintain performance without causing a server to run out of resources? The throughput of small messages can some- times overwhelm a system because the processing of each method takes time and over- head. Kafka uses batching of messages for sending data as well as for writing data. Writing to the end of a log helps too, rather than random access to the filesystem. We will discuss more on the log format of messages in chapters 7."
  ],
  "Part 1  Getting started<sec><sec><sec>Kafka in the real world<sec><sec>When Kafka might not be the right fit": [
    "Microservices used to talk to each other with APIs like REST, but they can now lever- age Kafka to communicate between asynchronous services with events [11]. Microser- vices can use Kafka as the interface for their interactions rather than specific API calls. Kafka has placed itself as a fundamental piece for allowing developers to get data quickly. Although Kafka Streams is now a likely default for many when starting work, Kafka had already established itself as a successful solution by the time the Streams API was released in 2016. The Streams API can be thought of as a layer that sits on top of producers and consumers. This abstraction layer is a client library that provides a higher-level view of working with your data as an unbounded stream.",
    "In the Kafka 0.11 release, exactly-once semantics was introduced. We will cover what that means in practice later, once we get a more solid foundation. However, users running end-to-end workloads on top of Kafka with the Streams API may bene- fit from hardened delivery guarantees. Streams make this use case easier than it has ever been to complete a flow without the overhead of any custom application logic, ensuring that a message was only processed once from the beginning to the end of the transaction.",
    "The number of devices for the Internet of Things (figure 1.7) will only increase with time. With all of those devices sending messages, sometimes in bursts when they get a Wi-Fi or cellular connection, something needs to be able to handle that data effectively. As you may have gathered, massive quantities of data are one of the critical areas where Kafka shines. As we discussed previously, small messages are not a prob- lem for Kafka. Beacons, cars, phones, homes, etc.\u2014all will be sending data, and some- thing needs to handle the fire hose of data and make it available for action [12].",
    "This are just a small selection of examples that are well-known uses for Kafka. As we will see in future chapters, Kafka has many practical application domains. Learning the upcoming foundational concepts is essential to see how even more practical appli- cations are possible.",
    "Water meter events",
    "Internet of Things",
    "Kafka brokers",
    "Door alarm sensor",
    "Temperature gauge reading",
    "When sensors or beacons get a Wi-Fi or cellular signal, they\u2019ll report their events, which might be a lot!",
    "Your Kafka cluster is designed to work well with lots of data and small messages.",
    "Figure 1.7  The Internet of Things (IoT)"
  ],
  "Part 1  Getting started<sec><sec><sec>Online resources to get started": [
    "It is important to note that although Kafka has been used in some interesting use cases, it is not always the best tool for the job at hand. Let\u2019s investigate some of the uses where other tools or code might shine.",
    "What if you only need a once-monthly or even once-yearly summary of aggregate data? Suppose you don\u2019t need an on-demand view, quick answer, or even the ability to reprocess data. In these cases, you might not need Kafka running throughout the entire year for those tasks alone (notably, if that amount of data is manageable to pro- cess at once as a batch). As always, your mileage may vary: different users have differ- ent thresholds on what is a large batch.",
    "If your main access pattern for data is a mostly random lookup of data, Kafka might not be your best option. Linear read and writes are where Kafka shines and will keep your data moving as quickly as possible. Even if you have heard of Kafka having index files, they are not really what you would compare to a relational database having fields and primary keys from which indexes are built.",
    "Similarly, if you need the exact ordering of messages in Kafka for the entire topic, you will have to look at how practical your workload is in that situation. To avoid any unordered messages, care should be taken to ensure that only one producer request thread is the maximum and, simultaneously, that there is only one partition in the topic. There are various workarounds, but if you have vast amounts of data that depend on strict ordering, there are potential gotchas that might come into play once you notice that your consumption is limited to one consumer per group at a time.",
    "One of the other practical items that come to mind is that large messages are an exciting challenge. The default message size is about 1 MB [13]. With larger messages, you start to see memory pressure increase. In other words, the lower number of mes- sages you can store in page cache could become a concern. If you are planning on sending huge archives around, you might want to see if there is a better way to man- age those messages. Keep in mind that although you can probably achieve your end goal with Kafka in the previous situations (it\u2019s always possible), it might not be the first choice to reach for in your toolbox."
  ],
  "Part 1  Getting started<sec><sec><sec>Summary": [
    "One of the most exciting and maybe most discussed features in Kafka is its exactly- once semantics. This book will not discuss the theory behind those views; however, we will touch on what these semantics mean for Kafka\u2019s everyday usage.",
    "One important thing to note is that the easiest way to maintain exactly-once is to stay within Kafka\u2019s walls (and topics). Having a closed system that can be completed as a transaction is why using the Streams API is one of the easiest paths to exactly-once. Various Kafka Connect connectors also support exactly-once and are great examples of bringing data out of Kafka because it won\u2019t always be the final endpoint for all data in every scenario."
  ],
  "Part 1  Getting started<sec><sec><sec>References": [
    "Messages represent your data in Kafka. Kafka\u2019s cluster of brokers handles this data and interacts with outside systems and clients.",
    "Kafka\u2019s use of a commit log helps in understanding the system overall.",
    "Messages appended to the end of a log frame how data is stored and how it can be used again. By being able to start at the beginning of the log, applications can reprocess data in a specific order to fulfill different use cases.",
    "Producers are clients that help move data into the Kafka ecosystem. Populating existing information from other data sources like databases into Kafka can help expose data that was once siloed in systems that provided a data interface for other applications.",
    "Consumer clients retrieve messages from Kafka. Many consumers can read the same data at the same time. The ability for separate consumers to start reading at various positions also shows the flexibility of consumption possible from Kafka topics.",
    "Continuously flowing data between destinations with Kafka can help us rede- sign systems that used to be limited to batch or time-delayed workflows."
  ],
  "Part 1  Getting started<sec><sec><sec>Producing and consuming a message": [
    "1  R. Moffatt. \u201cThe Changing Face of ETL.\u201d Confluent blog (September 17, 2018). https://www.confluent.io/blog/changing-face-etl/ (accessed May 10, 2019).",
    "2 \u201cIntroduction.\u201d Apache Software Foundation (n.d.). https://kafka.apache.org/ intro (accessed May 30, 2019).",
    "3 Documentation. Apache Software Foundation (n.d.). https://kafka.apache",
    ".org/documentation/#semantics (accessed May 30, 2020).",
    "4 N. Narkhede. \u201cExactly-once Semantics Are Possible: Here\u2019s How Apache Kafka Does It.\u201d Confluent blog (June 30, 2017). https://www.confluent.io/blog/ exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it (accessed December 27, 2017).",
    "5 N. Narkhede. \u201cApache Kafka Hits 1.1 Trillion Messages Per Day \u2013 Joins the 4 Comma Club.\u201d Confluent blog (September 1, 2015). https://www.confluent",
    ".io/blog/apache-kafka-hits-1-1-trillion-messages-per-day-joins-the-4-comma-club/ (accessed October 20, 2019).",
    "6 L. Dauber. \u201cThe 2017 Apache Kafka Survey: Streaming Data on the Rise.\u201d Con- fluent blog (May 4, 2017). https://www.confluent.io/blog/2017-apache-kafka",
    "-survey-streaming-data-on-the-rise/ (accessed December 23, 2017).",
    "7 K. Waehner. \u201cHow to Build and Deploy Scalable Machine Learning in Produc- tion with Apache Kafka.\u201d Confluent blog (September 29, 2017) https:// www.confluent.io/blog/build-deploy-scalable-machine-learning-production- apache-kafka/ (accessed December 11, 2018).",
    "8 Y. Matsuda. \u201cApache Kafka, Purgatory, and Hierarchical Timing Wheels.\u201d Con- fluent blog (October 28, 2015). https://www.confluent.io/blog/apache-kafka",
    "-purgatory-hierarchical-timing-wheels (accessed December 20, 2018).",
    "9 \u201cUse cases.\u201d Apache Software Foundation (n.d.). https://kafka.apache.org/ uses (accessed May 30, 2017).",
    "10 \u201cFlume 1.9.0 User Guide.\u201d Apache Software Foundation (n.d.). https:// flume.apache.org/FlumeUserGuide.html (accessed May 27, 2017).",
    "11 B. Stopford. \u201cBuilding a Microservices Ecosystem with Kafka Streams and KSQL.\u201d Confluent blog (November 9, 2017). https://www.confluent.io/blog/ building-a-microservices-ecosystem-with-kafka-streams-and-ksql/ (accessed May 1, 2020).",
    "12 \u201cReal-Time IoT Data Solution with Confluent.\u201d Confluent documentation. (n.d.). https://www.confluent.io/use-case/internet-of-things-iot/ (accessed May 1, 2020).",
    "13 Documentation. Apache Software Foundation (n.d.). https://kafka.apache.org/ documentation/#brokerconfigs_message.max.bytes (accessed May 30, 2020).",
    "Now that we have a high-level view of where Kafka shines and why one would use it, let\u2019s dive into the Kafka components that make up the whole system. Apache Kafka is a distributed system at heart, but it is also possible to install and run it on a single host. That gives us a starting point to dive into our sample use cases. As is often the case, the real questions start flowing once the hands hit the keyboard. By the end of this chapter, you will be able to send and retrieve your first Kafka message from the command line. Let\u2019s get started with Kafka and then spend a little more time dig- ging into Kafka\u2019s architectural details.",
    "NOTE  Visit appendix A if you do not have a Kafka cluster to use or are inter- ested in starting one locally on your machine. Appendix A works on updating",
    "the default configuration of Apache Kafka and on starting the three brokers we will use in our examples. Confirm that your instances are up and running before attempting any examples in this book! If any examples don\u2019t seem to work, please check the source code on GitHub for tips, errata, and suggestions."
  ],
  "Part 1  Getting started<sec><sec><sec>What are brokers?": [
    "A message, also called a record, is the basic piece of data flowing through Kafka. Mes- sages are how Kafka represents your data. Each message has a timestamp, a value, and an optional key. Custom headers can be used if desired as well [1]. A simple example of a message could be something like the following: the machine with host ID \u201c1234567\u201d (a message key) failed with the message \u201cAlert: Machine Failed\u201d (a message value) at \u201c2020-10-02T10:34:11.654Z\u201d (a message timestamp). Chapter 9 shows an exam- ple of using a custom header to set a key-value pair for a tracing use case.",
    "Figure 2.1 shows probably the most important and common parts of a message that users deal with directly. Keys and values will be the focus of most of our discussion in this chapter, which require analysis when designing our messages. Each key and value can interact in its own specific ways to serialize or deserialize its data. The details of how to use serialization will start to come into focus when covering producing mes- sages in chapter 4.",
    "Not required",
    "Where the content of your data goes",
    "Figure 2.1 Kafka messages are made up of a key and a value (timestamp and optional headers are not shown).",
    "Now that we have a record, how do we let Kafka know about it? You will deliver this message to Kafka by sending it to what are known as brokers."
  ],
  "Part 1  Getting started<sec><sec><sec>Tour of Kafka": [
    "Brokers can be thought of as the server side of Kafka [1]. Before virtual machines and Kubernetes\u00ae, you may have seen one physical server hosting one broker. Because almost all clusters have more than one server (or node), we will have three Kafka serv- ers running for most of our examples. This local test setup should let us see the out- put of commands against more than one broker, which will be similar to running with multiple brokers across different machines.",
    "For our first example, we will create a topic and send our first message to Kafka from the command line. One thing to note is that Kafka was built with the command",
    "line in mind. There is no GUI that we will use, so we need to have a way to interact with the operating system\u2019s command line interface. The commands are entered into a text-based prompt. Whether you use vi, Emacs, Nano, or whatever, make sure that it is something you feel comfortable editing with.",
    "NOTE Although Kafka can be used on many operating systems, it is often deployed in production on Linux, and command line skills will be helpful when using this product.",
    "To send our first message, we will need a place to send it. To create a topic, we will run the kafka-topics.sh command in a shell window with the --create option (listing 2.1). You will find this script in the installation directory of Kafka, where the path might look like this: ~/kafka_2.13-2.7.1/bin. Note that Windows users can use the .bat files with the same name as the shell equivalent. For example, kafka-topics.sh has the Windows equivalent script named kafka-topics.bat, which should be located in the <kafka_install_directory>/bin/windows directory.",
    "NOTE The references in this work to kinaction and ka (like used in kaProperties) are meant to represent different abbreviations of Kafka in Action and are not associated with any product or company.",
    "Listing 2.1  Creating the kinaction_helloworld topic",
    "bin/kafka-topics.sh --create --bootstrap-server localhost:9094",
    "--topic kinaction_helloworld --partitions 3 --replication-factor 3",
    "You should see the output on the console where you just ran the command: Created topic kinaction_helloworld. In listing 2.1, the name kinaction_helloworld is used for our topic. We could have used any name, of course, but a popular option is to fol- low general Unix/Linux naming conventions, including not using spaces. We can avoid many frustrating errors and warnings by not including spaces or various special characters. These do not always play nicely with command line interfaces and autocompletion.",
    "There are a couple of other options whose meaning may not be clear just yet, but to keep moving forward with our exploration, we will quickly define them. These top- ics will be covered in greater detail in chapter 6.",
    "The --partitions option determines how many parts we want the topic to be split into. For example, because we have three brokers, using three partitions gives us one",
    "partition per broker. For our test workloads, we might not need this many, based on data needs alone. However, creating more than one partition at this stage lets us see how the system works in spreading data across partitions. The --replication-factor also is set to three in this example. In essence, this says that for each partition, we want to have three replicas. These copies are a crucial part of our design to improve reli- ability and fault tolerance. The --bootstrap-server option points to our local Kafka broker. This is why the broker should be running before invoking this script. For our work right now, the most important goal is to get a picture of the layout. We will dig into how to best estimate the numbers we need in other use cases when we get into the broker details later.",
    "We can also look at all existing topics that have been created and make sure that our new one is on the list. The --list option is what we can reach for to achieve this output. Again, we run the next listing in the terminal window.",
    "Listing 2.2  Verifying the topic",
    "bin/kafka-topics.sh --list --bootstrap-server localhost:9094",
    "To get a feel for how our new topic looks, listing 2.3 shows another command that we can run to give us a little more insight into our cluster. Note that our topic is not like a traditional single topic in other messaging systems: we have replicas and partitions. The numbers we see next to the labels for the Leader, Replicas, and Isr fields are the broker.ids that correspond to the value for our three brokers that we set in our con- figuration files. Briefly looking at the output, we can see that our topic consists of three partitions: Partition 0, Partition 1, and Partition 2. Each partition was repli- cated three times as we intended on topic creation.",
    "Listing 2.3  Describing the topic kinaction_helloworld",
    "--describe lets us look",
    "bin/kafka-topics.sh --bootstrap-server localhost:9094 \\",
    "--describe --topic kinaction_helloworld",
    "at the details of the topic we pass in.",
    "Topic:kinaction_helloworld PartitionCount:3 ReplicationFactor:3 Configs: Topic: kinaction_helloworld Partition: 0 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2",
    "Topic: kinaction_helloworld Partition: 1 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0",
    "Topic: kinaction_helloworld Partition: 2 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1",
    "The output from listing 2.3 shows in the first line a quick data view of the total count of partitions and replicas that this topic has. The following lines show each partition for the topic. The second line of output is specific to the partition labeled 0 and so forth. Let\u2019s zoom in on partition 0, which has its replica copy leader on broker 0. This partition also has replicas that exist on brokers 1 and 2. The last column, Isr, stands for in-sync replicas. In-sync replicas show which brokers are current and not lagging behind the leader. Having a partition replica copy that is out of date or behind the leader is an issue that we will cover later. Still, it is critical to remember that replica",
    "Broker 0 only reads and writes for partition 0. The rest of the replicas get their copies from other brokers.",
    "Figure 2.2  View of one broker",
    "Topic kinaction_helloworld is actually made up of the leaders of each partition. In our case, that involves each broker holding a partition leader.",
    "health in a distributed system is something that we will want to keep an eye on. Figure",
    "shows a view if we look at the one broker with ID 0.",
    "For our kinaction_helloworld topic, note how broker 0 holds the leader replica for partition 0. It also holds replica copies for partitions 1 and 2 for which it is not the leader replica. In the case of its copy of partition 1, the data for this replica will be cop- ied from broker 1.",
    "NOTE When we reference a partition leader in the image, we are referring to a replica leader. It is important to know that a partition can consist of one or more replicas, but only one replica will be a leader. A leader\u2019s role involves being updated by external clients, whereas nonleaders take updates only from their leader.",
    "Now once we have created our topic and verified that it exists, we can start sending real messages! Those who have worked with Kafka before might ask why we took the preceding step to create the topic before sending a message. There is a configuration to enable or disable the autocreation of topics. However, it is usually best to control the creation of topics as a specific action because we do not want new topics to ran- domly show up if someone mistypes a topic name once or twice or to be recreated due to producer retries.",
    "To send a message, we will start a terminal tab or window to run a producer as a con- sole application to take user input [2]. The command in listing 2.4 starts an interactive program that takes over the shell; you won\u2019t get your prompt back to type more com- mands until you press Ctrl-C to quit the running application. You can just start typing, maybe something as simple as the default programmer\u2019s first print statement with a pre- fix of kinaction (for Kafka In Action) as the following listing demonstrates. We use kinaction_helloworld in the vein of the \u201chello, world\u201d example found in the book, The C Programming Language [3].",
    "Listing 2.4  Kafka producer console command",
    "bin/kafka-console-producer.sh --bootstrap-server localhost:9094 \\",
    "--topic kinaction_helloworld",
    "Notice in listing 2.4 that we reference the topic that we want to interact with using a bootstrap-server parameter. This parameter can be just one (or a list) of the cur- rent brokers in our cluster. By supplying this information, the cluster can obtain the metadata it needs to work with the topic.",
    "Now, we will start a new terminal tab or window to run a consumer that also runs as a console application. The command in listing 2.5 starts a program that takes over the shell as well [2]. On this end, we should see the message we wrote in the producer console. Make sure that you use the same topic parameter for both commands; oth- erwise, you won\u2019t see anything.",
    "Listing 2.5  Kafka consumer command",
    "bin/kafka-console-consumer.sh --bootstrap-server localhost:9094 \\",
    "--topic kinaction_helloworld --from-beginning",
    "The following listing shows an example of the output you might see in your console window.",
    "Listing 2.6  Example consumer output for kinaction_helloworld",
    "bin/kafka-console-consumer.sh --bootstrap-server localhost:9094 \\",
    "--topic kinaction_helloworld --from-beginning",
    "kinaction_helloworld",
    "As we send more messages and confirm the delivery to the consumer application, we can terminate the process and eliminate the --from-beginning option when we restart it. Notice that we didn\u2019t see all of the previously sent messages. Only those messages produced since the consumer console was started show up. The knowledge of which messages to read next and the ability to consume from a specific offset are tools we will leverage later as we discuss consumers in chapter 5. Now that we\u2019ve seen a simple exam- ple in action, we have a little more background to discuss the parts we utilized."
  ],
  "Part 1  Getting started<sec><sec><sec>Tour of Kafka<sec><sec>Producers and consumers": [
    "Table 2.1 shows the major components and their roles within the Kafka architecture. In the following sections, we\u2019ll dig into each of these items further to get a solid foun- dation for the following chapters.",
    "Table 2.1  The Kafka architecture"
  ],
  "Part 1  Getting started<sec><sec><sec>Tour of Kafka<sec><sec>Topics overview": [
    "Let\u2019s pause for a moment on the first stop on our tour: pro- ducers and consumers. Fig- ure\t2.3\thighlights\thow producers and consumers dif- fer in the direction of their data in relation to the cluster. A producer is a tool for sending messages to Kafka topics [1]. As mentioned in our use cases in chapter 1, a good example is a log file that is produced from an applica-",
    "Producer: source of data or messages being sent to Kafka",
    "We will both produce and consume from our kinaction topics.",
    "Consumer: data pulled from Kafka by consumers or sinks",
    "Figure 2.3  Producers vs. consumers",
    "tion. Those files are not a part of the Kafka system until they are collected and sent to Kafka. When you think of input (or data) going into Kafka, you are looking at a pro- ducer being involved somewhere internally.",
    "There are no default producers, per se, but the APIs that interact with Kafka use producers in their own implementation code. Some entry paths into Kafka might include using a separate tool such as Flume or even other Kafka APIs such as Connect and Streams. WorkerSourceTask, inside the Apache Kafka Connect source code (from version 1.0), is one example where a producer is used internally of its implementation. It provides its own higher-level API. This specific version 1.0 code is available under an Apache 2 license (https://github.com/apache/kafka/blob/trunk/LICENSE) and is viewable on GitHub (see http://mng.bz/9N4r). A producer is also used to send mes- sages inside Kafka itself. For example, if we are reading data from a specific topic and want to send it to a different topic, we would also use a producer.",
    "To get a feel for what our own producer will look like, it might be helpful to look at code similar in concept to WorkerSourceTask, which is the Java class that we men- tioned earlier. Listing 2.7 shows our example code. Not all of the source code is listed for the main method, but what is shown is the logic of sending a message with the stan- dard KafkaProducer. It is not vital to understand each part of the following example. Just try to get familiar with the producer\u2019s usage in the listing.",
    "Listing 2.7  A producer sending messages",
    "Alert alert = new Alert(1, \"Stage 1\", \"CRITICAL\", \"Stage 1 stopped\"); ProducerRecord<Alert, String> producerRecord =",
    "new ProducerRecord<Alert, String>",
    "(\"kinaction_alert\", alert, alert.getAlertMessage());",
    "The ProducerRecord holds each message sent into Kafka.",
    "producer.send(producerRecord, new AlertCallback());",
    "producer.close();",
    "Makes the actual call to send to our brokers",
    "Callbacks can be used for asynchronous sending of messages.",
    "To send data to Kafka, we created a ProducerRecord in listing 2.7. This object lets us define our message and specify the topic (in this case, kinaction_alert) to which we want to send the message. We used a custom Alert object as our key in the message. Next, we invoked the send method to send our ProducerRecord. While we can wait for the message, we can also use a callback to send asynchronous messages but still handle any errors. Chapter 4 provides this entire example in detail.",
    "Figure 2.4 shows a user interaction that could start the process of sending data into a producer. A user on a web page that clicks might cause an audit event that would be produced in a Kafka cluster.",
    "1. User-generated event",
    "Audit click event",
    "3. On completion, asynchronous",
    "Client/program application",
    "Figure 2.4  Producer example for user event",
    "In contrast to a producer, a consumer is a tool for retrieving messages from Kafka [1]. In the same vein as producers, if we are talking about getting data out of Kafka, we look at consumers as being involved directly or indirectly. WorkerSinkTask is another class inside the Apache Kafka Connect source code from version 1.0 that shows the use of a consumer that is parallel with the producer example from Connect as well (see http://mng.bz/WrRW). Consuming applications subscribe to the topics that they are interested in and continuously poll for data. WorkerSinkTask provides a real example in which a consumer is used to retrieve records from topics in Kafka. The fol- lowing listing shows the consumer example we will create in chapter 5. It displays con- cepts similar to WorkerSinkTask.java.",
    "Listing 2.8  Consuming messages",
    "consumer.subscribe(List.of(\"kinaction_audit\")); while (keepConsuming) {",
    "var records = consumer. poll(Duration.ofMillis(250));",
    "The consumer subscribes to the topics that it cares about.",
    "Messages are",
    "for (ConsumerRecord<String, String> record : records) { log.info(\"kinaction_info offset = {}, kinaction_value = {}\",",
    "record.offset(), record.value());",
    "OffsetAndMetadata offsetMeta =",
    "new OffsetAndMetadata(++record.offset(), \"\");",
    "returned from a poll of data.",
    "Map<TopicPartition, OffsetAndMetadata> kaOffsetMap = new HashMap<>(); kaOffsetMap.put(new TopicPartition(\"kinaction_audit\",",
    "record.partition()), offsetMeta);",
    "consumer.commitSync(kaOffsetMap);",
    "Listing 2.8 shows how a consumer object calls a subscribe method, passing in a list of topics that it wants to gather data from (in this case, kinaction_audit). The con- sumer then polls the topic(s) (see figure 2.5) and handles any data brought back as ConsumerRecords.",
    "User application",
    "Kafka (contains our data)",
    "Polling continues as long as the consumer client runs.",
    "Data is available for applications to use (show",
    "in a dashboard, for example).",
    "Figure 2.5  Consumer example flow",
    "The previous code listings 2.7 and 2.8 show two parts of a concrete use case example as displayed in figures 2.4 and 2.5. Let\u2019s say that a company wants to know how many users clicked on their web page for a new factory command action. The click events generated by users would be the data going into the Kafka ecosystem. The data\u2019s consumers would be the factory itself, which would be able to use its applications to make sense of the data. Putting data into Kafka and out of Kafka with code like the previous (or even with Kafka Connect itself) allows users to work with the data that can impact their business requirements and goals. Kafka does not focus on processing the data for applications: the consuming applications are where the data really starts to provide business value.",
    "Now that we know how to get data into and out of Kafka, the next area to focus on is where it lands in our cluster."
  ],
  "Part 1  Getting started<sec><sec><sec>Tour of Kafka<sec><sec>ZooKeeper usage": [
    "Topics are where most users start to think about the logic of what messages should go where. Topics consist of units called partitions [1]. In other words, one or more parti- tions can make up a single topic. As far as what is actually implemented on the com- puter\u2019s disk, partitions are what Kafka works with for the most part.",
    "NOTE  A single partition replica only exists on one broker and cannot be split between brokers.",
    "Figure 2.6 shows how each partition replica leader exists on a single Kafka broker and cannot be divided smaller than that unit. Think back to our first example, the",
    "kinaction_helloworld topic. If you\u2019re looking",
    "The topic kinaction_helloworld is made up of three partitions that will likely be spread out among different brokers.",
    "Partition 0\tPartition 1\tPartition 2",
    "Figure 2.6  Partitions make up topics.",
    "for reliability and want three copies of the data, the topic itself is not one entity (or a single file) that is copied; instead, it is the various partitions that are replicated three times each.",
    "NOTE The partition is even further bro- ken up into segment files written on the disk drive. We will cover these files\u2019 details and their location when we talk about brokers in later chapters. Although segment files make up partitions, you will likely not interact directly with them, and this should be considered an internal implementation detail.",
    "One of the most important concepts to under- stand at this point is the idea that one of the par- tition copies (replicas) will be what is referred to as a leader. For example, if you have a topic made up of three partitions and a total of three copies of each partition, every partition will have an",
    "elected leader replica. That leader will be one of the copies of the partition, and the other two (in this case, not shown in figure 2.6) will be followers, which update their information from their partition replica leader [1]. Producers and consumers only read or write from the leader replica of each partition it is assigned to during scenarios where there are no exceptions or failures (also known as a \u201chappy path\u201d scenario). But how does your producer or consumer know which partition replica is the leader? In the event of distributed computing and random failures, that answer is often influ- enced with help from ZooKeeper, the next stop on our tour."
  ],
  "Part 1  Getting started<sec><sec><sec>Tour of Kafka<sec><sec>Kafka\u2019s high-level architecture": [
    "One of the oldest sources of feared added complexity in the Kafka ecosystem might be that it uses ZooKeeper. Apache ZooKeeper (http://zookeeper.apache.org/) is a dis- tributed store that provides discovery, configuration, and synchronization services in a highly available way. In versions of Kafka since 0.9, changes were made in ZooKeeper that allowed for a consumer to have the option not to store information about how far it had consumed messages (called offsets). We will cover the importance of offsets in later chapters. This reduced usage did not get rid of the need for consensus and coor- dination in distributed systems, however.",
    "As you already saw, our cluster for Kafka includes more than one broker (server). To act as one correct application, these brokers need to not only communicate with each other, they also need to reach an agreement. Agreeing on which one is the replica leader of a partition is one example of the practical application of ZooKeeper within the Kafka ecosystem. For a real-world comparison, most of us have seen examples of clocks getting out of sync and how it becomes impossible to tell the correct time if multiple clocks are showing different times. The agreement can be challenging across separate brokers. Something is needed to keep Kafka coordinated and working in both success and failure scenarios.",
    "One thing to note for any production use case is that ZooKeeper will be an ensem- ble, but we will run just one server in our local setup [5]. Figure 2.7 shows the Zoo- Keeper cluster and how Kafka\u2019s interaction is with the brokers and not the clients. KIP-500 refers to this usage as the \u201ccurrent\u201d cluster design [4].",
    "Note: In our local setup, we only use one ZooKeeper process, not three.",
    "Note that in recent client versions,",
    "the only real interactions with ZooKeeper are with message brokers. Clients no longer store offsets in ZooKeeper.",
    "Figure 2.7  ZooKeeper interaction",
    "TIP If you are familiar with znodes or have experience with ZooKeeper already, one good place to start looking at the interactions inside Kafka\u2019s source code is ZkUtils.scala.",
    "Knowing the fundamentals of the preceding concepts increases our ability to make a practical application with Kafka. Also, we will start to see how existing systems that use Kafka are likely to interact to complete real use cases."
  ],
  "Part 1  Getting started<sec><sec><sec>Tour of Kafka<sec><sec>The commit log": [
    "In general, core Kafka can be thought of as Scala application processes that run on a Java virtual machine (JVM). Although noted for being able to handle millions of mes- sages quickly, what is it about Kafka\u2019s design that makes this possible? One of Kafka\u2019s keys is its usage of the operating system\u2019s page cache (as shown in figure 2.8). By avoid- ing caching in the JVM heap, the brokers can help prevent some of the issues that large heaps may have (for example, long or frequent garbage collection pauses) [6].",
    "Another design consideration is the access pattern of data. When new messages flood in, it is likely that the latest messages are of more interest to many consumers, which can then be served from this cache. Serving from a page cache instead of a disk is likely faster in most cases. Where there are exceptions, adding more RAM helps more of your workload to fall into the page cache.",
    "As mentioned earlier, Kafka uses its own protocol [7]. Using an existing protocol like AMQP (Advanced Message Queuing Protocol) was noted by Kafka\u2019s creators as",
    "No caching in the JVM heap!",
    "Kafka can process millions of messages quickly because it relies on the page cache instead of the JVM heap.",
    "Figure 2.8 The operating system\u2019s page cache",
    "having too large a part in the impacts on the actual implementation. For example, new fields were added to the message header to implement the exactly-once semantics of the 0.11 release. Also, that same release reworked the message format to compress messages more effectively. The protocol could change and be specific to the needs of the creators of Kafka.",
    "We are almost at the end of our tour. There\u2019s just one more stop\u2014brokers and the commit log."
  ],
  "Part 1  Getting started<sec><sec><sec>Various source code packages and what they do": [
    "One of the core concepts to help you master Kafka\u2019s foundation is to under- stand the commit log. The concept is simple but powerful. This becomes clearer as you understand the signifi- cance of this design choice. To clarify, the log we are talking about is not the same as the log use case that involved aggregating the output from loggers from an application process such as the LOGGER.error messages in Java.",
    "Figure 2.9 shows how simple the con-",
    "cept of a commit log can be as messages are added over time [8]. Although there are more mechanics that take place, such as what happens when a log file needs to come back from a broker fail- ure, this basic concept is a crucial part of understanding Kafka. The log used in Kafka is not just a detail that is hidden in",
    "Example of adding two messages (7 and 8) to a topic, such as kinaction_alert (see chapter 4)",
    "Here you see messages being received and added.",
    "As each new message comes",
    "in, it\u2019s added to the end of the log.",
    "Figure 2.9  Commit log",
    "other systems that might use something similar (like a write-ahead log for a database). It is front and center, and its users employ offsets to know where they are in that log.",
    "What makes the commit log special is its append-only nature in which events are always added to the end. The persistence as a log itself for storage is a major part of what separates Kafka from other message brokers. Reading a message does not remove it from the system or exclude it from other consumers.",
    "One common question then becomes, how long can I retain data in Kafka? In var- ious companies today, it is not rare to see that after the data in Kafka commit logs hits a configurable size or time retention period, the data is often moved into a perma- nent store. However, this is a matter of how much disk space you need and your pro- cessing workflow. The New York Times has a single partition that holds less than 100 GB [9]. Kafka is made to keep its performance fast even while keeping its messages. Retention details will be covered when we talk about brokers in chapter 6. For now, just understand that log data retention can be controlled by age or size using configu- ration properties."
  ],
  "Part 1  Getting started<sec><sec><sec>Various source code packages and what they do<sec><sec>Kafka Streams": [
    "Kafka is often mentioned in the titles of various APIs. There are also certain compo- nents that are described as standalone products. We are going to look at some of these to see what options we have. The packages in the following sections are APIs found in the same source code repository as Kafka core, except for ksqlDB [10]."
  ],
  "Part 1  Getting started<sec><sec><sec>Various source code packages and what they do<sec><sec>Kafka Connect": [
    "Kafka Streams has grabbed a lot of attention compared to core Kafka itself. This API is found in the Kafka source code project\u2019s streams directory and is mostly written in Java. One of the sweet spots for Kafka Streams is that no separate processing cluster is needed. It is meant to be a lightweight library to use in your application. You aren\u2019t required to have cluster or resource management software like Apache Hadoop to run your workloads. However, it still has powerful features, including local state with fault tolerance, one-at-a-time message processing, and exactly-once support [10]. The more you move throughout this book, the more you will understand the foundations of how the Kafka Streams API uses the existing core of Kafka to do some exciting and powerful work.",
    "This API was made to ensure that creating streaming applications is as easy as pos- sible, and it provides a fluent API, similar to Java 8\u2019s Stream API (also referred to as a domain-specific language, or DSL). Kafka Streams takes the core parts of Kafka and works on top of those smaller pieces by adding stateful processing and distributed joins, for example, without much more complexity or overhead [10].",
    "Microservice designs are also being influenced by this API. Instead of data being isolated in various applications, it is pulled into applications that can use data independently. Figure 2.10 shows a before and after view of using Kafka to",
    "implement a microservice system (see the YouTube video, \u201cMicroservices Explained by Confluent\u201d [11]).",
    "Although the top part of figure 2.10 (without Kafka) relies on each application talking directly to other applications at multiple interfaces, the bottom shows an approach that uses Kafka. Using Kafka not only exposes the data to all applications without some service munging it first, but it provides a single interface for all applica- tions to consume. The benefit of not being tied to each application directly shows how Kafka can help loosen dependencies between specific applications.",
    "Microservice interactions",
    "Microservices process and hold the data.",
    "Microservices leveraging Kafka",
    "Using Kafka Streams, you can share data, while processing is independent.",
    "Figure 2.10  Microservice design"
  ],
  "Part 1  Getting started<sec><sec><sec>Various source code packages and what they do<sec><sec>AdminClient package": [
    "Kafka Connect is found in the core Kafka Connect folder and is also mostly written in Java. This framework was created to make integrations with other systems easier [10]. In many ways, it can be thought to help replace other tools such as the Apache project Gobblin\u2122 and Apache Flume. If you are familiar with Flume, some of the terms used will likely seem familiar.",
    "Source connectors are used to import data from a source into Kafka. For example, if we want to move data from MySQL\u00ae tables to Kafka\u2019s topics, we would use a Connect source to produce those messages into Kafka. On the other hand, sink connectors are used to export data from Kafka into different systems. For example, if we want messages",
    "in some topic to be maintained long term, we would use a sink connector to consume those messages from the topic and place them somewhere like cloud storage. Figure",
    "2.11 shows this data flow from the database to Connect and then finally to a storage location in the cloud similar to a use case talked about in the article \u201cThe Simplest Use- ful Kafka Connect Data Pipeline in the World\u2026or Thereabouts \u2013 Part 1\u201d [12].",
    "A source connector streams table updates to Kafka topics.",
    "A sink connector exports data from Kafka topics to storage (maybe in the cloud) for later use or offline analysis.",
    "Connect is using a producer and consumer to move data.",
    "Our alert trend data in chapter 3 might be a reason to look at this flow.",
    "Figure 2.11  Connect use case",
    "As a note, a direct replacement of Apache Flume features is probably not the inten- tion or primary goal of Kafka Connect. Kafka Connect does not have an agent per Kafka node setup and is designed to integrate well with stream-processing frameworks to copy data. Overall, Kafka Connect is an excellent choice for making quick and sim- ple data pipelines that tie together common systems."
  ],
  "Part 1  Getting started<sec><sec><sec>Various source code packages and what they do<sec><sec>ksqlDB": [
    "Kafka introduced the AdminClient API recently. Before this API, scripts and other programs that wanted to perform specific administrative actions would either have to run shell scripts (which Kafka provides) or invoke internal classes often used by those shell scripts. This API is part of the kafka-clients.jar file, which is a different JAR than the other APIs discussed previously. This interface is a great tool that will come in handy the more involved we become with Kafka\u2019s administration [10]. This tool also uses a similar configuration that producers and consumers use. The source code can be found in the org/apache/kafka/clients/admin package."
  ],
  "Part 1  Getting started<sec><sec><sec>Confluent clients": [
    "In late 2017, Confluent released a developer preview of a new SQL engine for Kafka that was called KSQL before being renamed to ksqlDB. This allowed developers and data analysts who used mostly SQL for data analysis to leverage streams by using the interface they have known for years. Although the syntax might be somewhat familiar, there are still significant differences.",
    "Most queries that relational database users are familiar with involve on-demand or one-time queries that include lookups. The mindset shift to a continuous query over a data stream is a significant shift and a new viewpoint for developers. As with the Kafka Streams API, ksqlDB is making it easier to use the power of continuous data flows. Although the interface for data engineers will be a familiar SQL-like grammar, the idea that queries are continuously running and updating is where use cases like dash- boards on service outages would likely replace applications that once used point-in- time SELECT statements."
  ],
  "Part 1  Getting started<sec><sec><sec>Stream processing and terminology": [
    "Due to Kafka\u2019s popularity, the choice of which language to interact with Kafka usually isn\u2019t a problem. For our exercises and examples, we will use the Java clients created by the core Kafka project itself. There are many other clients supported by Confluent as well [13].",
    "Since all clients are not the same feature-wise, Confluent provides a matrix of supported features by programming language at the following site to help you out: https://docs.confluent.io/ current/clients/index.html. As a side note, taking a look at other open source clients can help you develop your own client or even help you learn a new language.",
    "Because using a client is the most likely way you will interact with Kafka in your applications, let\u2019s look at using the Java client (listing 2.9). We will do the same produce-and-consume process that we did when using the command line earlier. With a bit of additional boilerplate code (not listed here to focus on the Kafka-specific parts only), you can run this code in a Java main method to produce a message.",
    "Listing 2.9  Java client producer",
    "public class HelloWorldProducer {",
    "public static void main(String[] args) {",
    "Properties kaProperties = new Properties();",
    "kaProperties.put(\"bootstrap.servers\",",
    "The producer takes a map of name-value items to",
    "configure its various options.",
    "This property can take",
    "\"localhost:9092,localhost:9093,localhost:9094\");",
    "kaProperties.put(\"key.serializer\",",
    "a list of Kafka brokers.",
    "Tells the message\u2019s",
    "\"org.apache.kafka.common.serialization.StringSerializer\"); kaProperties.put(\"value.serializer\",",
    "\"org.apache.kafka.common.serialization.StringSerializer\");",
    "key and value what format to serialize",
    "try (Producer<String, String> producer = new KafkaProducer<>(kaProperties))",
    "ProducerRecord<String, String> producerRecord = new ProducerRecord<>(\"kinaction_helloworld\",",
    "Creates a producer instance. Producers implement the closable interface that\u2019s closed automatically by the Java runtime.",
    "null, \"hello world again!\");",
    "producer.send(producerRecord);",
    "Sends the record to the Kafka broker",
    "Represents our message",
    "The code in listing 2.9 is a simple producer. The first step to create a producer involves setting up configuration properties. The properties are set in a way that any- one who has used a map will be comfortable using.",
    "The bootstrap.servers parameter is one essential configuration item, and its pur- pose may not be apparent at first glance. This is a list of your Kafka brokers. The list does not have to be every server you have, though, because after the client connects, it will find the information about the rest of the cluster\u2019s brokers and not depend on that list. The key.serializer and value.serializer parameters are also something to take note of in development. We need to provide a class that will serialize the data as it",
    "moves into Kafka. Keys and values do not have to use the same serializer.",
    "Figure 2.12 displays the flow that happens when a producer sends a message. The producer we created takes in the configuration properties as an argument in the con- structor we used. With this producer, we can now send messages. The ProducerRecord contains the actual input that we want to send. In our examples, kinaction_helloworld is the name of the topic that we sent. The next fields are the message key followed by the message value. We will discuss keys more in chapter 4, but it is enough to know that these can, indeed, be a null value, which makes our current example less complicated.",
    "Producer record sent to topic kinaction_helloworld",
    "Thread-safe Kafka producer",
    "The call to send has already figured out",
    "which partition the producer record will be written to, although it is not defined in your client code explicitly. In this example, it is assigned to partition 1.",
    "Figure 2.12 Producer flow",
    "The message we send as the last argument is something different from the first mes- sage we sent with our console producer. Do you know why we want to make sure the message is different? We are working with the same topic with both producers, and because we have a new consumer, we should be retrieving the old message we pro- duced before in our Java client-initiated message. Once our message is ready, we asyn- chronously send it using the producer. In this case, because we are only sending one message, we close the producer, which waits until previously sent requests complete and then shuts down gracefully.",
    "Before running these Java client examples, we\u2019ll need to make sure we have the entry in the following listing in our pom.xml file [14]. We will use Apache Maven\u2122 in all of the examples in this book.",
    "Listing 2.10  Java client POM entry",
    "<dependency>",
    "<groupId>org.apache.kafka</groupId>",
    "<artifactId>kafka-clients</artifactId>",
    "<version>2.7.1</version>",
    "</dependency>",
    "Now that we have created a new message, let\u2019s use our Java client as in the following listing to create a consumer that can see the message. We can run the code inside a Java main method and terminate the program after we are done reading messages.",
    "Listing 2.11  Java client consumer",
    "public class HelloWorldConsumer { final static Logger log =",
    "LoggerFactory.getLogger(HelloWorldConsumer.class);",
    "private volatile boolean keepConsuming = true;",
    "public static void main(String[] args) { Properties kaProperties = new Properties(); kaProperties.put(\"bootstrap.servers\",",
    "\"localhost:9092,localhost:9093,localhost:9094\");",
    "Properties are set the same way as producers.",
    "kaProperties.put(\"group.id\", \"kinaction_helloconsumer\"); kaProperties.put(\"enable.auto.commit\", \"true\"); kaProperties.put(\"auto.commit.interval.ms\", \"1000\"); kaProperties.put(\"key.deserializer\",",
    "\"org.apache.kafka.common.serialization.StringDeserializer\"); kaProperties.put(\"value.deserializer\",",
    "\"org.apache.kafka.common.serialization.StringDeserializer\");",
    "HelloWorldConsumer helloWorldConsumer = new HelloWorldConsumer(); helloWorldConsumer.consume(kaProperties);",
    "Runtime.getRuntime().",
    "addShutdownHook(new Thread(helloWorldConsumer::shutdown));",
    "private void consume(Properties kaProperties) { try (KafkaConsumer<String, String> consumer =",
    "new KafkaConsumer<>(kaProperties)) { consumer.subscribe(",
    "\"kinaction_helloworld\"",
    "The consumer tells Kafka what topics it\u2019s interested in.",
    "while (keepConsuming) { ConsumerRecords<String, String> records =",
    "consumer.poll(Duration.ofMillis(250)); for (ConsumerRecord<String, String> record :",
    "Polls for new messages as they come in",
    "log.info(\"kinaction_info offset = {}, kinaction_value = {}\", record.offset(), record.value());",
    "}\tTo see the result, prints",
    "}\teach record that we",
    "}\tconsume to the console",
    "private void shutdown() { keepConsuming = false;",
    "One thing that jumps out is that we have an infinite loop in listing 2.11. It seems weird to do that on purpose, but we want to handle an infinite stream of data. The con- sumer is similar to the producer in taking a map of properties to create a consumer. However, unlike the producer, the Java consumer client is not thread safe [15]. We will need to take that into account as we scale past one consumer in later sections. Our code is responsible for ensuring that any access is synchronized: one simple option is having only one consumer per Java thread. Also, whereas we told the producer where to send the message, we now have the consumer subscribe to the topics it wants. A subscribe command can subscribe to more than one topic at a time.",
    "One of the most important sections to note in listing 2.11 is the poll call on the con- sumer. This is what is actively trying to bring messages to our application. No messages, one message, or many messages can all come back with a single poll, so it is important to note that our logic should account for more than one result with each poll call.",
    "Finally, we can Ctrl-C the consumer program when we retrieve the test messages and be done for now. As a note, these examples rely on many configuration properties that are enabled by default. We will have a chance to dig into them more in later chapters."
  ],
  "Part 1  Getting started<sec><sec><sec>Stream processing and terminology<sec><sec>Stream processing": [
    "We are not going to challenge distributed systems theories or certain definitions that could have various meanings, but rather look at how Kafka works. As you start to think of applying Kafka to your work, you will be presented with the following terms and can, hopefully, use the following descriptions as a lens through which to view your processing mindset.",
    "Data in (to partition)",
    "Messages can be replayed from the beginning of the log and consumed again.",
    "Partition 0",
    "Data out (from partition",
    "Logs are append only.",
    "New entries added to the end.",
    "No database storage, just disk.",
    "Each log is made up of entries labeled with offset numbers.",
    "ZooKeeper used for distributed configuration and management",
    "One of the brokers will be a controller.",
    "Figure 2.13  Kafka overview",
    "Figure 2.13 provides a high-level view of what Kafka does. Kafka has many moving parts that depend on data coming into and out of its core to provide value to its users. Producers send data into Kafka, which works as a distributed system for reliability and scale, with logs, which are the basis for storage. Once data is inside the Kafka ecosys- tem, consumers can help users utilize that data in their other applications and use cases. Our brokers make up the cluster and coordinate with a ZooKeeper cluster to maintain metadata. Because Kafka stores data on disk, the ability to replay data in case of an application failure is also part of Kafka\u2019s feature set. These attributes allow Kafka to become the foundation of powerful stream-processing applications."
  ],
  "Part 1  Getting started<sec><sec><sec>Stream processing and terminology<sec><sec>What exactly-once means": [
    "Stream processing seems to have various definitions throughout various projects. The core principle of streaming data is that data will keep arriving and will not end [16]. Also, your code should be processing this data all the time and not wait for a request",
    "or time frame with which to run. As we saw earlier, an infinite loop in our code hinted at this constant flow of data that does not have a defined endpoint.",
    "This approach does not batch data and then process it in groups. The idea of a nightly or monthly run is also not a part of this workflow. If you think of a never-ending waterfall, the same principles apply. Sometimes there is a massive amount of data to transit and sometimes not that much, but it continuously flows between destinations.",
    "Figure 2.14 shows that the Kafka Streams API depends on core Kafka. While event messages continue to come into the cluster, a consumer application can provide the end user with updated information continuously rather than wait for a query to pull a static snapshot of the events. No more refreshing the web page after 5 minutes for users to see the latest events!",
    "Current data is shown, which is not a query from static data.",
    "End user view",
    "In chapter 5, when we discuss alerts,",
    "this updated view would help us see the latest alerts.",
    "Kafka topics and producers/consumers are leveraged by the Streams API.",
    "As events keep coming into Kafka, the Streams API continually processes the data.",
    "Figure 2.14 Stream process"
  ],
  "Part 2  Applying Kafka": [
    "1 \u201cMain Concepts and Terminology.\u201d Apache Software Foundation (n.d.). https:// kafka.apache.org/documentation.html#intro_concepts_and_terms (accessed May 22, 2019).",
    "2  \u201cApache Kafka Quickstart.\u201d Apache Software Foundation (2017). https://kafka",
    ".apache.org/quickstart (accessed July 15, 2020).",
    "3 B. Kernighan and D. Ritchie. The C Programming Language, 1st ed. Englewood Cliffs, NJ, USA: Prentice Hall, 1978.",
    "4 KIP-500: \u201cReplace ZooKeeper with a Self-Managed Metadata Quorum.\u201d Wiki for Apache Kafka. Apache Software Foundation (July 09, 2020). https://cwiki",
    ".apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+ with+a+Self-Managed+Metadata+Quorum (accessed August 22, 2020).",
    "5 \u201cZooKeeper Administrator\u2019s Guide.\u201d Apache Software Foundation. (n.d.). https://zookeeper.apache.org/doc/r3.4.5/zookeeperAdmin.html (accessed June 10, 2020).",
    "6 \u201cKafka Design: Persistence.\u201d Confluent documentation (n.d.). https://docs.con- fluent.io/platform/current/kafka/design.html#persistence (accessed November 19, 2020).",
    "7  \u201cA Guide To The Kafka Protocol: Some Common Philosophical Questions.\u201d Wiki for Apache Kafka. Apache Software Foundation (n.d.). https://cwiki.apache.org/ confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToThe KafkaProtocol-SomeCommonPhilosophicalQuestions (accessed August 21, 2019).",
    "8 \u201cDocumentation: Topics and Logs.\u201d Apache Software Foundation (n.d.). https:// kafka.apache.org/23/documentation.html#intro_topics (accessed May 25, 2020).",
    "9  B. Svingen. \u201cPublishing with Apache Kafka at The New York Times.\u201d Confluent blog (September 6, 2017). https://www.confluent.io/blog/publishing-apache",
    "-kafka-new-york-times/ (accessed September 25, 2018).",
    "10 \u201cDocumentation: Kafka APIs.\u201d Apache Software Foundation (n.d.). https:// kafka.apache.org/documentation.html#intro_apis (accessed June 15, 2021).",
    "11 \u201cMicroservices Explained by Confluent.\u201d Confluent. Web presentation (August 23, 2017). https://youtu.be/aWI7iU36qv0 (accessed August 9, 2021).",
    "12  R. Moffatt. \u201cThe Simplest Useful Kafka Connect Data Pipeline in the World\u2026or Thereabouts \u2013 Part 1.\u201d Confluent blog (August 11, 2017). https://www.confluent",
    ".io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/ (accessed December 17, 2017).",
    "13 \u201cKafka Clients.\u201d Confluent documentation (n.d.). https://docs.confluent.io/ current/clients/index.html (accessed June 15, 2020).",
    "14 \u201cKafka Java Client.\u201d Confluent documentation (n.d.). https://docs.confluent",
    ".io/clients-kafka-java/current/overview.html (accessed June 21, 2021).",
    "15 \u201cClass KafkaConsumer<K,V>.\u201d Apache Software Foundation (November 09, 2019).\thttps://kafka.apache.org/24/javadoc/org/apache/kafka/clients/con",
    "sumer/KafkaConsumer.html (accessed November 20, 2019).",
    "16  \u201cStreams Concepts.\u201d Confluent documentation (n.d.). https://docs.confluent",
    ".io/platform/current/streams/concepts.html (accessed June 17, 2020)."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Designing a Kafka project": [
    "n part 2, we will build on our mental model of Kafka that we developed in part 1 by putting our knowledge into action. We will look at the foundations of Kafka and start with the fundamental subjects of producer and consumer clients. Even if you only plan to develop Kafka Streams or ksqlDB applications, part 2 is still worth your time. The core pieces discussed in this part will underlay most higher-level libraries and abstractions in the Kafka ecosystem:",
    "In chapter 3, we begin designing a sample project and learn how to apply Kafka to it. While schemas are covered in more detail in chapter 11, our project requirements show this need early on in our data project designs.",
    "In chapter 4, we go into detail about how we can use producers to move data into Kafka. We discuss important configuration options and their impact on our data sources.",
    "In chapter 5, we dig into the consumption of data from Kafka by using consumers. Parallels and differences are drawn between consumer clients and the producers from chapter 4.",
    "In chapter 6, we start to look at the brokers\u2019 role in our clusters. This chapter covers roles for both leaders and controllers as well as their rela- tionship to our clients.",
    "In chapter 7, we look at how topics and partitions fit together to provide the data we depend on. Compacted topics are also introduced.",
    "In chapter 8, we explore tools and architectures that are options for han- dling data that we need to retain or reprocess.",
    "In chapter 9, which finishes part 2, we review essential logs and metrics to help with administrative duties to keep our clusters healthy.",
    "Following part 2, you should have a solid understanding of the core pieces of Kafka and how to use these pieces in your use cases. Let\u2019s get started!",
    "In our previous chapter, we saw how we can work with Kafka from the command line and how to use a Java client. Now, we will expand on those first concepts and look at designing various solutions with Kafka. We will discuss some questions to consider as we lay out a strategy for the example project we\u2019ll start in this chapter. As we begin to develop our solutions, keep in mind that, like most projects, we might make minor changes along the way and are just looking for a place to jump in and start developing. After reading this chapter, you will be well on your way to solving real- world use cases while producing a design to facilitate your further exploration of Kafka in the rest of this book. Let\u2019s start on this exciting learning path!"
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Designing a Kafka project<sec><sec>Taking over an existing data architecture": [
    "Although new companies and projects can use Kafka as they get started, that is not the case for all Kafka adopters. For those of us who have been in enterprise environments or worked with legacy systems (and anything over five years old is probably considered legacy these days), in reality, starting from scratch is not a luxury we always have. How- ever, one benefit of dealing with existing architectures is that it gives us a list of pain points, that we can address. The contrast also helps us to highlight the shift in think- ing about the data in our work. In this chapter, we will work on a project for a com- pany that is ready to shift from their current way of handling data and apply this new hammer named Kafka."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Designing a Kafka project<sec><sec>A first change": [
    "Let\u2019s look at some background to give us our fictional example inspired by Kafka\u2019s ever-growing usage. One topic by Confluent mentioned in chapter 1 (https:// www.confluent.io/use-case/internet-of-things-iot/) and also an excellent article by Janakiram MSV, titled \u201cApache Kafka: The Cornerstone of an Internet-of-Things Data Platform,\u201d includes Kafka\u2019s use of sensors [1]. Using the topic of sensors as a use case, we will dig into a fictional example project.",
    "Our new fictional consulting company has just won a contract to help re-architect a plant that works on e-bikes and manages them remotely. Sensors are placed throughout the bike that continuously provide events about the condition and status of the internal equipment they are monitoring. However, so many events are gener- ated that the current system ignores most of the messages. We have been asked to help the site owners unlock the potential in that data for their various applications to uti- lize. Besides this, our current data includes traditional relational database systems that are large and clustered. With so many sensors and an existing database, how might we create our new Kafka-based architecture without impacting manufacturing?"
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Designing a Kafka project<sec><sec>Built-in features": [
    "One of the best ways to start our task is probably not with a big-bang approach\u2014all our data does not have to move into Kafka at once. If we use a database today and want to kick the tires on the streaming data tomorrow, one of the easiest on-ramps starts with Kafka Connect. Although it can handle production loads, it does not have to out of the gate. We will take one database table and start our new architecture while letting the existing applications run for the time being. But first, let\u2019s get into some examples to gain familiarity with Kafka Connect."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Designing a Kafka project<sec><sec>Data for our invoices": [
    "The purpose of Kafka Connect is to help move data into or out of Kafka without writ- ing our own producers and consumers. Connect is a framework that is already part of Kafka, which makes it simple to use previously built pieces to start your streaming work. These pieces are called connectors, and they were developed to work reliably with other data sources [2].",
    "If you recall from chapter 2, some of the producer and consumer Java client real- world code that we used as examples showed how Connect abstracts those concepts away by using them internally with Connect. One of the easiest ways to start is by look- ing at how Connect can take a typical application log file and move it into a Kafka topic. The easiest option to run and test Connect on your local machine is standalone mode. Scaling can come later if we like what we can do in standalone mode! In the folder where you installed Kafka, locate the following files under the config directory:",
    "connect-standalone.properties",
    "connect-file-source.properties",
    "Peeking inside the connect-standalone.properties file, you should see some configura- tion keys and values that should look familiar from some of the properties we used to make our own Java clients in chapter 2. Knowing the underlying producers and con- sumer clients can help us understand how Connect uses that same configuration to complete its work by listing items such as bootstrap.servers.",
    "In our example, we\u2019ll take data from one data source and put that into Kafka so that we can treat data as being sourced from a Kafka file. Using the file connect-file- source.properties, included with your Kafka installation as an example template, let\u2019s create a file called alert-source.properties and place the text from listing 3.1 inside as the contents of our file. This file defines the configurations that we need to set up the file alert.txt and to specify the data be sent to the specific topic kinaction_alert_connect. Note that this example is following steps similar to the excellent Connect Quickstart guide at https://docs.confluent.io/3.1.2/connect/quickstart.html if you need more reference material. To learn even more detailed information, we recommend watching the excellent presentation of Randall Hauch (Apache Kafka committer and PMC) from the Kafka Summit (San Francisco, 2018) located at http://mng.bz/8WeD.",
    "With configurations (and not code), we can get data into Kafka from any file. Because reading from a file is a common task, we can use Connect\u2019s prebuilt classes. In this case, the class is FileStreamSource [2]. For the following listing, let\u2019s pretend that we have an application that sends alerts to a text file.",
    "Listing 3.1  Configuring Connect for a file source",
    "name=alert-source connector.class=FileStreamSource tasks.max=1",
    "file=alert.txt topic=kinaction_alert_connect",
    "Names the topic where this data will be sent",
    "Specifies the class that interacts with our source file",
    "For standalone mode, 1 is a valid value to test our setup.",
    "file for changes",
    "The value of the topic property is significant. We will use it later to verify that mes- sages are pulled from a file into the specific kinaction_alert_connect topic. The file alert.txt is monitored for changes as new messages flow in. And finally, we chose 1 for the value of tasks.max because we only really need one task for our connector and, in this example, we are not worried about parallelism.",
    "NOTE If you are running ZooKeeper and Kafka locally, make sure that you have your own Kafka brokers still running as part of this exercise (in case you shut them down after the previous chapter).",
    "Now that we have done the needed configuration, we need to start Connect and send in our configurations. We can start the Connect process by invoking the shell script connect-standalone.sh, including our custom configuration file as a parameter to that script. To start Connect in a terminal, run the command in the following listing and leave it running [2].",
    "Listing 3.2  Starting Connect for a file source",
    "bin/connect-standalone.sh config/connect-standalone.properties \\ alert-source.properties",
    "Moving to another terminal window, create a text file named alert.txt in the directory in which we started the Connect service and add a couple of lines of text to this file using your text editor; the text can be anything you want. Now let\u2019s use the console- consumer command to verify that Connect is doing its job. For that, we\u2019ll open another terminal window and consume from the kinaction_alert_connect topic, using the following listing as an example. Connect should ingest this file\u2019s contents and produce the data into Kafka [2].",
    "Listing 3.3  Confirming file messages made it into Kafka",
    "bin/kafka-console-consumer.sh \\",
    "--bootstrap-server localhost:9094 \\",
    "--topic kinaction_alert_connect --from-beginning",
    "Before moving to another connector type, let\u2019s quickly talk about the sink connector and how it carries Kafka\u2019s messages back out to another file. Because the destination (or sink) for this data is another file, we want to look at the connect-file-sink.proper- ties file. A small change is shown in listing 3.4 as the new outcome is written to a file rather than read from a file as we did previously. We\u2019ll declare FileStreamSink to define a new role as a sink. The topic kinaction_alert_connect is the source of our data in this scenario. Placing the text from the following listing in a new file called alert-sink.properties sets up our new configuration [2].",
    "Listing 3.4  Configuring Connect for a file source and a sink",
    "name=alert-sink connector.class=FileStreamSink tasks.max=1",
    "file=alert-sink.txt topics=kinaction_alert_connect",
    "Names the topic that the data comes from",
    "An out-of-the-box class to which we delegate the work of interacting with our file",
    "For standalone mode, 1 is a valid value to test our setup.",
    "The destination file for any messages that make it into our Kafka topic",
    "If the Connect instance is still running in a terminal, we\u2019ll need to close that terminal window or stop the process by pressing Ctrl-C. Then we\u2019ll restart it with the file-source and file-sink property files. Listing 3.5 shows how to restart Connect with both our cus- tom alert source and sink properties [2]. The end result should be data flowing from a file into Kafka and back out to a separate destination.",
    "Listing 3.5  Starting Connect for a file source and a sink",
    "bin/connect-standalone.sh config/connect-standalone.properties \\ alert-source.properties alert-sink.properties",
    "To confirm that Connect is using our new sink, open the sink file we used in our con- figuration, alert-sink.txt, and verify that you can see the messages that were in the source file and that these were sent to the Kafka topic."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Sensor event design": [
    "Let\u2019s look at another requirement, dealing with our invoices for bike orders. Connect easily lets those with in-depth knowledge of creating custom connectors share them with others (to help those of us who may not be experts in these systems). Now that we have used a connector (listings 3.4 and 3.5), it should be relatively simple to integrate a different connector because Connect standardizes interaction with other systems.",
    "To use Connect in our manufacturing example, let\u2019s look at applying an existing source connector that streams table updates from a local database to a Kafka topic. Again, our goal is not to change the entire data processing architecture at once. Instead, we\u2019ll show how we can bring in updates from a table-based database applica- tion and develop our new application in parallel while letting the other system exist as is. Note that this example is following steps similar to the guide at https:// docs.confluent.io/kafka-connect-jdbc/current/source-connector/index.html, if you need more reference material.",
    "Our first step is to set up a database for our local examples. For ease of use and to get started quickly, we\u2019ll use connectors from Confluent for SQLite. If you can run sqlite3 in your terminal and get a prompt, then you are already set. Otherwise, use your favorite package manager or installer to get a version of SQLite that works on your operating system.",
    "TIP Check out the Commands.md file in the source code for this chapter to find installation instructions for the Confluent command line interface (CLI) as well as the JDBC connector using confluent-hub. The rest of the examples reference commands in the Confluent-installed directory only and not in the Kafka-installed directory.",
    "To create a database, we will run sqlite3 kafkatest.db from the command line. In this database, we will then run the code in listing 3.6 to create the invoices table and to insert some test data in the table. As we design our table, it is helpful to think of how we",
    "will capture changes into Kafka. Most use cases will not require us to capture the entire database but only changes after the initial load. A timestamp, sequence number, or ID can help us determine which data has changed and needs to be sent to Kafka. In the following listing, the ID or modified columns could be our guide for Connect to let Kafka know which data was modified in the table [3].",
    "Listing 3.6  Creating the invoices table",
    "CREATE TABLE invoices(",
    "id INT PRIMARY KEY\tNOT NULL, title\tTEXT\tNOT NULL,",
    "details\tCHAR(50),",
    "billedamt\tREAL,",
    "Creates an invoices table",
    "Sets an incremental ID so Connect knows which entries to capture",
    "modified\tTIMESTAMP DEFAULT (STRFTIME('%s', 'now')) NOT NULL",
    "INSERT INTO invoices (id,title,details,billedamt) \\ VALUES (1, 'book', 'Franz Kafka', 500.00 );",
    "Inserts test data into our table",
    "By creating a file in the location etc/kafka-connect-jdbc/kafkatest-sqlite.properties, and after making slight changes to our database table name, we can see how additional inserts and updates to the rows cause messages to be sent into Kafka. Refer to the source code for chapter 3 in the Git repository to find more detailed setup instructions for finding and creating the JDBC connector files in the Confluent installation directory. It is not part of the Apache Kafka distribution like the file connector. Also, if the mod- ified timestamp format gives an error, make sure to check out other options in the source code with this chapter.",
    "Now that we have a new configuration, we need to start Connect to pass it kafka- test-sqlite.properties.",
    "Listing 3.7  Starting Connect for a database table source",
    "confluent-hub install confluentinc/kafka-connect-jdbc:10.2.0 confluent local services connect start",
    "# See Commands.md for other steps",
    "confluent local services connect connector config jdbc-source",
    "--config etc/kafka-connect-jdbc/kafkatest-sqlite.properties",
    "We are using our new database properties file.",
    "Listing 3.7 shows how you can launch Connect with the Confluent CLI tool. The stand- alone connnect script, connect-standalone.sh, could have also been used [3]. Although the power of Kafka Connect is great for moving existing database tables to Kafka, our sensors (which are not database backed) are going to require a different technique."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Sensor event design<sec><sec>Existing issues": [
    "Because there are no existing connectors for our state-of-the-art sensors, we can directly interact with their event system through custom producers. The ability to hook into and write our producers to send data into Kafka is where we will look at the requirements in the following sections.",
    "Figure 3.1 shows that there is a critical path of stages that need to work together. One of the steps is an additional quality check sensor. This sensor can be skipped to avoid processing delays if it goes down for maintenance or failure. Sensors are attached to all of the bikes\u2019 internal steps (represented by gears in figure 3.1), and they send messages to the clustered database machines that exist in the current sys- tem. There is also an administration console used remotely to update and run com- mands against the sensors already built into the system.",
    "In the line, each gear represents a major step in our process. Each step",
    "Currently, we have the sensors send their events for storage to a clustered database solution. This is one of the main parts of",
    "An admin console issues",
    "commands to the sensors.",
    "We can dismiss this quality",
    "check in the line.",
    "Figure 3.1  Factory setup"
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Sensor event design<sec><sec>Why Kafka is the right fit": [
    "Let\u2019s start by discussing some of the issues that have come up in most of our previous use cases. The need for data to exist and to be available to users is a deep and chal- lenging problem. Let\u2019s look at how we can deal with two of those challenges: data silos and recoverability.",
    "Dealing with data silos",
    "In our factory, the data and the processing are owned by an application. If others want to use that data, they would need to talk to the application owner. And what are the chances that the data is provided in a format that can be easily processed? Or what if it does not provide the data at all?",
    "The shift from traditional \u201cdata thinking\u201d makes the data available to everyone in its raw source. If you have access to the data as it comes in, you do not have to worry about the application API exposing it to specific formats or after custom transforma- tions. And what if the application providing the API parses the original data incor- rectly? To untangle that mess might take a while if we have to recreate the data from changes to the original data source.",
    "Recoverability",
    "One of the excellent perks of a distributed system like Kafka is that failure is an expected condition: it\u2019s planned for and handled! However, along with system blips, we also have the human element in developing applications. If an application has a defect or a logic issue that destroys our data, what would be our path to correct it? With Kafka, that can be as simple as starting to consume from the beginning topic as we did with the console consumer flag --from-beginning in chapter 2. Additionally, data retention makes it available for use again and again. The ability to reprocess data for corrections is powerful. But if the original event is not available, it might be hard to retrofit the existing data.",
    "Because events are only produced once from the sensor source for a specific instance, the message broker can play a crucial part in our consumption pattern. If the message in a queuing system is removed from the broker after a subscriber reads the message, as in version 1.0 of the application in figure 3.2, it is gone from the sys- tem. If a defect in an application\u2019s logic is found after the fact, analysis would be needed to see if data can be corrected using what was left over from the processing of that original event because it will not be fired again. Fortunately, Kafka brokers allow for a different option.",
    "Beginning with version 1.1, the application can replay those messages already con- sumed with the new application logic. Our new application code that fixed a logic mistake from version 1.0 can process all the events again. The chance to process our events again makes it easier to enhance our applications without data loss or corruption.",
    "The replay of data can also show us how a value changes over time. It might be ben- eficial to draw a parallel between replaying the Kafka topic and the idea of a write-ahead log (WAL). With a WAL, we can tell what a value used to be and the changes that hap- pened over time because modifications to values are written in the log before they are applied. WALs are commonly found in database systems and help a system recover if an action fails during a transaction. If you follow the events from the beginning to the end, you would see how data moves from its initial value to its current value.",
    "Source of data (events are fired one time)",
    "Version 1.0 consumption removed in the event that a message is lost",
    "Version 1.0",
    "Kafka replays all the messages because nothing was removed and can be seen in version 1.1.",
    "Version 1.1",
    "Figure 3.2  Looking at a developer coding mistake",
    "When should data be changed?",
    "Whether data is coming from a database or a log event, our preference is to get the data into Kafka first; then the data will be available in its purest form. But each step before it is stored in Kafka is an opportunity for the data to be altered or injected with various formatting or programming logic errors. Keep in mind that hardware, software, and logic can and will fail in distributed computing, so it\u2019s always great to get data into Kafka first, which gives you the ability to replay data if any of those failures occur."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Sensor event design<sec><sec>Thought starters on our design": [
    "Does Kafka even make sense in our fictional sensor use case? Of course, this is a book about Kafka, right? However, let\u2019s quickly try to pinpoint a couple of compelling rea- sons to give Kafka a try.",
    "One thing that has been made clear by our clients is that their current database is getting expensive to scale vertically. By vertical scaling, we mean increasing things like CPU, RAM, and disk drives in an existing machine. (To scale dynamically, we would look at adding more servers to our environment.) With the ability to horizontally scale our cluster, we can hope to get more overall benefits for our buck. Although the servers that we run our brokers on might not be the cheapest machines money can buy, 32 GB or 64 GB of RAM on these servers can handle production loads [4].",
    "The other item that probably jumped out at you is that we have events being pro- duced continuously. This should sound similar to the definition of stream processing that we talked about earlier. The constant data feed won\u2019t have a defined end time or stopping point, so our systems should be ready to handle messages constantly. Another interesting point to note is that our messages are usually under 10 KB for our example. The smaller the message size and the amount of memory we can offer to page caches, the better shape we are in to keep our performance healthy.",
    "During this requirements review for our scenario, some security-minded develop- ers might have noticed there\u2019s no built-in disk encryption for the brokers (data at rest). However, that isn\u2019t a requirement for the current system. We will first focus on getting our system up and running and then worry about adding security at a later point in our implementation."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Sensor event design<sec><sec>User data requirements": [
    "One thing to note is which features are available for specific Kafka versions. Although we use a recent version for our examples (at the time of this writing, version 2.7.1), some developers might not have control over the current broker and client versions they are using due to their existing infrastructures. For this reason, it is good to keep in mind when some of the features and APIs we might use made their debut. Table 3.1 highlights some of the past major features but is not inclusive of all versions [5].",
    "Table 3.1  Past Kafka version milestones",
    "Another thing to note as we focus on clients in the next few chapters is the feature- improved client compatibility. Broker versions since 0.10.0 can work with newer client versions. This is important because we can now try new versions of clients by upgrad- ing them first, and the brokers can remain at their version until we decide that we want to upgrade them. This comes in handy as you work through this material if you are running against a cluster that already exists.",
    "Now that we have decided to give Kafka a try, this might be a good time to decide how we want our data to exist. The following questions are intended to make us think about how we want to process our data. These preferences impact various parts of our design, but our main focus here is on figuring out the data structure; we will cover the",
    "implementation in later chapters. This list is not meant to be complete, but it is a good starting point in planning our design:",
    "Is it okay to lose any messages in the system? For example, is one missed event about a mortgage payment going to ruin your customer\u2019s day and their trust in your business? Or is it a minor issue such as your social media account RSS feed miss- ing a post? Although the latter is unfortunate, would it be the end of your cus- tomer\u2019s world?",
    "Does your data need to be grouped in any way? Are the events correlated with other events that are coming in? For example, are we going to be taking in account changes? In that case, we\u2019d want to associate the various account changes with the customer whose account is changing. Grouping events up front might also prevent the need for applications to coordinate messages from multiple con- sumers while reading from the topic.",
    "Do you need data delivered in a specific order? What if a message gets delivered in an order other than when it occurred? For example, you get an order-canceled notice before the actual order. Because product ends up shipping due to order alone, the customer service impact is probably good enough reason to say that the ordering is indeed essential. Or course, not everything will need exact ordering. For example, if you are looking at SEO data for your business, the order is not as important as making sure that you can get a total at the end.",
    "Do you only want the last value of a specific item, or is the history of that item important? Do you care about how your data has evolved? One way to think about this looks at how data is updated in a traditional relational database table. It is mutated in place (the older value is gone and the newer value replaces it). The history of what that value looked like a day ago (or even a month ago) is lost.",
    "How many consumers are you going to have? Will they all be independent of each other, or will they need to maintain some sort of order when reading the mes- sages? If you are going to have a lot of data that you want to consume as quickly as possible, that will inform and help shape how you break up your messages on the tail end of your processing.",
    "Now that we have a couple of questions to ask for our factory, let\u2019s try to apply these to our actual requirements. We will use a chart to answer each scenario. We will learn how to do this in the following section."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Sensor event design<sec><sec>High-level plan for applying our questions": [
    "Our new architecture needs to provide a couple of specific key features. In general, we want the ability to capture messages even if the consuming service is down. For exam- ple, if one of the consumer applications is down in our remote plant, we want to make sure that it can later process the events without dropping messages entirely. Addition- ally, when the application is out of maintenance or comes back up after a failure, we want it to still have the data it needs. For our example use case, we also want the status from our sensors as either working or broken (a sort of alert), and we want to make sure we can see if any part of our bike process could lead to total failure.",
    "Along with the preceding information, we also want to maintain a history of the sensors\u2019 alert status. This data could be used in determining trends and in predicting failures from sensor data before actual events lead to broken equipment. We also want to keep an audit log of any users that push updates or queries directly against the sen- sors. Finally, for compliance reasons, we want to know who did what administration actions on the sensors themselves."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Sensor event design<sec><sec>Reviewing our blueprint": [
    "Let\u2019s focus closer on our requirements to create an audit log. Overall, it seems like everything that comes in from the management API will need to be captured. We want to make sure that only users with access permissions are able to perform actions against the sensors, and we should not lose messages, as our audit would not be com- plete without all the events. In this case, we do not need any grouping key because each event can be treated as independent.",
    "The order does not matter inside our audit topic because each message will have a timestamp in the data itself. Our primary concern is that all the data is there to process. As a side note, Kafka itself does allow messages to be sorted by time, but the message payload can include time. However, this specific use case does not warrant this usage. Figure 3.3 shows how a user would generate two audit events from a web adminis- tration console by sending a command to sensor 1 and another to sensor 3. Both com- mands should end up as separate events in Kafka. To make this a little clearer, table",
    "3.2 presents a rough checklist of things we should consider regarding data for each",
    "Process step 1 sensor",
    "Audit events are sent for sensors in step 1 and step 3. Commands were sent to those sensors only from the admin console and will be captured in Kafka.",
    "Console for admin apps www.",
    "Process step 1 audit event Process step 2",
    "Kafka brokers",
    "Process step 3 audit event",
    "Process step 3 sensor",
    "Two software update events are logged for the sensor updates: one for step 1 and one for step 3.",
    "Figure 3.3  Audit use case",
    "requirement. This at-a-glance view will help us when determining the configuration options we want to use for our producer clients.",
    "In this audit producer, we are concerned with making sure that no data is lost and that consuming appli- cations do not have any worries about data being ordered or coordi- nated. Furthermore, the alert trend of our status requirements deals with",
    "Table 3.2  Audit checklist",
    "each process in the bike\u2019s system with a goal of spotting trends. It might be helpful to group this data using a key. We have not addressed the term key in depth, but it can be thought of as a way to group related events.",
    "We will likely use the bikes\u2019 part ID names at each stage of the internal system where the sensor is installed because they will be unique from any other name. We want to be able to look across the key at all of the events for a given stage to spot these trends over time. By using the same key for each sensor, we should be able to consume these events easily. Because alert statuses are sent every 5 seconds, we are not con- cerned about missing a message, as the next one should arrive shortly. If a sensor sends a \u201cNeeds Maintenance\u201d message every couple of days, that is the type of infor- mation we want to have to spot trends in equipment failure.",
    "Figure 3.4 shows a sensor watching each stage of the process. Those equipment alert events go into Kafka. Although not an immediate concern for our system, Kafka does enable us to pull that data into other data storage or processing system like Hadoop.",
    "Hadoop HDFS",
    "Kafka brokers",
    "Sensor trend events by stage go into Kafka. These events can be moved to permanent storage for analysis.",
    "Figure 3.4  Alert trend use case",
    "Table 3.3  Audit checklist",
    "Table 3.3 highlights that our goal is to group the alert results by stage and that we are not concerned about los- ing a message from time to time.",
    "As for alerting on statuses, we also want to group by a key, which is the process stage. However, we do not care about past states of the sensor but rather the current status. In other words, the current status is all we care about and need for our requirements.",
    "The new status replaces the old, and we do not need to maintain a history. The word replace here is not entirely correct (or not what we are used to thinking). Internally, Kafka adds the new event that it receives to the end of its log file like any other message it receives. After all, the log is immutable and can only be appended to at the end of the file. How does Kafka make what appears to be an update happen? It uses a process called log compaction, which we will dig into in chapter 7.",
    "Another difference we have with this requirement is the consumer usage assigned to specific alert partitions. Critical alerts are processed first due to an uptime require- ment in which those events need to be handled quickly. Figure 3.5 shows an example of how critical alerts could be sent to Kafka and then consumed to",
    "This sensor on the critical path is noting a failure for that component.",
    "Kafka brokers",
    "Operators will see the current status in a dashboard web page.",
    "Critical path",
    "Figure 3.5  Alert use case",
    "populate an operator\u2019s display to get attention quickly. Table 3.4 reinforces the idea that we want to group an alert to the stage it was created in and that we want to know the latest status only. Taking the time to plan out our data requirements will not only help us clarify our application require- ments but, hopefully, validate the use",
    "of Kafka in our design."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Format of your data": [
    "Table 3.4  Audit checklist",
    "One of the last things to think about is how we want to keep these groups of data orga- nized. Logically, the groups of data can be thought of in the following manner:",
    "Alert trend data",
    "For those of you already jumping ahead, keep in mind that we might use our alert trend data as a starting point for our alerts topic; you can use one topic as the starting point to populate another topic. However, to start our design, we will write each event type from the sensors to their logical topic to make our first attempt uncomplicated and easy to follow. In other words, all audit events end up on an audit topic, all alert trend events end up on a alert trend topic, and our alert events on an alert topic. This one-to-one mapping makes it easier to focus on the requirements at hand for the time being."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Format of your data<sec><sec>Plan for data": [
    "One of the easiest things to skip, but critical to cover in our design, is the format of our data. XML and JSON are pretty standard formats that help define some sort of structure to our data. However, even with a clear syntax format, there can be informa- tion missing in our data. What is the meaning of the first column or the third one? What is the data type of the field in the second column of a file? The knowledge of how to parse or analyze our data can be hidden in applications that repeatedly pull the data from its storage location. Schemas are a means of providing some of this needed information in a way that can be used by our code or by other applications that may need the same data.",
    "If you look at the Kafka documentation, you may have noticed references to another serialization system called Apache Avro. Avro provides schema definition sup- port as well as schema storage in Avro files [6]. In our opinion, Avro is likely what you will see in Kafka code that you might encounter in the real world and why we will focus on this choice out of all the available options. Let\u2019s take a closer look at why this format is commonly used in Kafka."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Format of your data<sec><sec>Dependency setup": [
    "One of the significant gains of using Kafka is that the producers and consumers are not tied directly to each other. Further, Kafka does not do any data validation by default. However, there is likely a need for each process or application to understand what that data means and what format is in use. By using a schema, we provide a way for our application\u2019s developers to understand the structure and intent of the data. The definition doesn\u2019t have to be posted in a README file for others in the organiza- tion to determine data types or to try to reverse-engineer from data dumps.",
    "Listing 3.8 shows an example of an Avro schema defined as JSON. Fields can be created with details such as name, type, and any default values. For example, looking at the field daysOverDue, the schema tells us that the days a book is overdue is an int with a default value of 0. Knowing that this value is numeric and not text (such as one week) helps to create a clear contract for the data producers and consumers.",
    "Listing 3.8  Avro schema example",
    "\"type\" : \"record\",",
    "\"name\" : \"kinaction_libraryCheckout\",",
    "\"fields\" : [{\"name\" : \"materialName\",",
    "\"type\" : \"string\",",
    "\"default\" : \"\"},",
    "{\"name\" : \"daysOverDue\",",
    "JSON-defined Avro schema",
    "Maps directly to a field name",
    "Defines a field with a name, type, and default value",
    "\"type\" : \"int\",",
    "\"default\" : 0},",
    "{\"name\" : \"checkoutDate\",",
    "\"type\" : \"int\", \"logicalType\": \"date\", \"default\" : \"-1\"},",
    "Provides the default value",
    "{\"name\" : \"borrower\", \"type\" : {",
    "\"type\" : \"record\",",
    "\"name\" : \"borrowerDetails\", \"fields\" : [",
    "{\"name\" : \"cardNumber\",",
    "\"type\" : \"string\",",
    "\"default\" : \"NONE\"}",
    "\"default\" : {}",
    "By looking at the example of the Avro schema in listing 3.8, we can see that questions such as \u201cDo we parse the cardNumber as a number or a string (in this case, string)\u201d are easily answered by a developer looking at the schema. Applications could automatically",
    "use this information to generate data objects for this data, which helps to avoid parsing data type errors.",
    "Schemas can be used by tools like Apache Avro to handle data that evolves. Most of us have dealt with altered statements or tools like Liquibase to work around these changes in relational databases. With schemas, we start with the knowledge that our data will probably change.",
    "Do we need a schema when we are first starting with our data designs? One of the main concerns is that if our system\u2019s scale keeps getting larger, will we be able to con- trol the correctness of data? The more consumers we have could lead to a burden on the testing that we would need to do. Besides the growth in numbers alone, we might not even know all of the consumers of that data."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Summary": [
    "Because Kafka is a Scala\u2122 application, it has the ability to use JMX and the Yammer Metrics library [17]. This library is used to provide JMX metrics on various parts of the application, and we have already seen some options we can evaluate. But as Kafka usage has expanded, there are some tools out there that leverage not only JMX met- rics, but also administration-related commands and various other techniques to pro- vide easy-to-manage clusters. Of course, the following section does not have a complete list of options, and the features of those listed might change over time. Nev- ertheless, let\u2019s take a look at a few options that you might want to explore.",
    "The Cluster Manager for Apache Kafka, or CMAK (https://github.com/yahoo/ CMAK), once known as the Kafka Manager, is an interesting project that focuses on managing Kafka as well as being a UI for various administrative activities, and was shared from Yahoo\u2122! One key feature is its ability to manage multiple clusters. Other features include inspection of our overall cluster state and the ability to generate and run partition reassignment. This tool can also deal with authenticating users with LDAP, which might be helpful depending on product requirements for a project [18]. Cruise Control (https://github.com/linkedin/cruise-control) was created by developers at LinkedIn. Because they have thousands of brokers across their clusters, they have experience running Kafka clusters and have helped codify and automate dealing with some of Kafka\u2019s pain points over the years. A REST API can be used as well as the option to use a UI, so we have a couple of ways to interact with this tool. Some of the most interesting features to us are how Cruise Control can watch our clus-",
    "ter and can generate suggestions on rebalancing based on workloads [19].",
    "Confluent Control Center (https://docs.confluent.io/current/control-center/ index.html) is another web-based tool that can help us monitor and manage our clus- ters. But one item to note is that it currently is a commercial feature that would need an enterprise license for a production setup. If you already have a subscription to the Confluent platform, there is no reason not to check it out. This tool uses dashboards and can help identify message failures, network latency, and other external connectors. Overall, Kafka provides us with many options to not only manage but also monitor our cluster. Distributed systems are difficult, and the more experience you gain, the",
    "more your monitoring skills and practices will improve."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>References": [
    "Besides the shell scripts that are packaged with Kafka, an administrative client also exists to provide API access to important tasks such as creating a topic.",
    "Tools such as kcat and the Confluent REST Proxy API allow ways for developers to interact with the cluster.",
    "Although Kafka uses a log for client data at its core, there are still various logs specific to the operation of the broker that we need to maintain. We need to address managing these logs (and ZooKeeper logs) to provide details for trou- bleshooting when needed.",
    "Understanding advertised listeners can help explain behavior that at first appears inconsistent for client connections.",
    "Kafka uses JMX for metrics. You can see metrics from clients (producers and consumers) as well as from brokers.",
    "We can use producer and consumer interceptors to implement crosscutting concerns. One such example would be adding tracing IDs for monitoring mes- sage delivery."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>An example": [
    "1 J. Kreps. \u201cWhy Avro for Kafka Data?\u201d Confluent blog (February 25, 2015). https://www.confluent.io/blog/avro-kafka-data/ (accessed November 23, 2017).",
    "2  \u201cSender.java.\u201d Apache Kafka. GitHub (n.d.). https://github.com/apache/kafka/ blob/299eea88a5068f973dc055776c7137538ed01c62/clients/src/main/java/ org/apache/kafka/clients/producer/internals/Sender.java (accessed August 20, 2021).",
    "3 \u201cProducer Configurations: Retries.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/installation/configuration/producer",
    "-configs.html#producerconfigs_retries (accessed May 29, 2020).",
    "4 \u201cProducer Configurations: max.in.flight.requests.per.connection.\u201d Confluent documentation\t(n.d.).\thttps://docs.confluent.io/platform/current/installa tion/configuration/producer-configs.html#max.in.flight.requests.per.connec tion (accessed May 29, 2020).",
    "5 \u201cProducer Configurations: enable.idempotence.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/configuration",
    "/producer-configs.html#producerconfigs_enable.idempotence (accessed May 29, 2020).",
    "6 \u201cKafkaProducer.\u201d Apache Software Foundation (n.d.). https://kafka.apache",
    ".org/10/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html (accessed July 7, 2019).",
    "7 \u201cProducer Configurations.\u201d Confluent documentation (n.d.). https://docs.con fluent.io/platform/current/installation/configuration/producer-configs.html (accessed May 29, 2020).",
    "8  \u201cProducer Configurations: bootstrap.servers.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/configuration/  producer-configs.html #bootstrap.servers (accessed May 29, 2020).",
    "9 \u201cProducer Configurations: acks.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/installation/configuration/producer",
    "-configs.html#acks (accessed May 29, 2020).",
    "10 \u201cDocumentation: Message Delivery Semantics.\u201d Apache Software Foundation (n.d.). https://kafka.apache.org/documentation/#semantics (accessed May 30, 2020).",
    "11  \u201cTopic Configurations: message.timestamp.type.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/configuration/topic- configs.html#topicconfigs_message.timestamp.type (accessed July 22, 2020).",
    "12 KIP-42: \u201cAdd Producer and Consumer Interceptors,\u201d Wiki for Apache Kafka, Apache Software Foundation. https://cwiki.apache.org/confluence/display/ KAFKA/KIP-42%3A+Add+Producer+and+Consumer+Interceptors (accessed April 15, 2019).",
    "13  \u201cKafka Streams Data Types and Serialization.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/streams/developer-guide/data types.html (accessed August 21, 2021).",
    "14 J. Olshan. \u201cApache Kafka Producer Improvements with the Sticky Partitioner.\u201d Confluent blog (December 18, 2019). https://www.confluent.io/blog/apache",
    "-kafka-producer-improvements-sticky-partitioner/ (accessed August 21, 2021).",
    "15  \u201cDefaultPartitioner.java,\u201d Apache Software Foundation. GitHub (n.d.). https:// github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/ kafka/clients/producer/internals/DefaultPartitioner.java (accessed March 22, 2020).",
    "16 C. McCabe. \u201cUpgrading Apache Kafka Clients Just Got Easier.\u201d Confluent blog (July 18, 2017). https://www.confluent.io/blog/upgrading-apache-kafka-clients",
    "-just-got-easier/ (accessed August 21, 2021).",
    "In our previous chapter, we started writing data into our Kafka system. However, as you know, that is only one part of the story. Consumers get the data from Kafka and provide those values to other systems or applications. Because consumers are cli- ents that exist outside of brokers, they can be written in various programming lan- guages just like producer clients. Take note that when we look at how things work in this chapter, we will try to lean towards the defaults of the Java consumer client. After reading this chapter, we will be on our way to solving our previous business requirements by consuming data in a couple of different ways."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>An example<sec><sec>4.1.1\tProducer notes": [
    "The producer provides the way to push data into the Kafka system for our example project. As a refresher, figure 4.1 illustrates where producers fit into Kafka.",
    "We will focus on our alert producers in this chapter.",
    "Data in (to partition)",
    "Messages can be replayed from the beginning of the log and consumed again.",
    "Partition 0",
    "Data out (from partition)",
    "Logs are append only.",
    "New entries added to the end.",
    "No database storage, just disk.",
    "Each log is made up of entries labeled with offset numbers.",
    "ZooKeeper used for distributed configuration and management",
    "One of the brokers will be a controller.",
    "Figure 4.1  Kafka producers",
    "Looking at figure 4.1, let\u2019s focus on the top-left corner (the producer clients), which shows examples of data being produced into Kafka. This data could be the IoT events we are using in our fictional company. To make the idea of producing data more con- crete, let\u2019s imagine a practical example that we might have written for one of our proj- ects. Let\u2019s look at an application that takes user feedback on how a website is working for its customers.",
    "Currently, the user submits a form on the website that generates email to a support account or chatbot. Every now and then, one of our support staff checks the inbox to see what suggestions or issues customers have encountered. Looking to the future, we want to keep this information coming to us but in a way that allows the data to be more accessible than in an email inbox. If we instead send this message into a Kafka topic, we could produce more robust and varied replies, rather than just reactive email responses to customers. The benefit of flexibility comes from having the event in Kafka for any consuming applications to use.",
    "Let\u2019s first look at what using email as part of our data pipeline impacts. Looking at figure 4.2, it might be helpful to focus on the format that the data is stored in once a user submits a form with feedback on our website.",
    "Data format",
    "Figure 4.2  Sending data in email",
    "A traditional email uses Simple Mail Transfer Protocol (SMTP), and we will see that reflected in how the email event itself is presented and sometimes stored. We can use email clients like Microsoft\u00ae Outlook\u00ae to retrieve the data quickly, but rather than just reading email, how else can we pull data out of that system for other uses? Copy and paste are common manual steps, as well as email-parsing scripts. (Parsing scripts includes using a tool or programming language and libraries or frameworks to get the",
    "parsing correct.) In comparison, although Kafka uses its own protocol, it does not impose any specific format for our message data. We should be able to write the data in whatever format we choose.",
    "NOTE In the previous chapter, we looked at the Apache Avro format as one of the common formats that the Kafka community uses. Protobuf and JSON are also widely popular [1].",
    "Another usage pattern that comes to mind is to treat notifications of customer issues or website outages as temporary alerts that we can delete after replying to the cus- tomer. However, this customer input might serve more than one purpose. What if we are able to look for trends in outages that customers report? Does the site always slow to a crawl after sale coupon codes go out in mass-marketing emails? Could this data help us find features that our users are missing from our site? Do 40% of our user emails involve having trouble finding the Privacy settings for their account? Having this data present in a topic that can be replayed or read by several applications with different purposes can add more value to the customer than an automated support or bot email that is then deleted.",
    "Also, if we have retention needs, those would be controlled by the teams running our email infrastructure versus a configuration setting we can control with Kafka. Look- ing again at figure 4.3, notice that the application has an HTML form but writes to a",
    "Format determined by you.",
    "Data format",
    "Figure 4.3  Sending data to Kafka",
    "Kafka topic, not to an email server. With this approach, we can extract the information that is important for us in whatever format we need, and it can be used in many ways. Consuming applications can use schemes to work with the data and not be tied to a sin- gle protocol format. We can retain and reprocess these messages for new use cases because we control the retention of those events. Now that we have looked at why we might use a producer, let\u2019s quickly check out some details of a producer interacting with the Kafka brokers."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Producer options": [
    "The producer\u2019s job includes fetching metadata about the cluster [2]. Because produc- ers can only write to the replica leader of the partition they are assigned to, the meta- data helps the producer determine which broker to write to as the user might have only included a topic name without any other details. This is nice because the pro- ducer\u2019s end user does not have to make a separate call to get that information. The end user, however, needs to have at least one running broker to connect to, and the Java client library figures out the rest.",
    "Because this distributed system is designed to account for momentary errors such as a network blip, the logic for retries is already built in. However, if the ordering of the messages is essential, like for our audit messages, then besides setting the retries to a number like 3, we also need to set the max.in.flight.requests.per.connection value to 1 and set acks (the number of brokers that send acknowledgments back) to all [3] [4]. In our opinion, this is one of the safest methods to ensure that your pro- ducer\u2019s messages arrive in the order you intend [4]. We can set the values for both acks and retries as configuration parameters.",
    "Another option to be aware of is using an idempotent producer. The term idempo- tent refers to how sending the same message multiple times only results in producing the message once. To use an idempotent producer, we can set the configuration prop- erty enable.idempotence=true [5]. We will not be using the idempotent producer in our following examples.",
    "One thing we do not have to worry about is one producer getting in the way of another producer\u2019s data. Thread safety is not an issue because data will not be over- written but handled by the broker itself and appended to the broker\u2019s log [6]. Now it is time to look at how to enable the values like max.in.flight.requests.per",
    ".connection in code."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Producer options<sec><sec>Configuring the broker list": [
    "One of the things that was interesting when we started working with sending data into Kafka was the ease of setting options using the Java clients that we will specifically focus on in this book. If you have worked with other queue or messaging systems, the other systems\u2019 setups can include things like providing remote and local queues lists, manager hostnames, starting connections, connection factories, sessions, and more.",
    "Although far from being set up totally hassle free, the producer works from the con- figuration on its own to retrieve much of the information it needs, such as a list of all of our Kafka brokers. Using the value from the property bootstrap.servers as a start- ing point, the producer fetches metadata about brokers and partitions that it uses for all subsequent writes.",
    "As mentioned earlier, Kafka allows you to change key behaviors just by changing some configuration values. One way to deal with all of the producer configuration key names is to use the constants provided in the Java class ProducerConfig when devel- oping producer code (see http://mng.bz/ZYdA) and by looking for the Importance label of \u201chigh\u201d in the Confluent website [7]. However, in our examples, we will use the property names themselves for clarity.",
    "Table 4.1 lists some of the most crucial producer configurations that support our specific examples. In the following sections, we'll look at what we need to complete our factory work.",
    "Table 4.1  Important producer configurations"
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Producer options<sec><sec>How to go fast (or go safer)": [
    "From our examples of writing messages to Kafka, it is clear that we have to tell the pro- ducer which topic to send messages to. Recall that topics are made up of partitions, but how does Kafka know where a topic partition resides? We, however, do not have to know the details of those partitions when we send messages. Perhaps an illustration will help clarify this conundrum. One of the required configuration options for pro- ducers is bootstrap.servers. Figure 4.4 shows an example of a producer that has only broker 0 in its list of bootstrap servers, but it will be able to learn about all three brokers in the cluster by starting with one only.",
    "The bootstrap.servers property can take many or just one initial broker as in fig- ure 4.4. By connecting to this broker, the client can discover the metadata it needs, which includes data about other brokers in the cluster as well [8].",
    "Producer connects to bootstrap servers",
    "Broker 0\tBroker 1\tBroker 2",
    "Metadata sent back to producer letting it know its leader resides on Broker 2, which it did not know about at first. Kafka knows about its other brokers.",
    "Our alert producers connect to our servers since they are local on different ports on localhost.",
    "Figure 4.4  Bootstrap servers",
    "This configuration is key to helping the producer find a broker to talk to. Once the producer is connected to the cluster, it can obtain the metadata it needs to get the details (such as where the leader replica for the partition resides on disk) we did not previously provide. Producer clients can also overcome a failure of the partition leader they are writing to by using the information about the cluster to find a new leader. You might have noticed that ZooKeeper\u2019s information is not part of the config- uration. Any metadata the producer needs will be handled without the producer cli- ent having to provide ZooKeeper cluster details."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Producer options<sec><sec>Timestamps": [
    "Asynchronous message patterns are one reason that many use queue-type systems, and this powerful feature is also available in Kafka. We can wait in our code for the result of a producer send request, or we can handle success or failure asynchronously with callbacks or Future objects. If we want to go faster and not wait for a reply, we can still handle the results at a later time with our own custom logic.",
    "Another configuration property that applies to our scenario is the acks key, which stands for acknowledgments. This controls how many acknowledgments the producer needs to receive from the partition leader\u2019s followers before it returns a completed request. The valid values for this property are all, -1, 1, and 0 [9].",
    "Figure 4.5 shows how a message with ack set to 0 behaves. Setting this value to 0 will probably get us the lowest latency but at the cost of safety. Additionally, guarantees are not made if any broker receives the message and, also, retries are not attempted [9]. As a sample use case, say that we have a web-tracking platform that collects the clicks on a page and sends these events to Kafka. In this situation, it might not be a big deal to lose a single link press or hover event. If one is lost, there is no real business impact.",
    "In essence, the event in figure 4.5 was sent from the producer and forgotten. The message might have never made it to the partition. If the message did, by chance, make it to the leader replica, the producer will not know if any follower replica copies were successful.",
    "The producer writes to the leader of the partition.",
    "The leader doesn\u2019t wait to find out if the write was successful.",
    "Because we do not know if the leader write was successful, we are not aware of the state of any replica copies and if they were successful or not.",
    "This is not what we want for our kinaction_audit producer.",
    "Figure 4.5  The property acks equals 0.",
    "What we would consider the opposite setting to that used previously would be acks with values all or -1. The values all or -1 are the strongest available option for this configuration setting. Figure 4.6 shows how the value all means that a partition leader\u2019s replica waits on the entire list of its in-sync replicas (ISRs) to acknowledge completion [9]. In other words, the producer will not get an acknowledgment of suc- cess until after all replicas for a partition are successful. It is easy to see that it won\u2019t be the quickest due to the dependencies it has on other brokers. In many cases, it is",
    "The producer writes to the leader of the partition.",
    "3. The producer receives notification when all of the replicas are updated.",
    "Figure 4.6  The property",
    "acks equals all.",
    "worth paying the performance price in order to prevent data loss. With many brokers in a cluster, we need to be aware of the number of brokers the leader has to wait on. The broker that takes the longest to reply is the determining factor for how long until a producer receives a success message.",
    "Figure 4.7 shows the impact of setting the acks value to 1 and asking for an acknowledgment. An acknowledgment involves the receiver of the message (the leader replica of the specific partition) sending confirmation back to the producer. The producer client waits for that acknowledgment. However, the followers might not have copied the message before a failure brings down the leader. If that situation occurs before a copy is made, the message never appears on the replica followers for that partition [9]. Figure 4.7 shows that while the message was acknowledged by the leader replica and sent to the producer, a failure of any replica to make a copy of the message would appear as if the message never made it to the cluster.",
    "1. The producer writes to the leader of the partition.",
    "3. Before the leader that has success has time to copy the message to any follower replicas, the leader fails. This means that the message could be",
    "lost to the remaining brokers.",
    "These brokers never see the message even though it was seen by the leader.",
    "Figure 4.7  The property acks equals 1.",
    "NOTE  This is closely related to the ideas of at-most and at-least semantics that we covered in chapter 1 [10]. The acks setting is a part of that larger picture."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Generating code for our requirements": [
    "Recent versions of the producer record contain a timestamp on the events you send. A user can either pass the time into the constructor as a Java type long when sending a ProducerRecord Java object or the current system time. The actual time that is used in",
    "the message can stay as this value, or it can be a broker timestamp that occurs when the message is logged. Setting the topic configuration message.timestamp.type to CreateTime uses the time set by the client, whereas setting it to LogAppendTime uses the broker time [11].",
    "Why would you want to choose one over the other? You might want to use the cre- ated time in order to have the time that a transaction (like a sales order) takes place rather than when it made its way to the broker. Using the broker time can be useful when the created time is handled inside the message itself or an actual event time is not business or order relevant.",
    "As always, timestamps can be tricky. For example, we might get a record with an earlier timestamp than that of a record before it. This can happen in cases where a failure occurred and a different message with a later timestamp was committed before the retry of the first record completed. The data is ordered in the log by offsets and not by timestamp. Although reading timestamped data is often thought of as a con- sumer client concern, it is also a producer concern because the producer takes the first steps in ensuring message order.",
    "As discussed earlier, this is also why max.in.flight.requests.per.connection is important when considering whether you want to allow retries or many inflight requests at a time. If a retry happens and other requests succeed on their first attempt, earlier messages might be added after the later ones. Figure 4.8 provides an example of when a message can get out of order. Even though message 1 was sent first, it does not make it into the log in an ordered manner because retries were enabled.",
    "As a reminder, with Kafka versions before 0.10, timestamp information is not avail- able as that feature was not included in earlier releases. We can still include a time- stamp, though, but we would need to store it in the value of the message itself.",
    "Message 1 is sent and fails.",
    "Leader partition broker",
    "Message 2 is sent and succeeds.",
    "Message 1 is resent and appended to the leader log after message 2.",
    "Message 1 retry",
    "We used this retry logic to determine if we need ordering with our alert events from chapter 3 and for topic kinaction_audit.",
    "Figure 4.8  Retry impact on order",
    "Another option when using a producer is to create producer interceptors. These were introduced in KIP-42 (Kafka Improvement Proposal). Its main goal was to help sup- port measuring and monitoring [12]. In comparison to using a Kafka Streams work- flow to filter or aggregate data, or even creating different topics specifically for modified data, the usage of these interceptors might not be our first choice. At pres- ent, there are no default interceptors that run in the life cycle. In chapter 9, we will show a use case for tracing messages from producer clients to consumer clients with interceptors adding a trace ID."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Generating code for our requirements<sec><sec>Client and broker versions": [
    "Let\u2019s try to use the information we gathered about how producers work on our own solutions. We\u2019ll start with the audit checklist that we designed in chapter 3 for use with Kafka in our e-bike factory. As noted in chapter 3, we want to make sure that we do not lose any audit messages when operators complete commands against the sensors. One requirement was that there was no need to correlate (or group together) any events. Another requirement was to make sure we don\u2019t lose any messages. The following list- ing shows how we would start our producer configuration and how to make sure that we are safe for message acknowledgment by setting acks to all.",
    "Listing 4.1  Configuring the audit producer",
    "public class AuditProducer {",
    "Creates properties as before for our configuration",
    "private static final Logger log = LoggerFactory.getLogger (AuditProducer.class);Properties kaProperties = new Properties();",
    "kaProperties.put( \"bootstrap.servers\", \"localhost:9092,localhost:9093,localhost:9094\");",
    "kaProperties.put(\"acks\", \"all\"); kaProperties.put(\"retries\", \"3\");",
    "Sets acks to all to get the strongest guarantee",
    "kaProperties.put(\"max.in.flight.requests.per.connection\", \"1\");",
    "Lets the client retry in case of failure so we don\u2019t have to implement our own failure logic",
    "Notice that we did not have to touch anything except the configuration we send to the producer to address the concern of message loss. The acks configuration change is a small but powerful feature that has a significant impact on if a message arrives or not. Because we do not have to correlate (group) any events together, we are not using a key for these messages. However, there is a foundational part that we want to change in order to wait for the result before moving on. The following listing shows the get method, which is how we can bring about waiting for the response to complete syn- chronously before moving on in the code. Note that the following listing was",
    "informed by examples located at: https://docs.confluent.io/2.0.0/clients/producer",
    ".html#examples.",
    "Listing 4.2  Waiting for a result",
    "RecordMetadata result = producer.send(producerRecord).get();",
    "Waits on the response from the send call",
    "log.info(\"kinaction_info offset = {}, topic = {}, timestamp = {}\", result.offset(), result.topic(), result.timestamp());",
    "producer.close();",
    "Waiting on the response directly in a synchronous way ensures that the code is han- dling each record\u2019s results as they come back before another message is sent. The focus is on delivering the messages without loss, more than on speed!",
    "So far, we have used a couple of prebuilt serializers in earlier chapters. For plain text messages, our producer uses a serializer called StringSerializer. And when we talked about Avro in chapter 3, we reached for the class io.confluent.kafka",
    ".serializers.KafkaAvroSerializer. But what if we have a specific format we want to produce? This often happens when trying to work with custom objects. We\u2019ll use seri- alization to translate data into a format that can be transmitted, stored, and then retrieved to achieve a clone of our original data. The following listing shows the code for our Alert class.",
    "Listing 4.3  Alert class",
    "public class Alert implements Serializable { private final int alertId;",
    "private String stageId;",
    "private final String alertLevel; private final String alertMessage;",
    "public Alert(int alertId, String stageId,",
    "String alertLevel, String alertMessage) {",
    "Holds the alert\u2019s ID, level, and messages",
    "this.alertId = alertId; this.stageId = stageId; this.alertLevel = alertLevel; this.alertMessage = alertMessage;",
    "public int getAlertId() { return alertId;",
    "public String getStageId() { return stageId;",
    "public void setStageId(String stageId) { this.stageId = stageId;",
    "public String getAlertLevel() { return alertLevel;",
    "public String getAlertMessage() { return alertMessage;",
    "Listing 4.3 shows code that we use to create a bean named Alert to hold the informa- tion we want to send. Those familiar with Java will notice that the listing is nothing more than getters and setters and a constructor for the Alert class. Now that there is a format for the Alert data object, it is time to use it in making a simple alert Serial- izer called AlertKeySerde as the following listing shows.",
    "Listing 4.4  Our Alert serializer",
    "public class AlertKeySerde implements Serializer<Alert>,",
    "Deserializer<Alert> {",
    "public byte[] serialize(String topic, Alert key) { if (key == null) {",
    "return null;",
    "Sends the topic and the Alert object to our method",
    "return key.getStageId()",
    ".getBytes(StandardCharsets.UTF_8);",
    "Converts objects to bytes (our end goal)",
    "public Alert deserialize",
    "(String topic, byte[] value) {",
    "//could return Alert in future if needed return null;",
    "The rest of the interface methods do",
    "not need any logic at this point.",
    "In listing 4.5, we use this custom class only as the key serializer for the moment, leav- ing the value serializer as a StringSerializer. It is interesting to note that we can seri- alize keys and values with different serializers on the same message. But we should be mindful of our intended serializers and the configuration values for both. The code implements the Serializer interface and only pulls out the field stageId to use as a key for our message. This example should be straightforward because the focus is on the technique of using a serde. Other options for serdes that are often used are JSON and Avro implementations.",
    "NOTE  If you see or hear the term serde, it means that the serializer and deseri- alizer are both handled by the same implementation of that interface [13].",
    "However, it is still common to see each interface defined separately. Just watch when you use StringSerializer versus StringDeserializer; the dif- ference can be hard to spot!",
    "Another thing to keep in mind is that knowing how to deserialize the values involves the consumers in relation to how the values were serialized by the producer. Some sort of agreement or coordinator is needed for the data formats for clients even though Kafka does not care what data it stores on the brokers.",
    "Another goal of our design for the factory was to capture the alert trend status of our stages to track their alerts over time. Because we care about the information for each stage (and not all sensors at a time), it might be helpful to think of how we are going to group these events. In this case, as each stage ID is unique, it makes sense that we can use that ID as a key. The following listing shows the key.serializer prop- erty that we\u2019ll set, as well as sending a CRITICAL alert.",
    "Listing 4.5  Alert trending producer",
    "public class AlertTrendingProducer { private static final Logger log =",
    "LoggerFactory.getLogger(AlertTrendingProducer.class);",
    "public static void main(String[] args)",
    "throws InterruptedException, ExecutionException {",
    "Properties kaProperties = new Properties(); kaProperties.put(\"bootstrap.servers\",",
    "\"localhost:9092,localhost:9093,localhost:9094\"); kaProperties.put(\"key.serializer\",",
    "AlertKeySerde.class.getName()); kaProperties.put(\"value.serializer\",",
    "Tells our producer client how to serialize our custom Alert object into a key",
    "\"org.apache.kafka.common.serialization.StringSerializer\");",
    "try (Producer<Alert, String> producer = new KafkaProducer<>(kaProperties)) {",
    "Alert alert = new Alert(0, \"Stage 0\", \"CRITICAL\", \"Stage 0 stopped\"); ProducerRecord<Alert, String> producerRecord =",
    "new ProducerRecord<>(\"kinaction_alerttrend\", alert, alert.getAlertMessage());",
    "RecordMetadata result = producer.send(producerRecord).get(); log.info(\"kinaction_info offset = {}, topic = {}, timestamp = {}\",",
    "result.offset(), result.topic(), result.timestamp());",
    "}\tInstead of null for the second",
    "}\tparameter, uses the actual object",
    "we want to populate the key",
    "In general, the same key should produce the same partition assignment, and nothing will need to be changed. In other words, the same stage IDs (the keys) are grouped",
    "together just by using the correct key. We will keep an eye on the distribution of the size of the partitions to note if they become uneven in the future, but for now, we will go along with this. Also, note that for our specific classes that we created in the manuscript, we are setting the class properties in a different way to show a different option. Instead of hardcoding the entire path of the class, you can use something like AlertKey- Serde.class.getName() or even AlertKeySerde.class for the value of the property. Our last requirement was to have alerts quickly processed to let operators know about any critical outages so we can group by the stage ID in this case as well. One rea- son for doing this is that we can tell if a sensor failed or recovered (any state change) by looking at only the last event for that stage ID. We do not care about the history of the status checks, only the current scenario. In this case, we also want to partition our",
    "So far in our examples of writing to Kafka, the data was directed to a topic with no additional metadata provided from the client. Because the topics are made up of par- titions that sit on the brokers, Kafka provides a default way to send messages to a spe- cific partition. The default for a message with no key (which we used in the examples thus far) was a round-robin assignment strategy prior to Kafka version 2.4. In versions after 2.4, messages without keys use a sticky partition strategy [14]. However, some- times we have specific ways that we want our data to be partitioned. One way to take control of this is to write our own unique partitioner class.",
    "The client also has the ability to control what partition it writes to by configuring a unique partitioner. One example to think about is the alert levels from our sensor- monitoring service that was discussed in chapter 3. Some sensors\u2019 information might be more important than others; these might be on the critical path of our e-bike, which would cause downtime if not addressed. Let\u2019s say we have four levels of alerts: Critical, Major, Minor, and Warning. We could create a partitioner that places the dif- ferent levels in different partitions. Our consumer clients would always make sure to read the critical alerts before processing the others.",
    "If our consumers keep up with the messages being logged, critical alerts probably would not be a huge concern. However, listing 4.6 shows that we could change the partition assignment with a class to make sure that our critical alerts are directed to a specific partition (like partition 0). (Note that other alerts could end up on partition 0 as well due to our logic, but that critical alerts will always end up there.) The logic mirrors an example of the DefaultPartitioner used in Kafka itself [15].",
    "Listing 4.6  Partitioner for alert levels",
    "public int partition(final String topic",
    "AlertLevelPartitioner needs to implement the partition method for its core logic.",
    "int criticalLevelPartition = findCriticalPartitionNumber(cluster, topic);",
    "return isCriticalLevel(((Alert) objectKey).getAlertLevel()) ? criticalLevelPartition :",
    "findRandomPartition(cluster, topic, objectKey);",
    "}\tCritical alerts should end up",
    "the partition returned from findCriticalPartitionNumber",
    "By implementing the Partitioner interface, we can use the partition method to send back the specific partition we want our producer to write to. In this case, the value of the key ensures that any CRITICAL event makes it to a specific place, partition 0 can be imag- ined to be sent back from the method findCriticalPartitionNumber, for example. In addition to creating the class itself, listing 4.7 shows how we need to set the configura- tion key, partitioner.class, for our producer to use the specific class we created. The configuration that powers Kafka is used to leverage our new class.",
    "Listing 4.7  Configuring the partitioner class",
    "Properties kaProperties = new Properties();",
    "//... kaProperties.put(\"partitioner.class\",",
    "Updates the producer configuration to reference and use the custom partitioner AlertLevelPartitioner",
    "AlertLevelPartitioner.class.getName());",
    "This example, in which a specific partition number is always sent back, can be expanded on or made even more dynamic. We can use custom code to accomplish the specific logic of our business needs.",
    "Listing 4.8 shows the configuration of the producer to add the partitioner.class value to use as our specific partitioner. The intention is for us to have the data avail- able in a specific partition, so consumers that process the data can have access to the critical alerts specifically and can go after other alerts (in other partitions) when they are handled.",
    "Listing 4.8  Alert producer",
    "public class AlertProducer {",
    "public static void main(String[] args) {",
    "Properties kaProperties = new Properties(); kaProperties.put(\"bootstrap.servers\",",
    "\"localhost:9092,localhost:9093\"); kaProperties.put(\"key.serializer\",",
    "AlertKeySerde.class.getName()); kaProperties.put(\"value.serializer\",",
    "Reuses the Alert key serializer",
    "\"org.apache.kafka.common.serialization.StringSerializer\"); kaProperties.put(\"partitioner.class\",",
    "AlertLevelPartitioner.class.getName());",
    "try (Producer<Alert, String> producer = new KafkaProducer<>(kaProperties)) {",
    "Uses the property partitioner.class to set our specific partitioner class",
    "Alert alert = new Alert(1, \"Stage 1\", \"CRITICAL\", \"Stage 1 stopped\"); ProducerRecord<Alert, String>",
    "producerRecord = new ProducerRecord<> (\"kinaction_alert\", alert, alert.getAlertMessage());",
    "producer.send(producerRecord,",
    "new AlertCallback());",
    "This is the first time we\u2019ve used a callback to handle the completion or failure of a send.",
    "One addition we see in listing 4.8 is how we added a callback to run on completion. Although we said that we are not 100% concerned with message failures from time to time, due to the frequency of events, we want to make sure that we do not see a high failure rate that could be a hint at application-related errors. The following listing shows an example of implementing a Callback interface. The callback would log a message only if an error occurs. Note that the following listing was informed by exam- ples located at https://docs.confluent.io/2.0.0/clients/producer.html#examples.",
    "Listing 4.9  Alert callback",
    "public class AlertCallback implements Callback {",
    "private static final Logger log = LoggerFactory.getLogger(AlertCallback.class);",
    "Implements the Kafka Callback interface",
    "public void onCompletion (RecordMetadata metadata,",
    "Exception exception) {",
    "The completion can have success or failure.",
    "if (exception != null) { log.error(\"kinaction_error\", exception);",
    "log.info(\"kinaction_info offset = {}, topic = {}, timestamp = {}\", metadata.offset(), metadata.topic(), metadata.timestamp());",
    "Although we will focus on small samples in most of our material, we think that it is helpful to look at how to use a producer in a real project as well. As mentioned earlier, Apache Flume can be used alongside Kafka to provide various data features. When we use Kafka as a sink, Flume places data into Kafka. You might (or might not) be famil- iar with Flume, but we are not interested in its feature set for this. We want to see how it leverages Kafka producer code in a real situation.",
    "In the following examples, we reference Flume version 1.8 (located at https:// github.com/apache/flume/tree/flume-1.8, if you want to view more of the complete source code). The following listing shows a configuration snippet that would be used by a Flume agent.",
    "Listing 4.10  Flume sink configuration",
    "a1.sinks.k1.kafka.topic = kinaction_helloworld a1.sinks.k1.kafka.bootstrap.servers = localhost:9092 a1.sinks.k1.kafka.producer.acks = 1 a1.sinks.k1.kafka.producer.compression.type = snappy",
    "Some configuration properties from listing 4.10 seem familiar: topic, acks, boot- strap.servers. In our previous examples, we declared the configurations as proper- ties inside our code. However, listing 4.10 shows an example of an application that externalizes the configuration values, which is something we could do on our projects as well. The KafkaSink source code from Apache Flume, found at http://mng.bz/JvpZ, provides an example of taking data and placing it inside Kafka with producer code. The following listing is a different example of a producer using a similar idea, taking a con- figuration file like that in listing 4.10 and loading those values into a producer instance.",
    "Listing 4.11  Reading the Kafka producer configuration from a file",
    "Properties kaProperties = readConfig();",
    "String topic = kaProperties.getProperty(\"topic\"); kaProperties.remove(\"topic\");",
    "try (Producer<String, String> producer =",
    "new KafkaProducer<>(kaProperties)) { ProducerRecord<String, String> producerRecord =",
    "new ProducerRecord<>(topic, null, \"event\"); producer.send(producerRecord,",
    "new AlertCallback());",
    "Our familiar producer.send with a callback",
    "private static Properties readConfig() {",
    "Path path = Paths.get(\"src/main/resources/kafkasink.conf\");",
    "Properties kaProperties = new Properties(); try (Stream<String> lines = Files.lines(path))",
    "lines.forEachOrdered(line ->",
    "Reads an external file for configuration",
    "determineProperty(line, kaProperties));",
    "} catch (IOException e) { System.out.println(\"kinaction_error\" + e);",
    "return kaProperties;",
    "private static void determineProperty (String line, Properties kaProperties) { if (line.contains(\"bootstrap\")) {",
    "Parses configuration properties and sets those values",
    "kaProperties.put(\"bootstrap.servers\", line.split(\"=\")[1]);",
    "} else if (line.contains(\"acks\")) { kaProperties.put(\"acks\", line.split(\"=\")[1]);",
    "} else if (line.contains(\"compression.type\")) { kaProperties.put(\"compression.type\", line.split(\"=\")[1]);",
    "} else if (line.contains(\"topic\")) {",
    "kaProperties.put(\"topic\", line.split(\"=\")[1]);",
    "Although some code is omitted in listing 4.11, the core Kafka producer pieces might be starting to look familiar. Setting the configuration and the producer send method should all look like the code we wrote in this chapter. And now, hopefully, you have the confidence to dig into which configuration properties were set and what impacts they will have.",
    "One exercise left for the reader would be to compare how AlertCallback.java stacks up to the Kafka Sink callback class SinkCallback, located in the source code at http://mng.bz/JvpZ. Both examples uses the RecordMetadata object to find more information about successful calls. This information can help us learn more about where the producer message was written, including the partition and offset within that specific partition.",
    "It is true that you can use applications like Flume without ever having to dig into its source code and still be successful. However, we think that if you want to know what is going on internally or need to do some advanced troubleshooting, it is important to know what the tools are doing. With your new foundational knowledge of producers, it should be apparent that you can make powerful applications using these techniques yourself."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>An example<sec><sec>Consumer options": [
    "The consumer client is the program that subscribes to the topic or topics that interest them [1]. As with producer clients, the actual consumer processes can run on sepa- rate machines and are not required to run on a specific server. In fact, most consumer clients in production settings are on separate hosts. As long as the clients can connect to the Kafka brokers, they can read messages. Figure 5.1 reintroduces the broad scope of Kafka and shows consumers running outside the brokers to get data from Kafka.",
    "Why is it important to know that the consumer is subscribing to topics (pulling messages) and not being pushed to instead? The power of processing control shifts to the consumer in this situation. Figure 5.1 shows where consumer clients fit into the overall Kafka ecosystem. Clients are responsible for reading data from topics and",
    "Like our kinaction_alert producer",
    "Data in (to partition)",
    "Messages can be replayed from the beginning of the log and consumed again.",
    "Partition 0",
    "Like our kinaction_alert consumer",
    "Data out (from partition)",
    "Logs are append only.",
    "New entries added to the end.",
    "No database storage, just disk.",
    "Each log is made up of entries labeled with offset numbers.",
    "ZooKeeper used for distributed configuration and management",
    "One of the brokers will be a controller.",
    "Figure 5.1  Overview of Kafka consumer clients",
    "making it available to application (like metrics dashboards or analytics engines) or storing it in other systems. Consumers themselves control the rate of consumption.",
    "With consumers in the driver\u2019s seat, if a failure occurs and the consumer applica- tions come back online, they can start pulling again. There\u2019s no need to always have the consumers up and running to handle (or miss) notifications. Although you can develop applications that are capable of handling this constant data flow or even a buildup of back pressure due to volume, you need to know that you are not a listener for the brokers; consumers are the ones pulling the data. For those readers that have used Kafka before, you might know that there are reasons why you probably will not want to have your consumers down for extended periods. When we discuss more details about topics, we will look at how data might be removed from Kafka due to size or time limits that users can define."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>An example<sec><sec>Understanding our coordinates": [
    "In our discussion, you will notice a couple of properties that are related to the ones that were needed for the producer clients as well. We always need to know the brokers we can attempt to connect to on client startup. One minor \u201cgotcha\u201d is to make sure you use the deserializers for the keys and values that match the serializers you pro- duced the message with. For example, if you produce using a StringSerializer but try to consume using the LongDeSerializer, you will get an exception that you will need to fix.",
    "Table 5.1 lists some of the configuration values that we should know as we start writing our own consumers [2].",
    "Table 5.1  Consumer configuration",
    "One way to deal with all of the consumer configuration key names is to use the con- stants provided in the Java class ConsumerConfig (see http://mng.bz/oGgy) and by looking for the Importance label of \u201chigh\u201d in the Confluent website (http:// mng.bz/drdv). However, in our examples, we will use the property names themselves for clarity. Listing 5.1 shows four of these keys in action. The values for the configura- tions in table 5.1 determine how our consumer interacts with the brokers as well as other consumers.",
    "We will now switch to reading from a topic with one consumer as we did in chapter",
    "2. For this example, we have an application similar to how Kafka could have started in LinkedIn, dealing with user activity events (mentioned in chapter 1) [3]. Let\u2019s say that we have a specific formula that uses the time a user spends on the page as well as the number of interactions they have, which is sent as a value to a topic to project future click rates with a new promotion. Imagine that we run the consumer and process all of the messages on the topic and that we are happy with our application of the formula (in this case, multiplying by a magic number).",
    "Listing 5.1 shows an example of looking at the records from the topic kinaction",
    "_promos and printing a value based on the data from each event. This listing has many similarities to the producer code that we wrote in chapter 4, where properties are used to determine the behavior of the consumer. This use of deserializers for the keys and values is different than having serializers for producers, which varies depending on the topic we consume.",
    "NOTE Listing 5.1 is not a complete code listing but is meant to highlight spe- cific consumer lines. Remember, a consumer can subscribe to multiple topics, but in this instance, we are only interested in the kinaction_promos topic.",
    "In the listing, a loop is also used to poll the topic partitions that our consumer is assigned in order to process messages. This loop is toggled with a Boolean value. This sort of loop can cause errors, especially for beginner programmers! Why this loop then? Part of the streaming mindset encompasses events as a continuous stream, and this is reflected in the logic. Notice that this example uses 250 for the value of the poll dura- tion, which is in milliseconds. This timeout indicates how long the call blocks a main application thread by waiting, but it can return immediately when records are ready for delivery [4]. This value is something that you can fine-tune and adjust, based on the needs of your applications. The reference (and more details) for the Java 8 style of using addShutdownHook we use in the following listing can be seen at https://docs",
    ".confluent.io/platform/current/streams/developer-guide/write-streams.html.",
    "Listing 5.1  Promotion consumer",
    "private volatile boolean keepConsuming = true;",
    "public static void main(String[] args) { Properties kaProperties = new Properties(); kaProperties.put(\"bootstrap.servers\",",
    "Defines group.id. (We\u2019ll",
    "discuss this with consumer groups.)",
    "\"localhost:9092,localhost:9093,,localhost:9094\"); kaProperties.put(\"group.id\",",
    "\"kinaction_webconsumer\"); kaProperties.put(\"enable.auto.commit\", \"true\"); kaProperties.put(\"auto.commit.interval.ms\", \"1000\"); kaProperties.put(\"key.deserializer\",",
    "\"org.apache.kafka.common.serialization.StringDeserializer\"); kaProperties.put(\"value.deserializer\",",
    "\"org.apache.kafka.common.serialization.StringDeserializer\");",
    "Defines deserializers for the key and values",
    "WebClickConsumer webClickConsumer = new WebClickConsumer(); webClickConsumer.consume(kaProperties);",
    "Runtime.getRuntime()",
    ".addShutdownHook(",
    "new Thread(webClickConsumer::shutdown)",
    "private void consume(Properties kaProperties) { try (KafkaConsumer<String, String> consumer =",
    "new KafkaConsumer<>(kaProperties)) { consumer.subscribe(",
    "List.of(\"kinaction_promos\")",
    "while (keepConsuming) {",
    "Passes the properties into the KafkaConsumer constructor",
    "Subscribes to one topic, kinaction_promos",
    "Uses a loop to poll",
    "ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(250));",
    "for topic records",
    "for (ConsumerRecord<String, String> record : records) { log.info(\"kinaction_info offset = {}, key = {}\",",
    "record.offset(), record.key());",
    "log.info(\"kinaction_info value = {}\", Double.parseDouble(record.value()) * 1.543);",
    "private void shutdown() { keepConsuming = false;",
    "After generating a value for every message in the topic in listing 5.1, we find out that our modeling formula isn\u2019t correct! So what should we do now? Attempt to recalcu- late the data we have from our end results (assuming the correction would be harder than in the example) and then apply a new formula?",
    "This is where we can use our knowledge of consumer behavior in Kafka to replay the messages we already processed. By having the raw data retained, we do not have to worry about trying to recreate the original data. Developer mistakes, application logic mistakes, and even dependent application failures can be corrected because the data is not removed from our topics once it is consumed. This also explains how time travel, in a way, is possible with Kafka.",
    "Let\u2019s switch to looking at how to stop our consumer. You already saw where you used Ctrl-C to end your processing or stopped the process on the terminal. However, the proper way includes calling a close method on the consumer [23].",
    "Listing 5.2 shows a consumer that runs on a thread and a different class controls shutdown. When the code in listing 5.2 is started, the thread runs with a consumer instance. By calling the public method shutdown, a different class can flip the Boolean and stop our consumer from polling for new records. The stopping variable is our guard, which decides whether to continue processing or not. Calling the wakeup method also causes a WakeupException to be thrown that leads to the final block clos- ing the consumer resource correctly [5]. Listing 5.2 used https://kafka.apache.org/ 26/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html as a reference documentation.",
    "Listing 5.2  Closing a consumer",
    "public class KinactionStopConsumer implements Runnable { private final KafkaConsumer<String, String> consumer; private final AtomicBoolean stopping =",
    "new AtomicBoolean(false);",
    "public KinactionStopConsumer(KafkaConsumer<String, String> consumer) { this.consumer = consumer;",
    "public void run() { try {",
    "consumer.subscribe(List.of(\"kinaction_promos\")); while (!stopping.get()) {",
    "ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(250));",
    "The variable stopping determines whether to continue processing.",
    "} catch (WakeupException e) {",
    "if (!stopping.get()) throw e;",
    "} finally {",
    "The client shutdown hook triggers WakeupException.",
    "consumer.close();",
    "Stops the client and informs the broker of the shutdown",
    "public void shutdown() { stopping.set(true); consumer.wakeup();",
    "Calls shutdown from a different thread to stop the client properly",
    "As we move on to the next topic, to go further, we need to understand offsets and how they can be used to control how consumers will read data."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>How consumers interact": [
    "One of the items that we have only talked about in passing so far is the concept of off- sets. We use offsets as index positions in the log that the consumer sends to the broker.",
    "This lets the log know which messages it wants to consume and from where. If you think back to our console consumer example, we used the flag --from-beginning. This sets the consumer\u2019s configuration parameter auto.offset.reset to earliest behind the scenes. With that configuration, you should see all the records for that topic for the partitions you are connected to, even if they were sent before you started the console consumer. The top part of figure 5.2 shows reading from the start of the log every time you run in this mode.",
    "From beginning, reads starting at 0",
    "Example kinaction_alert offset numbers",
    "Latest reads start on next message. Offset numbers do not change.",
    "Figure 5.2  Kafka offsets [6]",
    "If you don\u2019t add the option auto.offset.reset, the default is latest. Figure 5.2 shows this mode as well. In this case, you will not see any messages from the producer unless you send them after you start the consumer. This option says to disregard pro- cessing the messages that already are in the topic partition your consumer is reading from; we only want to process what comes in after the consumer client starts polling the topic. You can think of this as an infinite array that has an index starting at 0. How- ever, there are no updates allowed for an index. Any changes need to be appended to the end of the log.",
    "Note that offsets always increase for each partition. Once a topic partition has seen offset 0, even if that message is removed at a later point, the offset number is not used again. Some of you might have run into the issue of numbers that keep increasing until they hit the upper bound of a data type. Each partition has its own offset sequence, so the hope is that the risk will be low.",
    "For a message written to a topic, what are the coordinates to find the message? First, we would find the partition within the topic that it was written to, and then we would find the index-based offset. As figure 5.3 shows, consumers usually read from the consumer\u2019s partition leader replica. This consumer leader replica could be differ- ent from any producer\u2019s leader replica due to changes in leadership over time; how- ever, they are generally similar in concept.",
    "Topic: 3 partitions, 2 replicas",
    "Partition 1",
    "Partition 2",
    "Each partition has its own log sequence. Consumers will only read from partition leaders.",
    "We will read from kinaction_alert later in",
    "this chapter from its leader.",
    "Figure 5.3  Partition leaders",
    "Also, when we talk about partitions, it is okay to have the same offset number across partitions. The ability to tell messages apart needs to include the details of which par- tition we are talking about within a topic, as well as the offset.",
    "As a side note, if you do need to fetch from a follower replica due to an issue like network latency concerns (for example, having a cluster that stretches across data cen- ters), KIP-392 introduced this ability in version 2.4.0 [7]. As you are starting out with your first clusters, we recommend starting with the default behavior and only reaching for this feature as it becomes necessary to impart a real impact. If you do not have your cluster across different physical sites, you likely will not need this feature at the current time.",
    "Partitions play an important role in how we can process messages. Although the topic is a logical name for what your consumers are interested in, they will read from the leader replicas of their assigned partitions. But how do consumers figure out which partition to connect to? And not just which partition, but where does the leader exist for that partition? For each group of consumers, a specific broker takes on the role of being a group coordinator [8]. The consumer client talks to this coordinator in order to get a partition assignment along with other details it needs in order to con- sume messages.",
    "The number of partitions also comes into play when talking about consumption. Some consumers will not get any work with more consumers than partitions. An exam- ple would be four consumers and only three partitions. Why might you be okay with that? In some instances, you might want to make sure that a similar rate of consumption occurs if a consumer dies unexpectedly. The group coordinator is not only in charge of assigning which consumers read which partitions at the beginning of group startup but",
    "also when consumers are added or fail and exit the group [8]. And, in an instance where there are more partitions than consumers, consumers handle more than one partition if needed.",
    "Figure 5.4 shows a generic view of how four consumers read all of the data on the brokers where the subscribed topic has partition leader replicas spread evenly, with one on each of the three brokers. In this figure, the data is roughly the same size, which might not always be the case. One consumer sits ready without work because each partition leader replica is handled by one consumer only.",
    "This consumer reads one section of the total data.",
    "This consumer reads one section of the total data.",
    "Our examples in this book usually only have 3 partitions, so 4 consumers",
    "would have 1 be without work.",
    "This consumer sits ready but does not read any data.",
    "This consumer reads one section of the total data.",
    "Figure 5.4  An extra Kafka consumer",
    "Because the number of partitions determines the amount of parallel consumers you can have, some might ask why you don\u2019t always choose a large number such as 500 partitions. This quest for higher throughput is not free [9]. This is why you need to choose what best matches the shape of your data flow.",
    "One key consideration is that many partitions might increase end-to-end latency. If milliseconds count in your application, you might not be able to wait until a partition is replicated between brokers [9]. This is key to having in-sync replicas, and it is done before a message is available for delivery to a consumer. You would also need to make sure that you watch the memory usage of your consumers. If you do not have a 1-to-1 mapping of partitions to consumers, each consumer\u2019s memory requirements can increase as it is assigned more partitions [9].",
    "If you run across older documentation for Kafka, you might notice consumer cli- ent configurations for Apache ZooKeeper. Unless one is using an old consumer client,",
    "Kafka does not have consumers rely directly on ZooKeeper. Although consumers used ZooKeeper to store the offsets that they consume to a certain point, now the offsets are often stored inside a Kafka internal topic [10]. As a side note, consumer clients do not have to store their offsets in either of these locations, but this will likely be the case. If you want to manage your own offset storage you can! You can either store it in a local file, in cloud storage with a provider like AWS\u2122, or a database. One of the advantages of moving away from ZooKeeper storage was to reduce the clients\u2019 depen- dency on ZooKeeper."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tracking": [
    "Why is the concept of consumer groups paramount? Probably the most important rea- son is that scaling is impacted by either adding customers to or removing consumers from a group. Consumers that are not part of the same group do not share the same coordination of offset knowledge.",
    "Listing 5.3 shows an example of a group named kinaction_team0group. If you instead make up a new group.id (like a random GUID), you will start a new consumer with no stored offsets and with no other consumers in your group [11]. If you join an existing group (or one that had offsets stored already), your consumer can share work with others or can even resume where it left off reading from any previous runs [1].",
    "Listing 5.3  Consumer configuration for consumer group",
    "Properties kaProperties = new Properties(); kaProperties.put(\"group.id\", \"kinaction_team0group\");",
    "group.id determines consumer behavior with other consumers.",
    "It is often the case that you will have many consumers reading from the same topic. An important detail to decide on if you need a new group ID is whether your consumers are working as part of one application or as separate logic flows. Why is this important?",
    "Let\u2019s think of two use cases for data that come from a human resource system. One team wonders about the number of hires from specific states, and the other team is more interested in the data for the impact on travel budgets for interviews. Would any- one on the first team care about what the other team is doing or would either of the teams want to consume only a portion of the messages? Likely not! How can we keep this separation? The answer is to assign a separate group.id to each application. Each consumer that uses the same group.id as another consumer will be considered to be working together to consume the partitions and offsets of the topic as one logical application."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tracking<sec><sec>Group coordinator": [
    "Going through our usage patterns so far, we have not talked too much about how we keep a record of what each client has read. Let\u2019s briefly talk about how some message brokers handle messages in other systems. In some systems, consumers do not record",
    "what they have read. They pull the message and then it does not exist on a queue after acknowledgment. This works well for a single message that needs to have exactly one application process it. Some systems use topics in order to publish the message to all those that are subscribers. And often, future subscribers will have missed this mes- sage entirely because they were not actively part of the receiver list when the event happened.",
    "Figure 5.5 shows non-Kafka message broker scenarios, including how messages are often removed after consumption. It also shows a second pattern where a message might come from the original source and then be replicated to other queues. In sys- tems where the message would be consumed and not available for more than one con- sumer, this approach is needed so that separate applications each get a copy.",
    "Read once and acknowledge",
    "The only message  available to read is 3.",
    "Multiple consumers want the same message.",
    "Our kinaction_alert topic messages are not removed.",
    "For each consumer to get the same message, some systems fan out with copies of the data.",
    "Figure 5.5  Other broker scenarios",
    "You can imagine that the copies grow in number as an event becomes a popular source of information. Rather than have entire copies of the queue (besides those for replication or failover), Kafka can serve multiple applications from the same partition leader replica.",
    "Kafka, as we mentioned in the first chapter, is not limited to having only one con- sumer. Even if a consuming application does not exist when a message is first created on a topic, as long as Kafka retains the message in its log, then it can still process the",
    "data. Because messages are not removed from other consumers or delivered once, consumer clients need a way to keep a record of where they have read in the topic. In addition, because many applications can read the same topic, it is important that the offsets and partitions are specific to a certain consumer group. The key coordinates to let your consumer clients work together is a unique blend of the following: group, topic, and partition number."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tracking<sec><sec>Partition assignment strategy": [
    "As mentioned earlier, the group coordinator works with the consumer clients to keep a record of where inside the topic that specific group has read [8]. The partition\u2019s coordinates of a topic and group ID make it specific to an offset value.",
    "Looking at figure 5.6, notice that we can use the offset commits as coordinates to find out where to read from next. For example, in the figure, a consumer that is part of a group called kinaction_teamoffka0 and is assigned partition 0 would be ready to read offset 3 next.",
    "Topic: kinaction_alert",
    "Partition 0",
    "Group ID kinaction_teamoffka0 last read:",
    "Partition 0, offset 2",
    "Three partitions",
    "Partition 1, offset 1",
    "Partition 2, offset 1",
    "This offset information tells you where you are and what to consume next!",
    "Figure 5.6  Coordinates",
    "Figure 5.7 shows a scenario where the same partitions of interest exist on three separate brokers for two different consumer groups, kinaction_teamoffka0 and kinaction_teamsetka1. The consumers in each group will get their own copy of the data from the partitions on each broker. They do not work together unless they are part of the same group. Correct group membership is important for each group to have their metadata managed accurately.",
    "Consumers from different groups ignore each other, getting their own copy of the data.",
    "Figure 5.7  Consumers in separate groups [12]",
    "As a general rule, only one consumer per consumer group can read one partition. In other words, whereas a partition might be read by many consumers, it can only be read by one consumer from each group at a time. Figure 5.8 highlights how one consumer can read two partitions leader replicas, where the second consumer can only read the data from a third partition leader [8]. A single partition replica is not to be divided or shared between more than one consumer with the same ID.",
    "This consumer reads one section of the total data.",
    "Two consumers reading a 3-partition kinaction_alert topic would have one consumer reading 2 partitions.",
    "This consumer reads two sections of the total data.",
    "Figure 5.8  Kafka consumers in a group",
    "One of the neat things about being part of a consumer group is that when a consumer fails, the partitions that it was reading are reassigned [8]. An existing consumer takes the place of reading a partition that was once read by the consumer that dropped out of the group.",
    "Table 5.1 listed heartbeat.interval.ms, which determines the amount of pings to the group coordinator [13]. This heartbeat is the way that the consumer communi- cates with the coordinator to let it know it is still replying in a timely fashion and work- ing away diligently [8].",
    "Failure by a consumer client to send a heartbeat over a period of time can happen in a couple of ways, like stopping the consumer client by either termination of the process or failure due to a fatal exception. If the client isn\u2019t running, it cannot send messages back to the group coordinator [8]."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Marking our place": [
    "One item that we will want to be aware of is how consumers get assigned to partitions. This matters since it will help you figure out how many partitions each of your con- sumers might be taxed with processing. The property partition.assignment.strat- egy is what determines which partitions are assigned to each consumer [14]. Range and RoundRobin are provided, as are Sticky and CooperativeSticky [15].",
    "The range assigner uses a single topic to find the number of partitions (ordered by number) and then is broken down by the number of consumers. If the split is not even, then the first consumers (using alphabetical order) get the remaining partitions [16]. Make sure that you employ a spread of partitions that your consumers can handle and consider switching the assignment strategy if some consumer clients use all their resources, though others are fine. Figure 5.9 shows how three clients will grab three out of seven total partitions and end up with more partitions than the last client.",
    "The round-robin strategy is where the partitions are uniformly distributed down the row of consumers [1]. Figure 5.9 is a modified figure from the article \u201cWhat I have learned from Kafka partition assignment strategy,\u201d which shows an example of three clients that are part of the same consumer group and assigned in a round-robin fash-",
    "Range\tRoundRobin",
    "Figure 5.9  Partition assignments",
    "ion for one topic made of seven partitions [17]. The first consumer gets the first parti- tion, the second consumer the second, and so on until the partitions run out.",
    "The sticky strategy was added in version 0.11.0 [18]. However, since we will use range assigner in most of our examples internally and already looked at round-robin as well, we will not dig into Sticky and CooperativeSticky."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Reading from a compacted topic": [
    "One of the important things to think about is your need for assuring that your applica- tions read all messages from your topic. Is it okay to miss a few, or do you need each message confirmed as it\u2019s read? The real decision comes down to your requirements and any trade-offs you are willing to make. Are you okay with sacrificing some speed in order to have a safer method of seeing each message? These choices are discussed in this section.",
    "One option is to use enable.auto.commit set to true, the default for consumer clients [19]. Offsets are committed on our behalf. One of the nicest parts of this option is that we do not have to make any other calls to commit the offsets that are consumed.",
    "Kafka brokers resend messages if they are not automatically acknowledged due to a consumer client failure. But what sort of trouble can we get into? If we process mes- sages that we get from our latest poll, say, in a separate thread, the automatic commit offset can be marked as being read even if everything is not actually done with those specific offsets. What if we had a message fail in our processing that we would need to retry? With our next poll, we could get the next set of offsets after what was already committed as being consumed [8]. It is possible and easy to lose messages that look like they have been consumed despite not being processed by your consumer logic.",
    "When looking at what you commit, notice that timing might not be perfect. If you do not call a commit method on a consumer with metadata noting your specific offset to commit, you might have some undefined behavior based on the timing of polls, expired timers, or even your own threading logic. If you need to be sure to commit a record at a specific time as you process it or a specific offset in particular, you should make sure that you send the offset metadata into the commit method.",
    "Let\u2019s explore this topic more by talking about using code-specific commits enabled by enable.auto.commit set to false. This method can be used to exercise the most management over when your application actually consumes a message and commits it. At-least-once delivery guarantees can be achieved with this pattern.",
    "Let\u2019s talk about an example in which a message causes a file to be created in Hadoop in a specific location. As you get a message, you poll a message at offset 999. During pro- cessing, the consumer stops because of an error. Because the code never actually com- mitted offset 999, the next time a consumer of that same group starts reading from that partition, it gets the message at offset 999 again. By receiving the message twice, the cli- ent was able to complete the task without missing the message. On the flip side, you did get the message twice! If for some reason your processing actually works and you",
    "achieve a successful write, your code has to handle the fact that you might have dupli- cates.",
    "Now let\u2019s look at some of the code that we would use to control our offsets. As we did with a producer when we sent a message earlier, we can also commit offsets in a syn- chronous or asynchronous manner. Listing 5.4 shows a synchronous commit. Looking at that listing for commitSync, it is important to note that the commit takes place in a manner that blocks any other progress in the code until a success or failure occurs [20].",
    "Listing 5.4  Waiting on a commit",
    "consumer.commitSync();",
    "#// Any code here will wait on line before",
    "commitSync waits for a success or fail.",
    "As with producers, we can also use a callback. Listing 5.5 shows how to create an asyn- chronous commit with a callback by implementing the OffsetCommitCallback inter- face (the onComplete method) with a lambda expression [21]. This instance allows for log messages to determine our success or failure even though our code does not wait before moving on to the next instruction.",
    "Listing 5.5  Commit with a callback",
    "public static void commitOffset(long offset,",
    "int partition, String topic,",
    "KafkaConsumer<String, String> consumer) { OffsetAndMetadata offsetMeta = new OffsetAndMetadata(++offset, \"\");",
    "Map<TopicPartition, OffsetAndMetadata> kaOffsetMap = new HashMap<>(); kaOffsetMap.put(new TopicPartition(topic, partition), offsetMeta);",
    "consumer.commitAsync(kaOffsetMap, (map, e) -> { if (e != null) {",
    "for (TopicPartition key : map.keySet()) {",
    "A lambda that creates an OffsetCommitCallback instance",
    "log.info(\"kinaction_error: offset {}\", map.get(key).offset());",
    "for (TopicPartition key : map.keySet()) { log.info(\"kinaction_info: offset {}\", map.get(key).offset());",
    "If you think back to chapter 4, this is similar to how we used asynchronous sends with a callback for acknowledgments. To implement your own callback, you need to use the interface OffsetCommitCallback. You can define an onComplete method defini- tion to handle exceptions or successes as needed.",
    "Why would you want to choose synchronous or asynchronous commit patterns? Keep in mind that your latency is higher if you wait for a blocking call. This time fac-",
    "tor might be worth the delay if your requirements include needs for data consistency [21]. These decisions help determine the amount of control you need to exercise when informing Kafka which messages your logic considers as processed."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Retrieving code for our factory requirements": [
    "Consumers should be made aware of reading from a compacted topic. Kafka com- pacts the partition log in a background process, and records with the same key might be removed except for the last one. Chapter 7 will go further into how these topics work, but in short, we need to update records that have the same key value. If you do not need a history of messages, but rather just the last value, you might wonder how this concept works with an immutable log that only adds records to the end. The biggest \u201cgotcha\u201d for consumers that might cause an error is that when reading records from a compacted topic, consumers can still get multiple entries for a single key [22]! How is this possible? Because compaction runs on the log files that are on disk, com- paction may not see every message that exists in memory during cleanup.",
    "Clients need to handle this case, where there is more than one value per key. We should have the logic in place to handle duplicate keys and, if needed, ignore all but the last value. To pique your interest about compacted topics, note that Kafka uses its own compacted internal topic, called  consumer_offsets, which relates directly to your consumer offsets themselves [23]. Compaction makes sense here because for a specific combination of a consumer group, partition, and topic, only the latest value is needed as it will have the latest offset consumed."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Retrieving code for our factory requirements<sec><sec>Reading options": [
    "Let\u2019s try to use the information we gathered about how consumers work to see if we can start working on our own solutions designed in chapter 3 for use with Kafka in our e-bike factory but from the consumer client perspective. As noted in chapter 3, we want to ensure that we do not lose any audit messages when operators complete commands against the sensors. First, let\u2019s look at the options we have in reading our offsets."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Retrieving code for our factory requirements<sec><sec>Requirements": [
    "Although there is no lookup of a message by a key option in Kafka, it is possible to seek to a specific offset. Thinking about our log of messages being an ever increasing array with each message having an index, we have a couple of options for this, includ- ing starting from the beginning, going to the end, or finding offsets based on specific times. Let\u2019s take a look at these options.",
    "One issue that we might run into is that we want to read from the beginning of a topic even if we have already done so. Reasons could include logic errors and a desire to replay the entire log or a failure in our data pipeline after starting with Kafka. The important configuration to set for this behavior is auto.offset.reset to earli- est[24]. Another technique that we can use is to run the same logic but use a different group ID. In effect, this means that the commit offset topics that Kafka uses internally",
    "will not be able to find an offset value but will be able to start at the first index found because the commit offset topic does not have any data on the new consumer group.",
    "Listing 5.6 is an example of setting the property auto.offset.reset to \"earliest\" to seek to a specific offset [24]. Setting a group ID to a random UUID also helps to achieve starting with no offset history for a consumer group. This is the type of reset we could use to look at kinaction_alerttrend with different code logic to determine trends against all of the data in that topic.",
    "Listing 5.6  Earliest offset",
    "Properties kaProperties = new Properties(); kaProperties.put(\"group.id\",",
    "UUID.randomUUID().toString()); kaProperties.put(\"auto.offset.reset\", \"earliest\");",
    "Creates a group ID for which Kafka does not have a stored offset",
    "Uses the earliest offset",
    "retained in our logs",
    "Sometimes you just want to start your logic from when the consumers start up and for- get about past messages [24]. Maybe the data is already too old to have business value in your topic. Listing 5.7 shows the properties you would set to get this behavior of starting with the latest offset. If you want to make sure that you don\u2019t find a previous consumer offset and want to instead default to the latest offset Kafka has for your sub- scriptions, using a UUID isn\u2019t necessary except for testing. If we are only interested about new alerts coming into our kinaction_alert topic, this might be a way for a consumer to see only those alerts.",
    "Listing 5.7  Latest offset",
    "Properties kaProperties = new Properties(); kaProperties.put(\"group.id\",",
    "UUID.randomUUID().toString()); kaProperties.put(\"auto.offset.reset\", \"latest\");",
    "Creates a group ID for which Kafka does not have a stored offset",
    "Uses the latest",
    "record offset",
    "One of the trickier offset search methods is offsetsForTimes. This method allows you to send a map of topics and partitions as well as a timestamp for each in order to get a map back of the offset and timestamp for the given topics and partitions [25]. This can be useful in situations where a logical offset is not known, but a timestamp is known. For example, if you have an exception related to an event that was logged, you might be able to use a consumer to determine the data that was processed around your specific timestamp. Trying to locate an audit event by time might be used for our topic kinaction_audit to locate commands happening as well.",
    "As listing 5.8 shows, we have the ability to retrieve the offset and timestamps per a",
    "topic or partition when we map each to a timestamp. After we get our map of meta- data returned from the offsetsForTimes call, we then can seek directly to the offset we are interested in by seeking to the offset returned for each respective key.",
    "Listing 5.8  Seeking to an offset by timestamps",
    "Map<TopicPartition, OffsetAndTimestamp> kaOffsetMap = consumer.offsetsForTimes(timeStampMapper);",
    "// We need to use the map we get consumer.seek(partitionOne,",
    "Finds the first offset greater or equal to that timeStampMapper",
    "kaOffsetMap.get(partitionOne).offset());",
    "Seeks to the first offset provided in kaOffsetMap",
    "One thing to be aware of is that the offset returned is the first message with a time- stamp that meets your criteria. However, due to the producer resending messages on failures or variations in when timestamps are added (by consumers, perhaps), times might appear out of order.",
    "Kafka also gives you the ability to find other offsets as can be referenced in the con- sumer Javadoc [26]. With all of these options, let\u2019s see how they apply to our use case."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Introducing the broker": [
    "1 S. Kozlovski. \u201cApache Kafka Data Access Semantics: Consumers and Member- ship.\u201d Confluent blog (n.d.). https://www.confluent.io/blog/apache-kafka-data",
    "-access-semantics-consumers-and-membership (accessed August 20, 2021).",
    "2 \u201cConsumer Configurations.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/installation/configuration/consumer- configs.html (accessed June 19, 2019).",
    "3 N. Narkhede. \u201cApache Kafka Hits 1.1 Trillion Messages Per Day \u2013 Joins the 4 Comma Club.\u201d Confluent blog (September 1, 2015). https://www.confluent.io/ blog/apache-kafka-hits-1-1-trillion-messages-per-day-joins-the-4-comma-club/ (accessed October 20, 2019).",
    "4 \u201cClass KafkaConsumer<K,V>.\u201d Kafka 2.7.0 API. Apache Software Foundation (n.d.). https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/con sumer/KafkaConsumer.html#poll-java.time.Duration- (accessed August 24, 2021).",
    "5  \u201cClass WakeupException.\u201d Kafka 2.7.0 API. Apache Software Foundation (n.d.). https://kafka.apache.org/27/javadoc/org/apache/kafka/common/errors/ WakeupException.html (accessed June 22, 2020).",
    "6 \u201cDocumentation: Topics and Logs.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/5.5.1/kafka/introduction.html#topics-and-logs (accessed October 20, 2021).",
    "7 \u201cKIP-392: Allow consumers to fetch from closest replica.\u201d Wiki for Apache Kafka. Apache Software Foundation (November 05, 2019). https://cwiki",
    ".apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+ fetch+from+closest+replica (accessed December 10, 2019).",
    "8 J. Gustafson. \u201cIntroducing the Kafka Consumer: Getting Started with the New Apache Kafka 0.9 Consumer Client.\u201d Confluent blog (January 21, 2016). https:// www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9- consumer-client/ (accessed June 01, 2020).",
    "9 J. Rao. \u201cHow to choose the number of topics/partitions in a Kafka cluster?\u201d Confluent blog (March 12, 2015). https://www.confluent.io/blog/how-choose-",
    "number-topics-partitions-kafka-cluster/ (accessed May 19, 2019).",
    "10 \u201cCommitting and fetching consumer offsets in Kafka.\u201d Wiki for Apache Kafka. Apache Software Foundation (March 24, 2015). https://cwiki.apache.org/ confluence/pages/viewpage.action?pageId=48202031 (accessed December 15, 2019).",
    "11 \u201cConsumer Configurations: group.id.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/installation/configuration/consumer- configs.html#consumerconfigs_group.id (accessed May 11, 2018).",
    "12 \u201cDocumentation: Consumers.\u201d Apache Software Foundation (n.d.). https:// kafka.apache.org/23/documentation.html#intro_consumers (accessed Decem- ber 11, 2019).",
    "13 \u201cConsumer Configurations: heartbeat.interval.ms.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/configuration/ consumer-configs.html#consumerconfigs_heartbeat.interval.ms (accessed May 11, 2018).",
    "14 \u201cConsumer Configurations: partition.assignment.strategy.\u201d Confluent docu- mentation (n.d.). https://docs.confluent.io/platform/current/installation/ configuration/consumer-configs.html#consumerconfigs_partition.assignment",
    ".strategy (accessed December 22, 2020).",
    "15 S. Blee-Goldman. \u201cFrom Eager to Smarter in Apache Kafka Consumer Rebal- ances.\u201d Confluent blog (n.d.). https://www.confluent.io/blog/cooperative",
    "-rebalancing-in-kafka-streams-consumer-ksqldb/ (accessed August 20, 2021).",
    "16 \u201cRangeAssignor.java.\u201d Apache Kafka GitHub (n.d.). https://github.com/ apache/kafka/blob/c9708387bb1dd1fd068d6d8cec2394098d5d6b9f/clients/ src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java (accessed August 25, 2021).",
    "17 A. Li. \u201cWhat I have learned from Kafka partition assignment strategy.\u201d Medium (December 1, 2017). https://medium.com/@anyili0928/what-i-have-learned- from-kafka-partition-assignment-strategy-799fdf15d3ab (accessed October 20, 2021).",
    "18 \u201cRelease Plan 0.11.0.0.\u201d Wiki for Apache Kafka. Apache Software Foundation (June  26,  2017).  https://cwiki.apache.org/confluence/display/KAFKA/",
    "Release+Plan+0.11.0.0 (accessed December 14, 2019).",
    "19 \u201cConsumer Configurations: enable.auto.commit.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/configuration/ consumer-configs.html#consumerconfigs_enable.auto.commit (accessed May 11, 2018).",
    "20 Synchronous Commits. Confluent documentation (n.d.). https://docs.confluent",
    ".io/3.0.0/clients/consumer.html#synchronous-commits (accessed August 24, 2021).",
    "21 Asynchronous Commits. Confluent documentation (n.d.). https://docs.conflu ent.io/3.0.0/clients/consumer.html#asynchronous-commits (accessed August 24, 2021).",
    "22 Kafka Design. Confluent documentation (n.d.). https://docs.confluent.io/ platform/current/kafka/design.html (accessed August 24, 2021).",
    "23 Kafka Consumers. Confluent documentation (n.d.). https://docs.confluent.io/ 3.0.0/clients/consumer.html (accessed August 24, 2021).",
    "24 \u201cConsumer Configurations: auto.offset.reset.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/configuration",
    "/consumer-configs.html#consumerconfigs_auto.offset.reset (accessed May 11, 2018).",
    "25 offsetsForTimes. Kafka 2.7.0 API. Apache Software Foundation (n.d.). https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/consumer/ Consumer.html#offsetsForTimes-java.util.Map- (accessed June 22, 2020).",
    "26 seek. Kafka 2.7.0 API. Apache Software Foundation (n.d.). https:// kafka.apache.org/27/javadoc/org/apache/kafka/clients/consumer/Consumer",
    ".html#seek-org.apache.kafka.common.TopicPartition-long- (accessed June 22, 2020).",
    "27 assign. Kafka 2.7.0 API. Apache Software Foundation (n.d.). https:// kafka.apache.org/27/javadoc/org/apache/kafka/clients/consumer/Kafka Consumer.html#assign-java.util.Collection- (accessed August 24, 2021).",
    "So far in our discussions, we have dealt with Kafka from the view of an application developer interacting from external applications and processes. However, Kafka is a distributed system that deserves attention in its own right. In this chapter, let\u2019s look at the parts that make the Kafka brokers work."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Role of ZooKeeper": [
    "Although we have focused on the client side of Kafka so far, our focus will now shift to another powerful component of the ecosystem: brokers. Brokers work together with other brokers to form the core of the system.",
    "As we start to discover Kafka, those who are familiar with big data concepts or who have worked with Hadoop before might see familiar terminologies such as rack aware- ness (knowing which physical server rack a machine is hosted on) and partitions. Kafka has a rack awareness feature that makes replicas for a partition exist physically on sep- arate racks [1]. Using familiar data terms should make us feel at home as we draw new parallels between what we\u2019ve worked with before and what Kafka can do for us. When setting up our own Kafka cluster, it is important to know that we have another cluster to be aware of: Apache ZooKeeper. This then is where we\u2019ll begin."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Options at the broker level": [
    "ZooKeeper is a key part of how the brokers work and is a requirement to run Kafka. Because Kafka needs to be running and exist before the brokers do, we will start our discussion there.",
    "NOTE As mentioned in chapter 2, to simplify the requirements of running Kafka, there was a proposal for the replacement of ZooKeeper with its own managed quorum [2]. Because this work was not yet complete at the time of publication, ZooKeeper is discussed in this work. But look for an early access release of the managed quorum, arriving in version 2.8.0.",
    "As ZooKeeper needs to have a minimum number in order to elect leaders and reach a decision, this cluster is indeed important for our brokers [3]. ZooKeeper itself holds information such as topics in our cluster [4]. ZooKeeper helps the brokers by coordi- nating assignments and notifications [5].",
    "With all of this interaction with the brokers, it is important that we have Zoo- Keeper running before starting our brokers. The health of the ZooKeeper cluster impacts the health of our Kafka brokers. For instance, if our ZooKeeper instances are damaged, topic metadata and configuration could be lost.",
    "Usually, we won\u2019t need to expose the details (IP addresses and ports) of our Zoo- Keeper cluster to our producer and consumer applications. Certain legacy frame- works we use might also provide a means of connecting our client application with our ZooKeeper cluster. One example of this is version 3.1.x of Spring Cloud Stream, which allowed us to set the zkNodes property [6]. The value defaulted to localhost and should be left alone in most cases to avoid a ZooKeeper dependency. The zkNodes property is marked as deprecated, but you never know if you will encounter older code for maintenance, so you want to keep an eye out for it. Why is this not needed currently and in the future? Besides the fact that Kafka will not always require ZooKeeper, it is also important for us to avoid unnecessary external dependencies in our applications. In addition, it gives us fewer ports to expose if we are working with firewalls for Kafka and our client to communicate directly.",
    "Using the Kafka tool zookeeper-shell.sh, which is located in the bin folder of our Kafka installation, we can connect to a ZooKeeper host in our cluster and look at how the data is stored [7]. One way to find the paths that Kafka uses is to look at the",
    "class ZkData.scala [8]. In this file, you will find paths like /controller, /controller",
    "_epoch, /config, and /brokers, for example. If we look at the /brokers/topics path, we will see a list of the topics that we have created. At this point, we should, hopefully, at least have the kinaction_helloworld topic in the list.",
    "NOTE We can also use a different Kafka tool, kafka-topics.sh, to see the list of topics, getting the same results! Commands in the following listings con- nect to ZooKeeper and Kafka, respectively, for their data but do so with a dif- ferent command interface. The output should include the topic we created in chapter 2, [kinaction_helloworld].",
    "Listing 6.1  Listing our topics",
    "bin/zookeeper-shell.sh localhost:2181",
    "Connects to our local",
    "ls /brokers/topics",
    "bin/kafka-topics.sh --list \\",
    "Lists all the topics with the ls command",
    "Using kafka-topics,",
    "ZooKeeper instance",
    "\u27a5 --bootstrap-server localhost:9094",
    "connects to ZooKeeper and lists the topics",
    "Even when ZooKeeper no longer helps to power Kafka, we might need to work with clusters that have not migrated yet, and we will likely see ZooKeeper in documenta- tion and reference material for quite a while. Overall, being aware of the tasks that Kafka used to rely on ZooKeeper to perform and the shift to handling those inside a Kafka cluster with internal metadata nodes provides insight into the moving pieces of the entire system.",
    "Being a Kafka broker means being able to coordinate with the other brokers as well as talking to ZooKeeper. In testing or working with proof-of-concept clusters, we might have only one broker node. However, in production, we will almost always have multiple brokers.",
    "Turning away from ZooKeeper for now, figure 6.1 shows how brokers exist in a cluster and how they are home to Kafka\u2019s data logs. Clients will be writing to and read- ing from brokers to get information into and out of Kafka, and they will demand bro- ker attention [9]."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Options at the broker level<sec><sec>Kafka\u2019s other logs: Application logs": [
    "Configuration is an important part of working with Kafka clients, topics, and brokers. If you looked at the setup steps to create our first brokers in appendix A, we modified the server.properties file there, which we then passed as a command line argument to the broker startup shell script. This file is a common way to pass a specific configuration to a broker instance. For example, the log.dirs configuration property in that file should always be set to a log location that makes sense for your setup.",
    "Like our kinaction_alert producer",
    "Data in (to partition)",
    "Messages can be replayed from the beginning of the log and consumed again.",
    "Partition 0",
    "Like our kinaction_alert consumer",
    "Data out (from partition)",
    "Logs are append only.",
    "New entries added to the end.",
    "No database storage, just disk.",
    "Each log is made up of entries labeled with offset numbers.",
    "ZooKeeper used for distributed configuration and management",
    "One of the brokers will be a controller.",
    "Figure 6.1  Brokers",
    "This file also deals with configurations related to listeners, log locations, log retention, ZooKeeper, and group coordinator settings [10]. As with the producer and consumer configurations, look for the Importance label of \u201chigh\u201d in the documentation at http:/",
    "/mng.bz/p9p2.",
    "The following listing provides an example of what happens when we have only one copy of our data and the broker it is on goes down. This can happen when we allow the broker defaults and do not pick them with purpose. To begin, make sure that your local test Kafka cluster is running with three nodes, and create a topic like listing 6.2 presents.",
    "Listing 6.2  Listing our topics",
    "bin/kafka-topics.sh --create \\",
    "--bootstrap-server localhost:9094 \\",
    "--topic kinaction_one_replica",
    "Creates a topic with only one partition and one replica",
    "bin/kafka-topics.sh --describe --bootstrap-server localhost:9094 \\",
    "--topic kinaction_one_replica",
    "Topic: one-replica PartitionCount: 1 ReplicationFactor: 1\tConfigs: Topic: kinaction_one_replica Partition: 0",
    "Leader: 2 Replicas: 2 Isr: 2",
    "Describes the kinaction_one_replica topic with all the data located on the broker with ID 2",
    "When we run the commands in listing 6.2 to create and describe the topic kinaction_one_replica, we\u2019ll see that there is only one value in the fields Partition, Leader, Replicas, and Isr (in-sync replicas). Further, the broker uses the same ID value. This means that the entire topic depends on that one broker being up and working.",
    "If we terminate the broker with ID 2 in this example and then try to consume a mes- sage for that topic, we would get a message such as \u201c1 partitions have leader brokers without a matching listener.\u201d Because there are no replica copies for the topic\u2019s parti- tion, there is no easy way to keep producing or consuming that topic without recovering that broker. Although this is just one example, it illustrates the importance that broker configuration can have when users create their topics manually as in listing 6.2.",
    "Another important configuration property to define sets the location for our appli- cation logs and errors during normal operation. Let\u2019s look at this next."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Options at the broker level<sec><sec>Server log": [
    "As with most applications, Kafka provides logs for letting us know what is going on inside the application. In the discussion that follows, the term application logs refers to the logs that we usually think of when working with any application, whether debug- ging or auditing. These application logs are not related to the record logs that form the backbone of Kafka\u2019s feature set.",
    "The location where these application logs are stored is also entirely different than those for records. When we start a broker, we will find the application log directory in the Kafka base installation directory under the folder logs/. We can change this loca- tion by editing the config/log4j.properties file and the value for kafka.logs.dir [11]."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Options at the broker level<sec><sec>Managing state": [
    "Many errors and unexpected behaviors can be traced back to configuration issues on startup. The server log file, server.log, is where we would look if there is a startup error or an exception that terminates the broker. It seems to be the most natural place to",
    "check first for any issues. Look (or use the grep command) for the heading Kafka- Config values.",
    "If you are overwhelmed when you first look at the directory that holds this file, note that you will likely see other files like controller.log (if the broker was ever in that role) and older dated files with the same name. One tool that you can use for log rota- tion and compression is logrotate (https://linux.die.net/man/8/logrotate), but there are many other tools available as well to manage older server logs.",
    "Something else to mention in regard to these logs is that they are located on each broker. They are not aggregated by default into one location. Various platforms might do this on our behalf, or we can gather them with a tool like Splunk\u2122 (https:// www.splunk.com/). It is especially important to know when we are trying to analyze logs to gather them when using something like a cloud environment in which the bro- ker instance might not exist."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Partition replica leaders and their role": [
    "As we discussed in chapter 2, each partition has a single leader replica. A leader replica resides on a single broker at any given time. A broker can host the leader replica of multiple partitions, and any broker in a cluster can host leader replicas. Only one broker in the cluster, however, acts as the controller. The role of the control- ler is to handle cluster management [12]. The controller also performs other admin- istrative actions like partition reassignment [13].",
    "When we consider a rolling upgrade of a cluster, shutting down and restarting one broker at a time, it is best to do the controller last [14]. Otherwise, we might end up restarting the controller multiple times.",
    "To figure out which broker is the current controller, we can use the zookeeper-shell script to look up the ID of the broker, as listing 6.3 shows. The path /controller exists in ZooKeeper, and in the listing, we run one command to look at the current value. Running that command for my cluster showed my broker with ID 0 as the controller.",
    "Listing 6.3  Listing the current controller",
    "bin/zookeeper-shell.sh localhost:2181",
    "Connects to your",
    "get /controller",
    "Uses get against the controller path",
    "ZooKeeper instance",
    "Figure 6.2 shows all of the output from ZooKeeper, including the brokerid value, \"brokerid\":0. If we migrate or upgrade this cluster, we would upgrade this broker last due to this role.",
    "We will also find a controller log file with the name controller.log that serves as an application log on broker 0 in this case. This log file can be important when we look at broker actions and failures.",
    "Figure 6.2  Example controller output"
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Partition replica leaders and their role<sec><sec>Losing data": [
    "As a quick refresher, topics are made up of partitions, and partitions can have replicas for fault tolerance. Also, partitions are written on the disks of the Kafka brokers. One of the replicas of the partition will have the job of being the leader. The leader is in charge of handling writes from external producer clients for that partition. Because the leader is the only one with newly written data, it also has the job of being the source of data for the replica followers [15]. And because the ISR list is maintained by the leader, it knows which replicas are up to date and have seen all the current messages. Replicas act as consumers of the leader partition and will fetch the messages [15].",
    "Figure 6.3 shows a three-node cluster with broker 3 as its leader and broker 2 and broker 1 as its followers, using kinaction_helloworld as a topic that might have been",
    "Figure 6.3  Leader",
    "ISR = [3, 2, 1]",
    "created in this manner. Broker 3 holds the leader replica for partition 2. As the leader, broker 3 handles all of the reads and writes from external producers and consumers. It also handles requests it receives from broker 2 and broker 1 as they pull new mes- sages into their copies. The ISR list [3,2,1] includes the leader in the first position",
    "(3) and then the remaining followers (2,1), who stay current with their copies of the",
    "messages from the leader.",
    "In some cases, a broker that fails may have hosted the leader replica for a partition. In figure 6.4, the previous example in figure 6.3 experiences a failure. Because broker 3 is not available, a new leader is elected. Figure 6.4 shows the new leader broker 2. Once a follower, it was elected as a leader replica to keep Kafka serving and receiving data for that partition. The ISR list is now [2,1] with the first position reflecting the new leader replica hosted on broker 2.",
    "Broker 3 fails and broker 2 becomes the new leader.",
    "Figure 6.4 New leader elected",
    "NOTE In chapter 5 we discussed a Kafka Improvement Proposal, KIP-392, which allows consumer clients to fetch from the closest replica [16]. Reading from a preferred follower rather than the leader replica is something that might make sense if our brokers span physical data centers. However, when discussing leaders and followers in this book, unless stated otherwise, we will focus on the default leader read and write behaviors.",
    "In-sync replicas (ISRs) are a key piece to really understanding Kafka. For a new topic, a specific number of replicas are created and added to the initial ISR list [17]. This number can be either from a parameter or, as a default, from the broker configuration. One of the details to note with Kafka is that replicas do not heal themselves by default. If you lose a broker on which one of your copies of a partition exists, Kafka does not (currently) create a new copy. We mention this because some users are used",
    "to filesystems like HDFS that maintain their replication number (self-heal) if a block is seen as corrupted or failed. An important item to look at when monitoring the health of our systems is how many of our ISRs are indeed matching our desired number.",
    "Why is watching this number so important? It is good to keep aware of how many copies you have before it hits 0! Let\u2019s say that we have a topic that is only one partition and that partition is replicated three times. In the best-case scenario, we would have two copies of the data that is in our lead partition replica. This, of course, means that the follower replicas are caught up with the leader. But what if we lose another ISR?",
    "It is also important to note that if a replica starts to get too far behind in copying messages from the leader, it can be removed from the ISR list. The leader notices if a follower is taking too long and drops it from its list of followers [17]. Then the leader continues to operate with a new ISR list. The result of this \u201cslowness\u201d to the ISR list is the same as in figure 6.4, in which a broker failed."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Peeking into Kafka": [
    "What if we have no ISRs and lose our lead replica due to a failure? When unclean",
    ".leader.election.enable is true, the controller selects a leader for a partition even if it is not up to date so that the system keeps running [15]. The problem with this is that data could be lost because none of the replicas have all the data at the time of the leader\u2019s failure.",
    "Unclean leader for kinaction_helloworld. Message 3 never made it to broker 1.",
    "Figure 6.5  Unclean leader election",
    "In sync but both fail",
    "Figure 6.5 shows data loss in the case of a partition with three replicas. In this case, both brokers 3 and 2 failed and are not online. Because unclean leader election was enabled, broker 1 is made the new leader even though it is not in sync with the other brokers. Broker 1 never sees message 3, so it cannot present that data to clients. At the cost of missing data, this option allows us to keep serving clients."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Peeking into Kafka<sec><sec>Cluster maintenance": [
    "There are many tools we can use to capture and view data from our applications. We will look at Grafana\u00ae (https://grafana.com/) and Prometheus\u00ae (https://prometheus",
    ".io/) as examples of tools that can be used to help set up a simple monitoring stack that can be used for Confluent Cloud [18].1 We\u2019ll use Prometheus to extract and store Kafka\u2019s metrics data. Then we\u2019ll send that data to Grafana to produce helpful graphi- cal views. To fully understand why we are setting up all of the following tools, let\u2019s quickly review the components and the work each one does (figure 6.6).",
    "Info from topics like kinaction_alert",
    "Figure 6.6  Graph flow",
    "In figure 6.6, we use JMX to look inside the Kafka applications. The Kafka exporter takes the JMX notifications and exports them into the Prometheus format. Prometheus scrapes the exporter data and stores the metrics data. Various tools can then take the information from Prometheus and display that information in a visual dashboard.",
    "There are many Docker\u2122 images and Docker Compose files that bundle all of these tools, or you can install each tool to a local machine in order to explore this pro- cess in greater detail.",
    "For the Kafka exporter, an outstanding option is available at https://github.com/ danielqsj/kafka_exporter. We prefer the simplicity of this tool because we can just run it and give it one or a list of Kafka servers to watch. It might work well for your use cases as well. Notice that we will get many client and broker-specific metrics because there are quite a few options that we might want to monitor. Even so, this is not a com- plete list of the metrics available to us.",
    "Figure 6.7 shows a query against a local data store, such as a local instance of Pro- metheus, that gathers metrics from our Kafka exporter tool. As we discussed about partitions, Kafka replicas do not heal themselves automatically, so one of the things we",
    "1 The Grafana Labs Marks are trademarks of Grafana Labs, and are used with Grafana Labs\u2019 permission. We are not affiliated with, endorsed or sponsored by Grafana Labs or its affiliates.",
    "want to monitor is under-replicated partitions. If this number is greater than 0, we might want to look at what is going on in the cluster to determine why there is a replica issue. We might display the data from this query in a chart or dashboard, or we can, potentially, send an alert.",
    "kafka_topic_partition_under_replicated_partition{instance=\u201clocalhost:9308\u201d,job=\u201ckafka_exporter\u201d,partition=\u201c0\u201d,topic=kinaction_helloworld)0",
    "Figure 6.7  Metric query example",
    "As noted, the Kafka exporter does not expose every JMX metric. To get more JMX metrics, we can set the JMX_PORT environment variable when starting our Kafka pro- cesses [19]. Other tools are available that use a Java agent to produce the metrics to an endpoint or port, which Prometheus can scrape.",
    "Listing 6.4 shows how we would set the variable JMX_PORT when starting a broker [19]. If we already have a broker running and do not have this port exposed, we will need to restart the broker to affect this change. We may also want to automate the set- ting of this variable to ensure that it is enabled on all future broker restarts.",
    "Listing 6.4  Starting a broker with a JMX port",
    "Adds the JMX_PORT variable",
    "JMX_PORT=$JMX_PORT bin/kafka-server-start.sh \\",
    "\u27a5 config/server0.properties"
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Peeking into Kafka<sec><sec>Adding a broker": [
    "when starting the cluster",
    "As we consider moving to production, we will want to configure more than one server. Another item to note is that various pieces of the ecosystem such as Kafka and Connect clients, Schema Registry, and the REST Proxy do not usually run on the same servers as the brokers themselves. Although we might run all of these on a laptop for testing (and we can run this software on one server), for safety and efficiency, we definitely don\u2019t want all of these processes running on a single server when we handle production work- loads. To draw a parallel to similarities with tools from the Hadoop ecosystem, Kafka scales well horizontally with more servers. Let\u2019s look at adding a server to a cluster."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Peeking into Kafka<sec><sec>Upgrading your cluster": [
    "Beginning with a small cluster is a great way to start, as we can always add brokers to grow our footprint. To add a Kafka broker to our cluster, we just start a new Kafka bro- ker with a unique ID. This ID can either be created with the configuration broker.id or with broker.id.generation.enable set to true [10]. That is pretty much it. But, there is something to be aware of in this situation\u2014the new broker will not be assigned to any partitions! Any topic partitions that we create before adding a new broker still persist on the brokers that existed at the time of their creation [20]. If we are okay with the new broker only handling new topics, then we don\u2019t need to do anything else."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Peeking into Kafka<sec><sec>Upgrading your clients": [
    "As with all software, updates and upgrades are a part of life. Not all systems can be brought down simultaneously and upgraded due to production workloads or business impact. One technique that can be used to avoid downtime for our Kafka applications is the rolling restart [14]. This means just upgrading one broker at a time. Figure 6.8 shows each broker being upgraded one at a time before moving on to the next broker for our cluster.",
    "The first broker completes and is back online.",
    "Status: Online\tStatus: Offline\tStatus: Online",
    "Waiting for second broker to complete before next broker upgrade.",
    "Our 3 brokers for our Kafka in Action cluster would be online and offline at different times.",
    "Figure 6.8  Rolling restart",
    "An important broker configuration property for rolling restarts is controlled.shut- down.enable. Setting this to true enables the transfer of partition leadership before a broker shuts down [21]."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Peeking into Kafka<sec><sec>Backups": [
    "As mentioned in chapter 4, although Kafka does its best to decouple the clients from the broker, it\u2019s beneficial to know the versions of clients with respect to brokers. This bidirectional client compatibility feature was new in Kafka 0.10.2, and brokers version",
    "0.10.0 or later support this feature [22]. Clients can usually be upgraded after all of the Kafka brokers in a cluster are upgraded. As with any upgrade, though, take a peek at the version notes to make sure newer versions are compatible."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>A note on stateful systems": [
    "Kafka does not have a backup strategy like one would use for a database; we don\u2019t take a snapshot or disk backup per se. Because Kafka logs exist on disk, why not just copy the entire partition directories? Although nothing is stopping us from doing that, one concern is making a copy of all of the data directories across all locations. Rather than performing manual copies and coordinating across brokers, one preferred option is for a cluster to be backed by a second cluster [23]. Between the two clusters, events are then replicated between topics. One of the earliest tools that you might have seen in production settings is MirrorMaker. A newer version of this tool (called Mirror- Maker 2.0) was released with Kafka version 2.4.0 [24]. In the bin subdirectory of the Kafka install directory, we will find a shell script named kafka-mirror-maker as well as a new MirrorMaker 2.0 script, connect-mirror-maker.",
    "There are also some other open source as well as enterprise offerings for mirroring data between clusters. Confluent Replicator (http://mng.bz/Yw7K) and Cluster Link- ing (http://mng.bz/OQZo) are also options to be aware of [25]."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Exercise": [
    "Kafka is an application that definitely works with stateful data stores. In this book, we will work on our own nodes and not with any cloud deployments. There are some great resources, including Confluent\u2019s site on using the Kubernetes Confluent Operator API (https://www.confluent.io/confluent-operator/) as well as Docker images available to do what you need done. Another interesting option is Strimzi\u2122 (https://github.com/ strimzi/strimzi-kafka-operator), if you are looking at running your cluster on Kuberne- tes. At the time of this writing, Strimzi is a Cloud Native Computing Foundation\u00ae (https://www.cncf.io/) sandbox project. If you are familiar with these tools, it might be a quick way for you to kick the tires on a proof of concept (PoC) setup if you find some interesting projects out in the Docker Hub. There is not, however, a one-size-fits-all mandate for our infrastructure.",
    "One benefit of Kubernetes that stands out is its ability to create new clusters quickly and with different storage and service communication options that Gwen Sha- pira explores further in her paper, \u201cRecommendations for Deploying Apache Kafka on Kubernetes\u201d [26]. For some companies, giving each product its own cluster might be easier to manage than having one huge cluster for the entire enterprise. The ability to spin up a cluster quickly rather than adding physical servers can provide the quick turnaround products need.",
    "Figure 6.9 shows a general outline of how Kafka brokers can be set up in Kuberne- tes with an operator pod, similar to how the Confluent and Strimzi operators might work. The terms in the figure are Kubernetes-specific, and we do not provide much explanation here because we do not want to shift the focus away from learning about Kafka itself. We, rather, provide a general overview. Note that this is how a cluster could work, not a specific setup description.",
    "These 3 brokers would replace our local 3 instances we are using for our examples.",
    "Operator pod",
    "Manages components",
    "Persistent volume",
    "Persistent volume",
    "Persistent volume",
    "Persistent volume",
    "Persistent volume",
    "Persistent volume",
    "Figure 6.9  Kafka on Kubernetes",
    "The Kubernetes operator is its own pod that lives inside of the Kubernetes cluster. As well, each broker is in its own pod as a part of a logical group called a StatefulSet. The purpose of the StatefulSet is to manage the Kafka pods and help guarantee ordering and an identity for each pod. If the pod that hosts a broker (the JVM process) with ID 0 fails, for example, a new pod is created with that identity (and not a random ID) and attaches to the same persistent storage volume as before. Because these volumes hold the messages of the Kafka partitions, the data is maintained. This statefulness helps overcome the sometimes short lives of containers. Each ZooKeeper node would also be in its own pod and part of its own StatefulSet.",
    "For those who are new to Kubernetes or are anxious about the transition to such a platform, one migration strategy that can be helpful is to run Kafka clients and appli- cations on a Kubernetes cluster before the Kafka brokers. Besides being stateless, run- ning our clients in this manner can help us get a feel for Kubernetes at the start of our learning path. However, we should not neglect the need to understand Kubernetes well in order to run Kafka on top of this platform.",
    "One developer team of four that one of the authors worked with recently focused half of the team on Kubernetes and half on running Kafka. Of course, this ratio might not be what every team encounters. The developer time required to focus on Kuber- netes depends on your team and overall experience."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>7.1\tTopics": [
    "1 \u201cPost Kafka Deployment.\u201d Confluent documentation (n.d.). https://docs.con- fluent.io/platform/current/kafka/post-deployment.html#balancing-replicas- across-racks (accessed September 15, 2019).",
    "2  \u201cKIP-500: Replace ZooKeeper with a Self-Managed Metadata Quorum.\u201d Wiki for Apache Kafka. Apache Software Foundation (July 09, 2020). https:// cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+Zoo- Keeper+with+a+Self-Managed+Metadata+Quorum (accessed August 22, 2020).",
    "3  F. Junqueira and N. Narkhede. \u201cDistributed Consensus Reloaded: Apache Zoo- Keeper and Replication in Apache Kafka.\u201d Confluent blog (August 27, 2015). https://www.confluent.io/blog/distributed-consensus-reloaded-apache-zoo- keeper-and-replication-in-kafka/ (accessed September 15, 2019).",
    "4 \u201cKafka data structures in Zookeeper [sic].\u201d Wiki for Apache Kafka. Apache Soft- ware Foundation (February 10, 2017). https://cwiki.apache.org/confluence/ display/KAFKA/Kafka+data+structures+in+Zookeeper (accessed January 19, 2020).",
    "5 C. McCabe. \u201cApache Kafka Needs No Keeper: Removing the Apache Zoo- Keeper Dependency.\u201d Confluent blog. (May 15, 2020). https://www.confluent",
    ".io/blog/upgrading-apache-kafka-clients-just-got-easier (accessed August 20, 2021).",
    "6  Apache\tKafka\tBinder\t(n.d.).\thttps://docs.spring.io/spring-cloud-stream",
    "-binder-kafka/docs/3.1.3/reference/html/spring-cloud-stream-binder-kafka",
    ".html#_apache_kafka_binder (accessed July 18, 2021).",
    "7 \u201cCLI Tools for Confluent Platform.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/installation/cli-reference.html (accessed August 25, 2021).",
    "8 \u201cZkData.scala.\u201d Apache Kafka GitHub. https://github.com/apache/kafka/ blob/99b9b3e84f4e98c3f07714e1de6a139a004cbc5b/core/src/main/scala/ kafka/zk/ZkData.scala (accessed August 27, 2021).",
    "9 \u201cA Guide To The Kafka Protocol.\u201d Wiki for Apache Kafka. Apache Software Foundation (June 14, 2017). https://cwiki.apache.org/confluence/display/",
    "KAFKA/A+Guide+To+The+Kafka+Protocol (accessed September 15, 2019).",
    "10  \u201cKafka Broker Configurations.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/installation/configuration/broker-configs",
    ".html (accessed August 21, 2021).",
    "11 \u201cLogging.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/ current/kafka/post-deployment.html#logging (accessed August 21, 2021).",
    "12 \u201cController.\u201d Confluent documentation (n.d.). https://docs.confluent.io/ platform/current/kafka/post-deployment.html#controller (accessed August 21, 2021).",
    "13 \u201cKafka Controller Internals.\u201d Wiki for Apache Kafka. Apache Software Founda- tion (January 26, 2014). https://cwiki.apache.org/confluence/display/",
    "KAFKA/Kafka+Controller+Internals (accessed September 15, 2019).",
    "14 \u201cPost Kafka Deployment.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/kafka/post-deployment.html#rolling-restart (accessed July 10, 2019).",
    "15 \u201cReplication.\u201d Confluent documentation (n.d.). https://docs.confluent.io/plat- form/current/kafka/design.html#replication (accessed August 21, 2021).",
    "16 \u201cKIP-392: Allow consumers to fetch from closest replica.\u201d Wiki for Apache Kafka. Apache Software Foundation (November 5, 2019). https://cwiki.apache",
    ".org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+ from+closest+replica (accessed December 10, 2019).",
    "17 N. Narkhede. \u201cHands-free Kafka Replication: A lesson in operational simplicity.\u201d Confluent blog (July 1, 2015). https://www.confluent.io/blog/hands-free-kafka",
    "-replication-a-lesson-in-operational-simplicity/ (accessed October 02, 2019).",
    "18 \u201cObservability Overview and Setup.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/tutorials/examples/ccloud-observability/ docs/observability-overview.html (accessed August 26, 2021).",
    "19 \u201cKafka Monitoring and Metrics Using JMX\u201d. Confluent documentation. (n.d.). https://docs.confluent.io/platform/current/installation/docker/operations/ monitoring.html (accessed June 12, 2020).",
    "20 \u201cScaling the Cluster (Adding a node to a Kafka cluster).\u201d Confluent documen- tation (n.d.). https://docs.confluent.io/platform/current/kafka/post-deploy ment.html#scaling-the-cluster-adding-a-node-to-a-ak-cluster (accessed August 21, 2021).",
    "21  \u201cGraceful shutdown.\u201d Apache Software Foundation (n.d.). https://kafka.apache",
    ".org/documentation/#basic_ops_restarting (accessed May 11, 2018).",
    "22  C. McCabe. \u201cUpgrading Apache Kafka Clients Just Got Easier.\u201d Confluent blog. (July  18,  2017).  https://www.confluent.io/blog/upgrading-apache-kafka-cli-",
    "ents-just-got-easier (accessed October 02, 2019).",
    "23 \u201cBackup and Restoration.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/kafka/post-deployment.html#backup-and",
    "-restoration (accessed August 21, 2021).",
    "24 Release Notes, Kafka Version 2.4.0. Apache Software Foundation (n.d.). https:// archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html (accessed May 12, 2020).",
    "25  \u201cMulti-DC Solutions.\u201d Confluent documentation (n.d.). https://docs.confluent",
    ".io/platform/current/multi-dc-deployments/index.html#multi-dc-solutions (accessed August 21, 2021).",
    "26 G. Shapira. \u201cRecommendations_for_Deploying_Apache_Kafka_on_Kuberne- tes.\u201d White paper (2018). https://www.confluent.io/resources/recommenda- tions-for-deploying-apache-kafka-on-kubernetes (accessed December 15, 2019).",
    "27 \u201cReplication tools.\u201d Wiki for Apache Kafka. Apache Software Foundation (Feb- ruary 4, 2019). https://cwiki.apache.org/confluence/display/kafka/replica-",
    "tion+tools (accessed January 19, 2019).",
    "In this chapter, we will look further into how we might store our data across topics as well as how to create and maintain topics. This includes how partitions fit into our design considerations and how we can view our data on the brokers. All of this information will help us as we also look at how to make a topic update data rather than appending it to a log."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>7.1\tTopics<sec><sec>Topic-creation options": [
    "To quickly refresh our memory, it is important to know that a topic is a non- concrete concept rather than a physical structure. It does not usually exist on only one broker. Most applications consuming Kafka data view that data as being in a",
    "single topic; no other details are needed for them to subscribe. However, behind the topic name are one or more parti- tions that actually hold the data [1]. Kafka writes the data that makes up a topic in the cluster to logs, which are written to the broker filesystems.",
    "Figure 7.1 shows partitions that make up\tone\ttopic\tnamed\tkinaction",
    "_helloworld. A single partition\u2019s copy is not split between brokers and has a phys- ical footprint on each disk. Figure 7.1 also shows how those partitions are made up of messages that are sent to the topic. If writing to a topic is so simple in getting-started examples, why do we need to understand the role and pieces that make up a topic? At the highest level, this impacts how our consumers get to the data. Let\u2019s say that our com- pany is selling spots for a training class",
    "The topic kinaction_helloworld is made up of three partitions that will likely be spread out among different brokers.",
    "Figure 7.1  Example topic with partitions",
    "using a web-based application that sends the events of user actions into our Kafka clus- ter. Our overall application process could generate droves of events. For example, there would be an event for the initial search on the location, one for the specific training being selected by the customer, and a third for classes that are confirmed. Should the producing applications send all of this data to a single topic or several top- ics? Is each message a specific type of event, and should each remain separated in dif- ferent topics? There are adjustments with each approach and some things to consider that will help us determine the best method to take in every situation.",
    "We see topic design as a two-step process. The first looks at the events we have. Do they belong in one topic or more than one? The second considers each topic. What is the number of partitions we should use? The biggest takeaway is that partitions are a per-topic design question and not a cluster-wide limitation or mandate. Although we can set a default number of partitions for topic creation, in most cases, we should con- sider how the topic will be used and what data it will hold.",
    "We should have a solid reason to pick a specific number of partitions. Jun Rao wrote a fantastic article titled \u201cHow to choose the number of topics/partitions in a Kafka cluster?\u201d on the Confluent blog about this very subject [2]! Let\u2019s say that we want to have a partition for each server as a generic rule. However, because we have one partition on each server does not mean producers will write evenly among them. To do so, we would have to ensure that each partition leader is spread out in that man- ner and stays that way.",
    "We also need to get familiar with our data. Let\u2019s take a look at a list of items to think about, both in general and in this training class scenario:",
    "Data correctness",
    "The volume of messages of interest per consumer",
    "How much data you will have or need to process",
    "Data correctness is at the top of most data concerns in real-world designs. This term could be considered vague, so our defintion is explained here as our opinion. With regard to topics, this involves making sure that events that must be ordered end up in the same partition and, thus, the same topic. Although we can place events by our consumers in an order based on a timestamp, it is more trouble (and error prone) to handle cross-topic event coordination than it is worth, in our opinion. If we use keyed messages and need those in order, we should care about partitions and any future changes to those partitions [1].",
    "For data correctness with our three previous example events, it might be helpful to place the events with a message key (including the student ID) in two separate topics for the actual booked and confirmed/billed events. These events are student-specific, and this approach would be helpful to ensure that confirmation of a class occurs for that specific student. The search events themselves, however, may not be of interest or need to be ordered for a specific student if, for example, our analytics team is looking for the most popular searched cities rather than student information.",
    "Next, we should consider the volume of messages of interest per consumer. For our theoretical training system, let\u2019s look at the number of events as we consider the topic placement. The search events themselves would far outnumber the other events. Let\u2019s say that a training location near a large city gets 50,000 searches a day but only has room for 100 students. Traffic on most days produces 50,000 search events and fewer than 100 actual booked training events. Will our confirmation team have an applica- tion that would want to subscribe to a generic event topic in which it uses or cares about less than 1% of the total messages? Most of the consumer\u2019s time would be, in effect, filtering out the mass of events to process only a select few.",
    "Another point to account for is the quantity of data we will be processing. Will the number of messages require multiple consumers to be running in order to process within the time constraints required by our applications? If so, we have to be aware of how the number of consumers in a group is limited by the partitions in our topic [2]. It is easier at this point to create more partitions than we think we might require. Hav- ing more capacity for consumers to grow allows us to increase in volume without hav- ing to deal with repartitioning data. However, it is important to know that partitions are not an unlimited free resource, as talked about in Rao\u2019s article that we mentioned earlier. It also means having more brokers to migrate in case of a broker failure, which could be a potential headache in the making.",
    "It\u2019s best to find a happy medium and to go with that as we design our systems. Fig- ure 7.2 shows how our design might be best suited to two topics for the three event",
    "Booking details",
    "Partition count driven by analytics workload",
    "for search topic",
    "driven by keys and workload",
    "Partition X",
    "Figure 7.2  Example training event topic design",
    "types we used in our scenario. As always, more requirements or details can change our future implementations.",
    "A last thing to consider when deciding on the number of partitions for a topic is that reducing that number is not currently supported [3]. There may be ways to do this, but it is definitely not advised! Let\u2019s take a moment to think about why this would not be desirable.",
    "When consumers subscribe to a topic, they really are attached to a partition. The removal of a partition could lose its current position when or if a consumer starts reading from a reassigned partition. This is where we need to make sure our keyed messages and consuming clients can follow any changes we make at the broker level. We impact consumers with our actions. Now that we\u2019ve discussed topic design, let\u2019s dig a little deeper into the options that we can set when creating topics. We touched on these briefly when we created topics to produce messages in chapter 3, so we\u2019ll dive a bit deeper here."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>7.1\tTopics<sec><sec>Replication factors": [
    "Kafka topics have a couple of core options that must be set in order to create a topic. Although we have created topics since chapter 2 (with our kinaction_helloworld topic), we need to make sure we dig into the basic parameters that were glossed over. For these parameters, it\u2019s best to treat these decisions with thought and care and be intentional [4].",
    "Another important decision to make at creation time is if you will ever need to delete a topic. Because this operation is significant, we want to make sure it cannot happen without a logical confirmation. For this, Kafka requires us to enable the delete.topic.enable option. If this is switched to true, we will be able to successfully delete the topic and it will then be removed [5].",
    "It is nice to know that Kafka scripts have good usage documentation in general. We recommend running the command kafka-topics.sh first to see what various actions you can attempt. The following listing shows an incomplete command to get help.",
    "Listing 7.1  Listing our topic options",
    "bin/kafka-topics.sh",
    "Runs the generic Kafka topic-related command",
    "In the output that we\u2019ll see, one obvious command stands out: --create. Adding that parameter helps us get further information related to the create action itself (for example, \u201cMissing required argument \"[topic]\"\u201d). The following listing shows our still incomplete generic command built a little further.",
    "Listing 7.2  Listing our topic options with --create",
    "bin/kafka-topics.sh --create",
    "Lists command-specific errors and the help documentation",
    "Why spend time even talking about these steps, as some users are familiar with manual (man) pages as part of their Linux\u00ae work? Even though Kafka does not present data about how to use the tooling in that manner, this command is available before you have to search on Google.",
    "Once we have a name that does not have over 249 characters (it\u2019s been attempted before), we can create our topic [6]. For our examples, we\u2019ll create kinaction",
    "_topicandpart with a replication factor of 2 and with two partitions. The next listing shows the syntax to use in the command prompt [3].",
    "Listing 7.3  Creating another topic",
    "bin/kafka-topics.sh",
    "--create --bootstrap-server localhost:9094 \\",
    "Adds the create option to our command",
    "--topic kinaction_topicandpart \\",
    "--partitions 2 \\",
    "Names our topic",
    "Ensures that we have two",
    "--replication-factor 2",
    "Creates our topic with two partitions",
    "copies of our data",
    "After we create our topic, we can describe that topic to make sure our settings look correct. Notice in figure 7.3 how our partition and replication factor match the com- mand we just ran.",
    "Figure 7.3  Describing a topic with two partitions",
    "In our opinion, another option that is good to take care of at the broker level is to set",
    "auto.create.topics.enable to false [7]. Doing this ensures that we create our",
    "topics on purpose and not from a producer sending a message to a topic name that was mistyped and never actually existed before a message was attempted. Although not tightly coupled, usually producers and consumers do need to know the correct topic name of where their data should live. This automatic topic creation can cause confusion. But while testing and learning Kafka, autocreated topics can be helpful.",
    "For a concrete example, if we run the command",
    "kafka-console-producer.sh --bootstrap-server localhost:9094 --topic notexisting",
    "without that topic existing, Kafka creates that topic for us. And if we run",
    "kafka-topics.sh --bootstrap-server localhost:9094 --list",
    "we would now have that topic in our cluster.",
    "Although we usually focus on not removing data from production environments, as we continue in our own exploration of topics, we might run across some mistakes. It\u2019s good to know that we can indeed remove a topic if needed [3]. When we do that, all the data in the topic is removed. This is not something we would do unless we\u2019re ready to get rid of that data for good! Listing 7.4 shows how to use the kafka-topic command we used before, but this time to delete a topic named kinaction_topicandpart [3].",
    "Listing 7.4  Deleting a topic",
    "bin/kafka-topics.sh --delete --bootstrap-server localhost:9094",
    "--topic kinaction_topicandpart",
    "Removes topic kinaction_topicandpart",
    "Note that the --delete option is passed to our Kafka topics command. After running this command, you will not be able to work with this topic for your data as before."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Partitions": [
    "For practical purposes, we should plan on having the total number of replicas less than or equal to the number of brokers. In fact, attempting to create a topic with the number of replicas being greater than the total number of brokers results in an error: InvalidReplicationFactorException [8]. We may imagine why this is an error. Imagine, we only have two brokers, and we want three replicas of a partition. One of those replicas would exist on one broker and two on the other broker. In this case, if we lost the broker that was hosting two of the replicas, we would be down to only one copy of the data. Losing multiple replicas of your data at once is not the ideal way to provide recovery in the face of failure."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Partitions<sec><sec>Partition location": [
    "Moving on from dealing with Kafka commands at a (mostly) topic level, let\u2019s start to look deeper at partitions. From a consumer standpoint, each partition is an immutable log of messages. It should only grow and append messages to our data store. Although this data does not grow forever in practice, thinking of the data as",
    "being added to rather than modified in place is a good mental model to maintain. Also, consumer clients cannot directly delete messages. This is what makes it possible to replay messages from a topic, which is a feature that can help us in many scenarios."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Partitions<sec><sec>Viewing our logs": [
    "One thing that might be helpful is to look at how the data is stored on our brokers. To start, let\u2019s find the location of the log.dirs (or log.dir) directory. Its location can be found by looking for log.dirs in your server.properties file if you followed along from appendix A. Under that directory, we should be able to see subfolders with a topic name and a partition number. If we pick one of those folders and look inside, we will see a couple of different files with these extensions: .index, .log, and .timeindex. Fig- ure 7.4 shows how a single partition (in this case, 1) in our test topic looks by issuing a directory listing (ls).",
    "Figure 7.4  Partition directory listing",
    "Sharp-eyed readers might see the file named leader-epoch-checkpoint and maybe even files with a .snapshot extension (not shown above) in their own directory. The leader- epoch-checkpoint file and snapshot files are those that we will not spend time looking at. The files with the .log extension are where our data payload is stored. Other important information in the log file includes the offset of the message as well as the CreateTime field. Why the need for any other files then? Because Kafka is built for speed, it uses the .index and .timeindex files to store a mapping between the logical",
    "message offset and a physical position inside the index file [9].",
    "As shown so far, partitions are made up of many files. In essence, this means that on a physical disk, a partition is not one single file but is rather split into several seg- ments [10]. Figure 7.5 shows how multiple segments might make up a partition.",
    "Partition made up of one to many segments",
    "kinaction_topicandpart filename lengths are shortened for this example.",
    "Each segment has multiple similarly named files.",
    "Figure 7.5 Segments make up a partition.",
    "An active segment is the file to which new messages are currently written [11]. In our illustration, 10.log is where messages are being written in the partition directory. Older segments are managed by Kafka in various ways in which the active segment will not be; this includes being governed for retention based on the size of the messages or time configuration. These older segments (like 7.log in figure 7.5) can be eligible for topic compaction, which we will touch on later in this chapter.",
    "To recap what we now know about segments, we know why we might have multiple files with the same name in a partition directory but with an .index, .timeindex, or .log extension. For example, if we have 4 segments, we would have a set of 4 files, each with one of the previous 3 extensions, for a total of 12 files. If we only see 1 of each file extension, we only have 1 segment."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Testing with EmbeddedKafkaCluster": [
    "Let\u2019s try to take a peek at a log file to see the messages we have produced for our topic so far. If we open it in a text editor, we will not see those messages in a human- readable format. Confluent has a script that we can use to look at those log segments [12]. Listing 7.5 shows us passing the command to awk and grep to look at a segment log file for partition 1 of the topic kinaction_topicandpart.",
    "Listing 7.5  Looking at a dump of a log segment",
    "Prints data that cannot be",
    "bin/kafka-dump-log.sh --print-data-log \\",
    "--files /tmp/kafkainaction/kafka-logs-0/",
    "\u27a5 kinaction_topicandpart-1/*.log \\",
    "| awk -F: '{print $NF}' | grep kinaction",
    "viewed easily with a text editor",
    "Passes a file to read",
    "By using the --files option, which is required, we chose to look at a segment file. Assuming the command is successful, we should see a list of messages printed to the screen. Without using awk and grep, you would also see offsets as well as other related metadata like compression codecs. This is definitely an interesting way to see how Kafka places messages on the broker and the data it retains around those messages. The ability to see the actual messages is empowering as it really helps you see the log in action that drives Kafka.",
    "Looking at figure 7.6, we can see a payload in text that is a little easier to read than when we tried to cat the log file directly. For example, we can see a message in the segment file with the payload kinaction_helloworld. Hopefully, you will have more valuable data!",
    "Figure 7.6  Viewing a log segment",
    "As for the large number in the log filename, it is not random. The segment name should be the same as the first offset in that file.",
    "One of the impacts of being able to see this data is that we now have to be con- cerned with who else can see it. Because data security and access controls are common concerns with most data that holds values, we will look at ways you can secure Kafka and topics in chapter 10. Facts about the segment log and index files are details that we would not normally rely on in our applications. However, knowing how to look at these logs might be helpful when understanding how our logs really exist.",
    "It helps to imagine Kafka as a living and complex system (it is distributed, after all) that might need some care and feeding from time to time. In this next section, we will tackle testing our topic."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Testing with EmbeddedKafkaCluster<sec><sec>Using Kafka Testcontainers": [
    "With all of the configuration options we have, it might be nice to test them as well. What if we could spin up a Kafka cluster without having a real production-ready clus- ter handy? Kafka Streams provides an integration utility class called EmbeddedKafka- Cluster that serves as a middle ground between mock objects and a full-blown cluster. This class provides an in-memory Kafka cluster [13]. Although built with Kafka Streams in mind, we can use it to test our Kafka clients.",
    "Listing 7.6 is set up like the tests found in the book Kafka Streams in Action by William",
    "P. Bejeck Jr., for example, his KafkaStreamsYellingIntegrationTest class [14]. That book and his following book, Event Streaming with Kafka Streams and ksqlDB, show more in-depth testing examples. We recommend checking those out, including his sugges- tion of using Testcontainers (https://www.testcontainers.org/). The following listing shows testing with EmbeddedKafkaCluster and JUnit 4.",
    "Listing 7.6  Testing with EmbeddedKafkaCluster",
    "public static final EmbeddedKafkaCluster embeddedKafkaCluster",
    "= new EmbeddedKafkaCluster(BROKER_NUMBER);",
    "private Properties kaProducerProperties; private Properties kaConsumerProperties;",
    "public void setUpBeforeClass() throws Exception { embeddedKafkaCluster.createTopic(TOPIC,",
    "PARTITION_NUMBER, REPLICATION_NUMBER);",
    "kaProducerProperties = TestUtils.producerConfig( embeddedKafkaCluster.bootstrapServers(), AlertKeySerde.class,",
    "StringSerializer.class);",
    "kaConsumerProperties = TestUtils.consumerConfig( embeddedKafkaCluster.bootstrapServers(), AlertKeySerde.class, StringDeserializer.class);",
    "Uses JUnit-specific annotation to create the cluster with a specific number of brokers",
    "Sets the consumer configuration to point to the embedded cluster brokers",
    "public void testAlertPartitioner() throws InterruptedException { AlertProducer alertProducer = new AlertProducer();",
    "alertProducer.sendMessage(kaProducerProperties);",
    "} catch (Exception ex) {",
    "fail(\"kinaction_error EmbeddedKafkaCluster exception\"",
    "\u27a5 + ex.getMessage());",
    "Calls the client without any changes, which is",
    "AlertConsumer alertConsumer = new AlertConsumer(); ConsumerRecords<Alert, String> records =",
    "clueless of the underlying cluster being embedded",
    "alertConsumer.getAlertMessages(kaConsumerProperties); TopicPartition partition = new TopicPartition(TOPIC, 0);",
    "List<ConsumerRecord<Alert, String>> results = records.records(partition);",
    "assertEquals(0, results.get(0).partition());",
    "Asserts that the embedded cluster handled the message from production to consumption",
    "When testing with EmbeddedKafkaCluster, one of the most important parts of the setup is to make sure that the embedded cluster is started before the actual testing begins. Because this cluster is temporary, another key point is to make sure that the producer and consumer clients know how to point to this in-memory cluster. To dis- cover those endpoints, we can use the method bootstrapServers() to provide the needed configuration to the clients. Injecting that configuration into the client instances is again up to your configuration strategy, but it can be as simple as setting the values with a method call. Besides these configurations, the clients should be able to test away without the need to provide mock Kafka features!",
    "The test in listing 7.6 verifies that the AlertLevelPartitioner logic was correct. Using that custom partitioner logic with a critical message should have landed the alert on partition 0 with our example code in chapter 4. By retrieving the messages for Topic- Partition(TOPIC, 0) and looking at the included messages, the message partition location was confirmed. Overall, this level of testing is usually considered integration testing and moves you beyond just a single component under test. At this point, we have tested our client logic together with a Kafka cluster, integrating more than one module.",
    "NOTE Make sure that you reference the pom.xml changes in the source code for chapter 7. There are various JARs that were not needed in previous chap- ters. Also, some JARs are only included with specific classifiers, noting that they are only needed for test scenarios."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Topic compaction": [
    "If you find that you are having to create and then tear down your infrastructure, one option that you can use (especially for integration testing) is Testcontainers (https:// www.testcontainers.org/modules/kafka/). This Java library uses Docker and one of a variety of JVM testing frameworks like JUnit. Testcontainers depends on Docker images to provide you with a running cluster. If your workflow is Docker-based or a development technique your team uses well, Testcontainers is worth looking into to get a Kafka cluster set up for testing.",
    "NOTE One of the coauthors of this book, Viktor Gamov, maintains a reposi- tory (https://github.com/gAmUssA/testcontainers-java-module-confluent- platform) of integration testing Confluent Platform components (including Kafka, Schema Registry, ksqlDB)."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>How long to store data": [
    "1 \u201cMain Concepts and Terminology.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/kafka/introduction.html#main-concepts- and-terminology (accessed August 28, 2021).",
    "2 J. Rao. \u201cHow to choose the number of topics/partitions in a Kafka cluster?\u201d (March 12, 2015). Confluent blog. https://www.confluent.io/blog/how-choose",
    "-number-topics-partitions-kafka-cluster/ (accessed May 19, 2019).",
    "3 \u201cDocumentation: Modifying topics.\u201d Apache Software Foundation (n.d.). https://kafka.apache.org/documentation/#basic_ops_modify_topic (accessed May 19, 2018).",
    "4 \u201cDocumentation: Adding and removing topics.\u201d Apache Software Foundation (n.d.).\thttps://kafka.apache.org/documentation/#basic_ops_add_topic (accessed December 11, 2019).",
    "5  \u201cdelete.topic.enable.\u201d Confluent documentation (n.d.). https://docs.confluent",
    ".io/platform/current/installation/configuration/broker-configs.html#broker configs_delete.topic.enable (accessed January 15, 2021).",
    "6 Topics.java. Apache Kafka GitHub. https://github.com/apache/kafka/blob/ 99b9b3e84f4e98c3f07714e1de6a139a004cbc5b/clients/src/main/java/org/ apache/kafka/common/internals/Topic.java (accessed August 27, 2021).",
    "7 \u201cauto.create.topics.enable.\u201d Apache Software Foundation (n.d.). https://docs. confluent.io/platform/current/installation/configuration/broker-configs",
    ".html#brokerconfigs_auto.create.topics.enable (accessed December 19, 2019).",
    "8 AdminUtils.scala. Apache Kafka GitHub. https://github.com/apache/kafka/ blob/d9b898b678158626bd2872bbfef883ca60a41c43/core/src/main/scala/ kafka/admin/AdminUtils.scala (accessed August 27, 2021).",
    "9 \u201cDocumentation: index.interval.bytes.\u201d Apache Kafka documentation. https:// kafka.apache.org/documentation/#topicconfigs_index.interval.bytes (accessed August 27, 2021).",
    "10 \u201cLog Compaction.\u201d Confluent documentation (n.d.). https://docs.confluent.io/ platform/current/kafka/design.html#log-compaction (accessed August 20, 2021).",
    "11 \u201cConfiguring The Log Cleaner.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/kafka/design.html#configuring-the-log",
    "-cleaner (accessed August 27, 2021).",
    "12 \u201cCLI Tools for Confluent Platform.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/installation/cli-reference.html (accessed August 25, 2021).",
    "13 EmbeddedKafkaCluster.java. Apache Kafka GitHub. https://github.com/ apache/kafka/blob/9af81955c497b31b211b1e21d8323c875518df39/streams/ src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafka Cluster.java (accessed August 27, 2021).",
    "14  W. P. Bejeck Jr. Kafka Streams in Action. Shelter Island, NY, USA: Manning, 2018.",
    "15 \u201ccleanup.policy.\u201d Confluent documentation (n.d.). https://docs.confluent.io/ platform/current/installation/configuration/topic-configs.html#topicconfigs",
    "_cleanup.policy (accessed November 22, 2020).",
    "16 \u201cLog Compaction Basics.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/kafka/design.html#log-compaction-basics  (accessed August 20, 2021).",
    "So far we have thought of our data as moving into and out of Kafka for brief peri- ods of time. Another decision to consider is where our data should live long term. When you use databases like MySQL or MongoDB\u00ae, you may not always think about if or how that data expires. Rather, you know that the data is (likely) going to exist for the majority of your application\u2019s entire lifetime. In comparison, Kafka\u2019s storage logically sits somewhere between the long-term storage solutions of a data- base and the transient storage of a message broker, especially if we think of message brokers holding onto messages until they are consumed by a client, as it often is in other message brokers. Let\u2019s look at a couple of options for storing and moving data in our Kafka environment."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Data movement": [
    "Currently, the default retention limit for data in Kafka topics is seven days, but we can easily configure this by time or data size [1]. But can Kafka hold data itself for a period of years? One real-world example is how the New York Times uses Kafka. The content in their cluster is in a single partition that was less than 100 GB at the time of writing [2]. If you recall from our discussion in chapter 7 about partitions, you know that all of this data exists on a single broker drive (as do any replica copies on their own drives) as partitions are not split between brokers. Because storage is considered to be rela- tively cheap and the capacity of modern hard drives is way beyond hundreds of giga- bytes, most companies would not have any size issues with keeping that data around. Is this a valid use of Kafka or an abuse of its intended purpose and design? As long as you have the space for your planned growth on a disk for future use, you might have found a good pattern for handling your specific workload.",
    "How do we configure retention for brokers? The main considerations are the size of the logs and the length of time the data exists. Table 8.1 shows some of the broker configuration options that are helpful for retention [3].",
    "Table 8.1  Broker retention configuration",
    "How do we disable log retention limits and allow them to stay forever? By setting both log.retention.bytes and log.retention.ms to \u20131, we can effectively turn off data deletion [4].",
    "Another thing to consider is how we can get similar retention for the latest values by using keyed events with a compacted topic. Although we can still remove data during compaction cleaning, the most recent keyed messages will always be in the log. This is a good way to retain data in use cases where we do not need every event (or his- tory) of how a key changed state from the current value.",
    "What if we want our data to stick around for a while, but simply do not have the disk space to hold our data on brokers? Another option for long-term storage is to move the",
    "data outside of Kafka and not retain it internally to the Kafka brokers themselves. Before data is removed by retention from Kafka, we could store the data in a database, in a Hadoop Distributed File System (HDFS\u2122), or upload our event messages into something like cloud storage. All of these paths are valid options and could provide more cost-effective means of holding onto our data after our consumers process it."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Data movement<sec><sec>Keeping the original event": [
    "Almost all companies seem to have a need for transforming the data that they receive. Sometimes, it is specific to an area within the company or due to third-party integra- tions. A popular term that many people use in this data transformation space is ETL (extract, transform, load). We can use tooling or code to take data in its original for- mat, transform the data, and then place it into a different table or data store. Kafka can play a key role in these data pipelines."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Data movement<sec><sec>Moving away from a batch mindset": [
    "One thing that we would like to note is our preference for event formats inside of Kafka. Although open to debate and your use case requirements, our preference is to store messages in the original format in a topic. Why keep the original message and not format it immediately before placing it into a topic? Having the original message makes it easier to go back and start over if you inadvertently messed up your transform logic. Instead of having to try to figure out how to fix your mistake on the altered data, you can always just go back to the original data and start again. We know that most of us usually have that experience when trying to format a date or the first time we run a regular expression. Sometimes you need a couple of shots at formatting the data the way you want.",
    "Another plus for getting the entire original message is that data you don\u2019t use today might be used in the future. Let\u2019s say the year is 1995, and you are getting a field from a vendor called mobile. Your business will never need that field, right? Once you see the need to launch your first text marketing campaign, you\u2019ll be thanking your past self that you kept that original, \u201cuseless\u201d data.",
    "Although the mobile field might be a trivial example for some, it is interesting to think about usage for data analysis. What if your models start to see trends on data that you once thought wouldn\u2019t matter? By retaining all the data fields, you might be able to go back to that data and find insights you never expected."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tools": [
    "Does the general topic of ETL or data pipelines bring terms to mind such as batch, end of day, monthly, or even yearly? One of the shifts from the data transformation processes of the past is the idea that you can continuously stream your data into various systems without delay. With Kafka, for example, you can keep the pipeline running in near- real time, and you can use its stream-processing platform to treat your data as an infinite series of events.",
    "We mention this as a reminder that Kafka can help enable a shift in the way you think of your data altogether. You do not have to wait for a nightly job to run and update a database. You also do not have to wait for a nightly window with less traffic to do intensive ETL tasks; you can do these as they stream into your system and have pipelines that are constantly working for your applications in real time. Let\u2019s take a look at tools available that might help you use your pipelines in the future or make better use of your pipelines today."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tools<sec><sec>Apache Flume": [
    "Data movement is a key to many systems, Kafka included. Although you can stay inside the open source Kafka and Confluent offerings like Connect, which was discussed in chapter 3, there are other tools that might fit your infrastructure or are already avail- able in your tool suite. Depending on your specific data source or sinks, the options mentioned in the following sections might help you achieve your goals. Note that although some tools in this section include sample configuration and commands, more setup (not shown) might be required before you can run these commands on your local machines. Hopefully, this section gives you enough information to pique your interest and allow you to start exploring on your own."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tools<sec><sec>Red Hat\u00ae Debezium\u2122": [
    "If you were first introduced to Kafka through work in the big data space, it is a strong possibility that you might have used Flume in relation to your cluster. If you have ever heard the term Flafka, you have definitely used this Kafka and Flume integration. Flume can provide an easier path for getting data into a cluster and relies more on configuration than on custom code. For example, if you want to ingest data into your Hadoop cluster and already have support from a vendor on these various pieces, Flume is a solid option to get data into your Kafka cluster.",
    "Figure 8.1 shows an example of how a Flume agent runs on a node as its own pro- cess. It watches the files local to that server and then uses the configuration for the agent that you provided to send data to a sink.",
    "Topic: kinaction_flumetopic partition",
    "Sink destination",
    "Figure 8.1  Flume agent",
    "Let\u2019s take a look again at integrating log files (our source of data) using a Flume agent into a Kafka topic (our data sink). Listing 8.1 shows a sample configuration file that we could use to set up a local Flume agent to watch a directory for changes [5]. The changes are placed in a Kafka topic, titled kinaction_flumetopic. To imagine this example, here\u2019s a comparison: it is like using a cat command on a file in a directory to read the file and send the result to a specific Kafka topic.",
    "Listing 8.1  Flume configuration for watching a directory",
    "ag.sources = logdir ag.sinks = kafkasink ag.channels = c1",
    "Defines custom names for the source, sink, and channel",
    "Specific spooldir source lets",
    "#Configure the source directory to watch ag.sources.logdir.type = spooldir",
    "Flume know which directory to watch for log entries.",
    "ag.sources.logdir.spoolDir = /var/log/kafkainactionlogs",
    "ag.sinks.kafkasink.channel = c1",
    "ag.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink ag.sinks.kafkasink.kafka.topic = kinaction_flumetopic",
    "# Bind both the sink and source to the same channel",
    "This section defines our topic and Kafka cluster information where we want our data to end up.",
    "ag.sources.logdir.channels = c1 ag.sinks.kafkasink.channel = c1",
    "Attaches the source to the sink by the defined channel",
    "Listing 8.1 shows how we could configure a Flume agent running on a server. You should notice that the sink configuration looks a lot like the properties we have used before in our Java client producer code.",
    "It is also interesting to note that Flume can use Kafka as not only a source or as a sink, but also as a channel. Because Kafka is seen as a more reliable channel for events, Flume can use Kafka to deliver messages between various sources and sinks.",
    "If you are reviewing Flume configurations and see Kafka mentioned, be sure to notice where and how it is actually used. The following listing shows the Flume agent configuration we can use to provide a reliable channel between various sources and sinks that Flume supports [5].",
    "Listing 8.2  Flume Kafka channel configuration",
    "Flume uses the KafkaChannel",
    "ag.channels.channel1.type =",
    "\u27a5 org.apache.flume.channel.kafka.KafkaChannel ag.channels.channel1.kafka.bootstrap.servers =",
    "\u27a5 localhost:9092,localhost:9093,localhost:9094",
    "class as the Kafka channel type.",
    "Provides our servers to connect to",
    "ag.channels.channel1.kafka.topic = kinaction_channel1_ch",
    "ag.channels.channel1.kafka.consumer.group.id =",
    "The topic that holds the data",
    "\u27a5 kinaction_flume",
    "Provides a consumer group to avoid collisions with other consumers",
    "between source and sink"
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tools<sec><sec>Secor": [
    "Debezium (https://debezium.io) describes itself as a distributed platform that helps turn databases into event streams. In other words, updates to our database can be treated as events! If you have a database background (or not), you may have heard of the term change data capture (CDC). As the name implies, the data changes can be tracked and used to react to those changes. At the time of writing this chapter, Debezium supports MySQL, MongoDB, PostgreSQL\u00ae, Microsoft SQL Server\u2122, Oracle, and IBM Db2. Cassandra\u2122 and Vitess\u2122 are in an incubating status as well [6]. Please see the cur- rent list of connectors at https://debezium.io/documentation/reference/connectors/. Debezium uses connectors and Kafka Connect to record the events our applica- tion consumes from Kafka as a normal client. Figure 8.2 shows an example of Debe-",
    "zium when it is registered as a connector in regard to Kafka Connect.",
    "Kafka Connect process",
    "Figure 8.2  Kafka Connect and Debezium used with a MySQL database",
    "In our scenario, a developer uses a command line interface (CLI) and deletes a user against the MySQL database instance that is being monitored for changes. Debezium captures the event written to the database\u2019s internal log, and that event goes through the connector service and feeds into Kafka. If a second event, such as a new user, is inserted into the database, a new event is captured.",
    "As an additional note, although not Kafka-specific, there are other examples of using techniques like CDC to provide timely events or changes to your data that might help you draw a parallel to what Debezium is aiming for overall."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tools<sec><sec>Example use case for data storage": [
    "Secor (https://github.com/pinterest/secor) is an interesting project from Pinterest that has been around since 2014. It aims to help persist Kafka log data to a variety of storage options, including S3 and Google Cloud Storage\u2122 [7]. The options for",
    "output are also various, including sequence, Apache ORC\u2122, and Apache Parquet\u2122 files as well as other formats. As always, one major benefit of projects having source code in a public repository is that we can see how other teams have implemented requirements that might be similar to ours.",
    "Figure 8.3 shows how Secor would act as a consumer of a Kafka cluster, much like any other application. Having a consumer added to a cluster for data backup is not a big deal. It leverages the way Kafka has always handled multiple readers of the events.",
    "Secor acts as another consumer of your cluster. kinaction_alerttrend is one topic to consider moving data to for longer-term keeping.",
    "Figure 8.3  Secor acting as a consumer and placing data into storage.",
    "Secor runs as a Java process and can be fed our specific configurations. In effect, it acts as another consumer of our existing topic(s) to gather data to end up in a specific destination like an S3 bucket. Secor does not get in the way of our other consumers, and it allows us to have a copy of our events so that they are not lost once Kafka reten- tion removes data from its logs.",
    "Invoking Secor should be familiar to those who are used to working with JARs in a Java environment. We can pass arguments with the standard -D parameters to the Secor application. In this instance, the most important file to update is the properties file with the configuration options. This file lets us fill in the details about our specific cloud storage bucket, for example."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Bringing data back into Kafka": [
    "Let\u2019s look at an example of how moving data out of Kafka for storage could be used at a later time. First, to clarify, we will break down our usage of the same data between two different areas. One area is working with the data in an operational manner as it comes into Kafka.",
    "Operational data is the events that are produced by our day-to-day operations. We can think of an event to order an item from a website as an example. A purchase event triggers our application into motion and does so in a low-latency way. The value of this data to our real-time applications might warrant keeping the data for a couple of days",
    "until the order is completed and mailed. After this timeframe, the event may become more important for our analytical systems.",
    "Analytical data, while based on that same operational data, is usually used more to make business decisions. In traditional systems, this is where processes like a data warehouse, an online analytical processing system (OLAP), and Hadoop shine. That event data can be mined using different combinations of fields in our events in differ- ent scenarios to find insights into sales data, for instance. If we notice that sales of cleaning supplies always spike before a holiday, we might use that data to generate bet- ter sale options for our business in the future."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Bringing data back into Kafka<sec><sec>Tiered storage": [
    "One of the most important things to note is that just because our data has left Kafka does not mean that it can\u2019t be put back in again. Figure 8.4 shows an example of data that lived out its normal lifespan in Kafka and was archived in cloud storage like S3. When a new application logic change required the older data be reprocessed, we did not have to create a client to read from both S3 and Kafka. Rather, using a tool like Kafka Connect, we can load that data from S3 back into Kafka! The interface stays the same from the point of view of our applications. Although it might not seem obvious at first glance why we would want to do such a thing, let\u2019s consider a situation in which we find value in moving our data back into Kafka after we have processed it and the retention period has passed.",
    "Imagine a team working on trying to find patterns in data that they collected throughout years of handling events. In our example, there are terabytes of data. To serve operational real-time data collection, this data was moved from Kafka into HDFS after real-time consumers dealt with the messages. Does our application logic now have to pull from HDFS directly? Why not just pull it back into Kafka, and our applica- tion can process the data as it had before? Loading data into Kafka again is a valid way of reprocessing data that may have aged out of our system. Figure 8.4 shows another example of how we can move data back into Kafka.",
    "Application rerun",
    "Older data moved to\tS3 external data store like kinaction_alerttrend data",
    "Kafka Connect",
    "Loaded from S3 store",
    "Figure 8.4  Moving data back into Kafka",
    "After some time, events are not available to the applications due to data retention con- figurations within Kafka. However, we have a copy of all previous events in an S3 bucket. Let\u2019s say that we have a new version of our previous application and would prefer to go through all of the previous data events as in our previous application. However, because those events are not in Kafka, do we pull them from S3 now? Do we want our application logic to pull from various sources or just to have one interface (that being Kafka)? We can create a new topic in our existing Kafka cluster and load the data from S3 with Kafka Connect, placing the data into a new Kafka topic. Our application can then run against Kafka, processing events without having to change any processing logic.",
    "The thought process is really to keep Kafka as the interface of our application and not have to create multiple ways to pull data into processing. Why create and maintain custom code to pull from different locations when we can use an existing tool like Connect to move the data to or from Kafka? Once we have our data in that one inter- face, we can process it the same.",
    "NOTE Keep in mind this technique only applies to data that has been removed from Kafka. If you still have the total timeline of data that you need in Kafka, you can always seek to the earlier offsets."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Architectures with Kafka": [
    "A newer option from the Confluent Platform version 6.0.0 on is called Tiered Storage. In this model, local storage is still the broker itself, and remote storage is introduced for data that is older (and stored in a remote location) and controlled by time config- uration (confluent.tier.local.hotset.ms) [8]."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Architectures with Kafka<sec><sec>Lambda architecture": [
    "Although there are various architectural patterns that view your data as events when building your products, such as model-view-controller (MVC), peer-to-peer (P2P), or service-oriented architecture (SOA) to name a few, Kafka can change the way you think about your entire architectural design. Let\u2019s take a peek at a couple of architec- tures that could be powered by Kafka (and to be fair, other streaming platforms). This will help us get a different perspective on how we might design systems for our customers.",
    "The term big data is used in reference to some of these discussions. It is important to note that the amount of data and the need to process that data in a timely manner were the drivers that led to some of these system designs. However, these architectures are not limited to fast data or big data applications only. By hitting the limits of spe- cific traditional database technologies, new views on data evolved. Let\u2019s look at two of them in the following sections."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Architectures with Kafka<sec><sec>Kappa architecture": [
    "If you have ever researched or worked with data applications that have included needs for both batch processing and operational workloads, you might have seen references to lambda architecture. The implementation of this architecture can start with Kafka as well, but it is a little more complex.",
    "The real-time view of the data is combined with a historical view to serve end users. The complexity of merging these two data views should not be ignored. For the authors, it was a challenge to rebuild the serving table. Also, you are likely going to have to maintain different interfaces for your data as you work with the results from both systems.",
    "The book Big Data, written by Nathan Marz with James Warren, discusses the lambda architecture more fully and goes into details about the batch, serving, and speed layers [9]. Figure 8.5 shows an example of how taking customer orders can be thought of in a batch and a real-time way. The customer totals from the previous days can be integrated with orders happening during the day into a combined data view to end users.",
    "Overnight batch totals",
    "Real-time totals",
    "This could also be shown by trends in kinaction_alerttrend being shown as trends from months before and combined",
    "with new events happening today.",
    "Customer D total",
    "Customer E total",
    "Figure 8.5  Lambda architecture",
    "Taking the concepts from figure 8.5 and to get a feel for this architecture, let\u2019s look at each layer at a high level. These layers are discussed in Big Data by Marz:",
    "Batch\u2014This layer is similar to the way batch processing with MapReduce occurs in a system like Hadoop. As new data is added to your data stores, the batch layer continues to precompute the view of the data that already lives in the system.",
    "Speed\u2014This layer is similar in concept to the batch layer except it produces views from recent data.",
    "Serving\u2014This layer updates the views it sends to consumers after each update to the batch views.",
    "For the end user, the lambda architecture unites data from the serving layer and the speed layer to answer requests with a complete view of all recent and past data. This real-time streaming layer is the most obvious place for Kafka to play a role, but it can also be used to feed the batch layer."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Multiple cluster setups": [
    "Another architectural pattern that can leverage the power of Kafka is kappa architec- ture. This architecture was proposed by the co-creator of Kafka, Jay Kreps [10]. Think about wanting to maintain a system that impacts your users without disruption. One way to do this is to switch out your updated views like in lambda. Another way to do this is by running the current system in parallel to the new one and cutting over once the new version is ready to serve traffic. Part of this cutover is of course making sure that the data that is being served by the older version will be reflected correctly in the newer version.",
    "You only regenerate the user-facing data when you need to. There is no need to merge old and new data, which is an ongoing process for some lambda implementa- tions. It does not have to be a continuous job, but rather invoked when you need an application logic change. Also, there\u2019s no need to change your interface to your data. Kafka can be used by both your new and old application code at the same time. Figure",
    "shows how customer events are used to create a view without using a batch layer.",
    "Figure 8.6 shows customer events from the past and present being used directly to create a view. Imagine the events being sourced from Kafka and then using Kafka Streams or ksqlDB to read all the events in near-real time and creating a view for end",
    "Past events",
    "Custom customer event",
    "Running total logic",
    "Customer events 0 to X",
    "No batch totals, just events",
    "Custom customer event",
    "For example, kinaction_alerttrend data not being batched",
    "but replayed if needed",
    "Figure 8.6  Kappa architecture",
    "users. If a change is ever needed to how customer events are processed, a second application can be created with different logic (like a new ksqlDB query), using the same data source (Kafka) as before. There is no need to have a batch layer (and man- age it) as there is only streaming logic used for making your end user views."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Multiple cluster setups<sec><sec>Scaling by adding clusters": [
    "Most of our topics and discussions so far have been from the viewpoint of our data in one cluster. But Kafka scales well, and it is not unheard of to reach hundreds of bro- kers for a single cluster. However, a one-size cluster does not fit all infrastructures. One of the concerns we run into when talking about cluster storage is where you serve your data in relation to your end user clients. In this section, we will talk about scaling by adding clusters rather than by adding brokers alone."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Cloud- and container-based storage options": [
    "Usually, the first things to scale would be the resources inside your existing cluster. The number of brokers is the first option that makes a straightforward path to growth. Netflix\u00ae\u2019s multicluster strategy is a captivating take on how to scale Kafka clusters [11]. Instead of using only the broker number as the way to scale the cluster, they found they could scale by adding clusters themselves!",
    "This design brings to mind the idea of Command Query Responsibility Segrega- tion (CQRS). For more details on CQRS, check out Martin Fowler\u2019s site at https:// martinfowler.com/bliki/CQRS.html, specifically the idea of separating the load of reading data from that of writing data [12]. Each action can scale in an independent manner without limiting other actions. Although CQRS is a pattern that can add com- plexity to our systems, it is interesting to note how this specific example helps manage the performance of a large cluster by separating the load of producers sending data into Kafka from the sometimes much larger load of consumers reading the data."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Cloud- and container-based storage options<sec><sec>Kubernetes clusters": [
    "Although we talked about Kafka log directories in chapter 6, we did not address the types of instances to use in environments that provide more short-lived storage. For reference, Confluent shared a study on deployments with AWS considerations in which they looked at the storage type trade-offs [13].",
    "Another option is to look at Confluent Cloud (https://www.confluent.io/ confluent-cloud/). This option allows you to worry less about the underlying storage used across cloud providers and how it is managed. As always, remember that Kafka itself keeps evolving and reacting to the needs that users run into as daily challenges. KIP-392 shows an item that was accepted at the time of this writing, which seeks to help address the issues of a Kafka cluster spanning data centers. The KIP is titled \u201cAllow consumers to fetch from the closest replica\u201d [14]. Be sure to check out recent KIPs (Kafka Improvement Proposals) from time to time to see how Kafka evolves in exciting ways."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Administration clients": [
    "1 \u201cKafka Broker Configurations.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/installation/configuration/broker-con- figs.html#brokerconfigs_log.retention.hours (accessed December 14, 2020).",
    "2  B. Svingen. \u201cPublishing with Apache Kafka at The New York Times.\u201d Confluent blog (September 6, 2017). https://www.confluent.io/blog/publishing-apache-",
    "kafka-new-york-times/ (accessed September 25, 2018).",
    "3 \u201cKafka Broker Configurations.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/installation/configuration/broker-configs",
    ".html (accessed December 14, 2020).",
    "4 \u201cKafka Broker Configurations: log.retention.ms.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/configura- tion/broker-configs.html#brokerconfigs_log.retention.ms (accessed December 14, 2020).",
    "5 \u201cFlume 1.9.0 User Guide: Kafka Sink.\u201d Apache Software Foundation (n.d.). https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka",
    "-sink (accessed October 10, 2019).",
    "6 \u201cConnectors.\u201d Debezium documentation (n.d.). https://debezium.io/docu- mentation/reference/connectors/ (accessed July 20, 2021).",
    "7 \u201cPinterest Secor.\u201d Pinterest. GitHub. https://github.com/pinterest/secor/ blob/master/README.md (accessed June 1, 2020).",
    "8 \u201cTiered Storage.\u201d Confluent documentation (n.d.). https://docs.confluent.io/ platform/current/kafka/tiered-storage.html (accessed June 2, 2021).",
    "9 N. Marz and J. Warren. Big Data: Principles and best practices of scalable real-time data systems. Shelter Island, NY, USA: Manning, 2015.",
    "10 J. Kreps. \u201cQuestioning the Lambda Architecture.\u201d O\u2019Reilly Radar (July 2, 2014). https://www.oreilly.com/radar/questioning-the-lambda-architecture/ (accessed October 11, 2019).",
    "11 A. Wang. \u201cMulti-Tenant, Multi-Cluster and Hierarchical Kafka Messaging Ser- vice.\u201d Presented at Confluent\u2019s Kafka Summit, San Francisco, USA, 2017 Pre- sentation [online]. https://www.confluent.io/kafka-summit-sf17/multitenant",
    "-multicluster-and-hieracrchical-kafka-messaging-service/.",
    "12 M. Fowler. \u201cCQRS\u201d (July 14, 2011). https://martinfowler.com/bliki/CQRS.html",
    "(accessed December 11, 2017).",
    "13 A. Loddengaard. \u201cDesign and Deployment Considerations for Deploying Apache Kafka on AWS.\u201d Confluent blog (July 28, 2016). https://www.conflu- ent.io/blog/design-and-deployment-considerations-for-deploying-apache-kafka",
    "-on-aws/ (accessed June 11, 2021).",
    "14 KIP-392: \u201cAllow consumers to fetch from closest replica.\u201d Wiki for Apache Kafka. Apache Software Foundation (November 05, 2019). https:// cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers",
    "+to+fetch+from+closest+replica (accessed December 10, 2019).",
    "15 cp-helm-charts. Confluent Inc. GitHub (n.d.). https://github.com/conflu entinc/cp-helm-charts (accessed June 10, 2020).",
    "16 \u201cConfluent for Kubernetes.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/operator/2.0.2/overview.html (accessed August 16, 2021).",
    "We have spent some time discussing brokers in depth in chapter 6 and client con- cerns throughout the earlier chapters. We saw some development practices that can be applied in most situations, but there will always be environments where spe- cial handling is required. The best way to keep a cluster moving along is to under- stand the data that is flowing through it and to monitor that activity at run time. Although operating Apache Kafka may not be the same as writing and running Java applications per se, it still requires monitoring log files and being aware of what is happening with our workloads."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Administration clients<sec><sec>Administration in code with AdminClient": [
    "So far, we have performed most of our cluster management activities with the com- mand line tools that come with Kafka. And, in general, we need to be comfortable with a shell environment to set up and install Kafka. However, there are some helpful options we can use to branch out from these provided scripts."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Administration clients<sec><sec>kcat": [
    "One useful tool to look at is the AdminClient class [1]. Although the Kafka shell scripts are great to have at hand for quick access or one-off tasks, there are situations such as automation where the Java AdminClient really shines. The AdminClient is in the same kafka-clients.jar that we used for the producer and consumer clients. It can be pulled into a Maven project (see the pom.xml from chapter 2,) or it can be found in the share/ or libs/ directory of the Kafka installation.",
    "Let\u2019s look at how we can execute a command we have used before to create a new topic but this time with AdminClient. The following listing shows how we ran this from the command line in chapter 2.",
    "Listing 9.1  Creating the kinaction_selfserviceTopic topic from the command line",
    "bin/kafka-topics.sh",
    "--create --topic kinaction_selfserviceTopic \\",
    "--bootstrap-server localhost:9094 \\",
    "Uses the kafka-topic.sh script to create a new topic",
    "--partitions 2 \\",
    "--replication-factor 2",
    "Includes our custom integers for the number",
    "of partitions and replicas for our topic",
    "Though this command line example works fine, we don\u2019t want it to be called every time someone needs a new topic. Instead, we\u2019ll create a self-service portal that other developers can use to create new topics on our development cluster. The form for our application takes a topic name and the numbers for partitions and replicas. Figure 9.1 shows an example of how this application might be set up for end users. Once the user submits the web form, the AdminClient Java code runs, creating a new topic.",
    "In this example, we could add logic to make sure that naming conventions for new topics fit a certain pattern (if we had such a business requirement). This is a way to maintain more control over our cluster rather than users working from the command line tools. To start, we need to create a NewTopic class. The constructor for this class takes three arguments:",
    "The number of partitions",
    "The number of replicas",
    "Application server",
    "Custom application",
    "End user submits a web form",
    "New topic kinaction_selfserviceTopic created",
    "Figure 9.1  Self-service Kafka web application",
    "Once we have this information, we can use the AdminClient object to complete the work. AdminClient takes a Properties object that contains the same properties we\u2019ve used with other clients, like bootstrap.servers and client.id. Note that the class AdminClientConfig (http://mng.bz/8065) holds constants for configuration values such as BOOTSTRAP_SERVERS_CONFIG as a helper for those names. Then we\u2019ll call the method createTopics on the client. Notice that the result, topicResult, is a Future object. The following listing shows how to use the AdminClient class to create a new topic called kinaction_selfserviceTopic.",
    "Listing 9.2  Using AdminClient to create a topic",
    "Creates a NewTopic object",
    "NewTopic requestedTopic =",
    "new NewTopic(\"kinaction_selfserviceTopic\", 2,(short) 2);",
    "with the topic name, two partitions, and two replicas",
    "AdminClient client = AdminClient.create(kaProperties);",
    "CreateTopicsResult topicResult = client.createTopics(",
    "List.of(requestedTopic)); topicResult.values().",
    "Creates an AdminClient, the client interface to the cluster",
    "Invokes createTopics on the client to return a Future object",
    "get(\"kinaction_selfserviceTopic\").get();",
    "Shows how to get a specific Future for the topic kinaction_selfserviceTopic",
    "At this time, there is no synchronous API, but we can make a synchronous call by using the get() function. In our case, that would mean starting with the topicResult variable and evaluating the Future object that was returned for the specific topic.",
    "Because this API is still evolving, the following list of client administrative tasks that can be accomplished with AdminClient highlights only a few common functions that are available at the time of writing [1]:",
    "Change configurations",
    "Create/delete/list access control lists (ACLs)",
    "Create partitions",
    "Create/delete/list topics",
    "Describe/list consumer groups",
    "Describe clusters",
    "AdminClient is a great tool for building a user-facing application for those who wouldn\u2019t normally need or want to use the Kafka shell scripts. It also provides a way to control and monitor what is being done on the cluster."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Administration clients<sec><sec>Confluent REST Proxy API": [
    "kcat (https://github.com/edenhill/kcat) is a handy tool to have on your workstation, especially when connecting remotely to your clusters. At this time, it focuses on being a producer and consumer client that can also give you metadata about your cluster. If you ever want to quickly work with a topic and don\u2019t have the entire Kafka toolset downloaded to your current machine, this executable helps you avoid the need to have those shell or bat scripts.",
    "The following listing shows how to quickly get data into a topic using kcat [2].",
    "Compare this with the kafka-console-producer script that we used in chapter 2.",
    "Listing 9.3  Using a kcat producer",
    "Sends a broker and topic",
    "kcat -P -b localhost:9094 \\",
    "-t kinaction_selfserviceTopic",
    "name from our cluster to write messages to that topic",
    "// vs. the shell script we used before",
    "bin/kafka-console-producer.sh --bootstrap-server localhost:9094 \\",
    "--topic kinaction_selfserviceTopic",
    "A reminder of the same functionality as the console producer command",
    "In listing 9.3, notice that the -P argument is passed to kcat to enable producer mode, which helps us send messages to the cluster. We use the -b flag to pass in our broker list and -t to pass the name of our target topic. Because we may also want to test the consumption of these messages, let\u2019s look at how we can use kcat as a consumer (list- ing 9.4). As before, listing 9.4 shows the comparison between running the kcat com- mand versus the kafka-console-consumer command. Notice also that although the",
    "-C flag enables consumer mode, the broker information is sent with the same parame- ter as in the producer mode [2].",
    "Listing 9.4  Using a kcat consumer",
    "Sends a broker and topic",
    "kcat -C -b localhost:9094 \\",
    "-t kinaction_selfserviceTopic",
    "name from our cluster to read messages from that topic",
    "// vs. the shell script we used before",
    "bin/kafka-console-consumer.sh --bootstrap-server localhost:9094 \\",
    "--topic kinaction_selfserviceTopic",
    "A reminder of the same functionality as the console consumer command",
    "Having a quick way to test our topics and gather metadata on our cluster makes this small utility nice to have in our toolbox. But by this point, you might be wondering if there are any other tools that we can use that are not command line driven. And the good news is yes, there are! For those that like REST, there is Confluent\u2019s REST Proxy."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Running Kafka as a systemd service": [
    "Sometimes the users of our cluster might prefer to use APIs that are RESTful because it is a common way to work between applications, either due to preference or ease of use. Also, some companies with strict firewall rules about ports might express caution with opening more ports like those we\u2019ve used so far for broker connections (for exam- ple, 9094) [3]. One good option is to use the Confluent REST Proxy API (figure 9.2). This proxy is a separate application that would likely be hosted on its own server for pro- duction usage, and its functionality is similar to the kcat utility we just discussed.",
    "Kafka REST proxy server",
    "Figure 9.2  The Confluent REST Proxy looks up topics.",
    "At the time of this writing, the administration functions are limited to querying the state of your cluster. The Confluent documentation lists administration options as future supported features, however [4]. To use the REST proxy and to test drive it, let\u2019s start it up as the following listing shows. For this to work, we need to already have ZooKeeper and Kafka instances running before we start the proxy.",
    "Listing 9.5  Starting up a REST Proxy",
    "bin/kafka-rest-start.sh \\",
    "etc/kafka-rest/kafka-rest.properties",
    "Run this command from the installed Kafka folder to start the REST endpoint.",
    "Because we\u2019re already familiar with listing topics, let\u2019s look at how that can be done with the REST Proxy using a command like curl to hit an HTTP endpoint as in the following listing [5]. Because this is a GET request, we can also copy http:/./localhost",
    ":8082/topics into a browser and see the result.",
    "Listing 9.6  A cURL call to the REST Proxy for a topic list",
    "curl -X GET \\",
    "-H \"Accept: application/vnd.kafka.v2+json\" \\",
    "Specifies a format",
    "localhost:8082/topics",
    "Our target, the endpoint /topics, lists the",
    "topics we\u2019ve created and Kafka\u2019s internal topics.",
    "and version",
    "[\" confluent.support.metrics\",\"_confluent-metrics\",",
    "\u27a5 \"_schemas\",\"kinaction_alert\"]",
    "Sample output of the curl command",
    "Using a tool like curl allows us to control the header we send with the request. Accept in listing 9.6 allows us to tell our Kafka cluster what format and version we are using, spec- ifying v2 as the API version and the JSON format that pertains to our metadata requests.",
    "NOTE Because this is an evolving API, keep up with the \u201cConfluent REST Proxy API Reference\u201d at http://mng.bz/q5Nw as newer versions come out with more features."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Logging": [
    "One decision we need to make concerning running Kafka is how to perform broker starts and restarts. Those who are used to managing servers as Linux-based services with a tool like Puppet (https://puppet.com/) may be familiar with installing service unit files and can likely use that knowledge to create running instances with systemd. For those not familiar with systemd: it initializes and maintains components through- out the system [6]. One common way to define ZooKeeper and Kafka are as unit files used by systemd.",
    "Listing 9.7 shows part of an example service unit file that starts a ZooKeeper ser- vice when the server starts. It also restarts ZooKeeper after an abnormal exit. In prac- tice, this means something like a kill -9 command against the process ID (PID) that triggers a restart of the process. If you installed the Confluent tar during your setup (refer to appendix A if needed), there is an example service file located in the lib/ systemd/system/confluent-zookeeper.service path. The \u201cUsing Confluent Platform systemd Service Unit Files\u201d documentation at (http://mng.bz/7lG9) provides details on using these files. The unit file in the listing should look familiar to how we have started ZooKeeper so far in our examples.",
    "Listing 9.7  ZooKeeper unit file",
    "Captures the start command to run",
    "ZooKeeper (similar to what we",
    "manually ran to start ZooKeeper)",
    "ExecStart=/opt/kafkainaction/bin/zookeeper-server-start.sh",
    "\u27a5 /opt/kafkainaction/config/zookeeper.properties",
    "/opt/kafkainaction/bin/zookeeper-server-stop.sh",
    "Shuts down the",
    "Restart=on-abnormal",
    "Runs ExecStart if an error condition causes a failure",
    "ZooKeeper instance",
    "There is also an example file for the Kafka service in the Confluent tar in the lib/ systemd/system/confluent-kafka.service path. The next listing shows that because our unit files are defined, we can now manage the services with systemctl commands [6].",
    "Listing 9.8  Kafka startup with systemctl",
    "sudo systemctl start zookeeper sudo systemctl start kafka",
    "Starts the Kafka service",
    "Starts the ZooKeeper service",
    "If you are using the example files that came when downloading the Confluent bundle, once you unzip the folder, check inside the root folder, ../lib/systemd/system, to see examples of service files that you can use for other services. Some of these include Connect, the Schema Registry, and the REST API, to name a few."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Logging<sec><sec>Kafka application logs": [
    "Besides Kafka\u2019s event logs that hold our event data, other items that we need to remember are the application logs, which Kafka produces as part of being a running program. The logs addressed in this section are not the events and messages from Kafka servers but the output of the operation of Kafka itself. And we cannot forget about ZooKeeper either!"
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Logging<sec><sec>ZooKeeper logs": [
    "Although we might be used to one log file for an entire application, Kafka has multi- ple log files that we might be interested in or need to access for troubleshooting. Due to multiple files, we might have to look at modifying different Log4j appenders to maintain the necessary views of our operations.",
    "By default, the server logs are continually added to the directory as new logs are pro- duced. No logs are removed, however, and this might be the preferred behavior if these files are needed for auditing or troubleshooting. If we want to control the num- ber and size, the easiest way is to update the file log4j.properties before we start the broker server. The following listing sets two important properties for kafkaAppender: MaxFileSize and MaxBackupIndex [7].",
    "Listing 9.9  Kafka server log retention",
    "Defines the file size to determine",
    "log4j.appender.kafkaAppender.MaxFileSize=500KB log4j.appender.kafkaAppender.MaxBackupIndex=10",
    "when to create a new log file",
    "Sets the number of older files to keep, which helps if we want more than the current log for troubleshooting",
    "Note that modifying kafkaAppender changes only how the server.log file is treated. If we want to apply different file sizes and backup file numbers for various Kafka-related files, we can use the appender to log a filename table to determine which appenders to update. In table 9.1, the appender name in the left column is the logging key, which affects how the log files on the right are stored on the brokers [8].",
    "Table 9.1  Appender to log pattern",
    "Changes to the log4j.properties file require the broker to be restarted, so it is best to determine our logging requirements before starting our brokers for the first time, if possible. We could also change the value with JMX, but the value would not be per- sistent across broker restarts.",
    "Although we focused on Kafka logs in this section, we need to address our Zoo- Keeper logs as well. Because ZooKeeper runs and logs data just like our brokers, we will need to be mindful of logging output for those servers as well."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Firewalls": [
    "Depending on how we installed and chose to manage ZooKeeper, we may also need to modify its logging configuration. The default configuration for ZooKeeper does not remove log files, but our Kafka install may have added that feature for us. If you fol- lowed the setup of our local ZooKeeper node from appendix A, these values can be set in the file config/zookeeper.properties. Either way, it is a good idea to make sure that the retention of the ZooKeeper application logs are controlled by the following configuration values and are what we need for troubleshooting:",
    "autopurge.purgeInterval\u2014The interval, in hours, in which a purge is trig- gered. This must be set above 0 for cleanup to occur [9].",
    "autopurge.snapRetainCount\u2014This contains the number of recent snapshots and the related transaction logs in the dataDir and dataLogDir locations [9]. Once we exceed the number, the older log files are deleted. Depending on our needs, we might want to keep more or fewer. For example, if the logs are only used for troubleshooting, we would need lower retention than if they are needed for audit scenarios.",
    "snapCount\u2014ZooKeeper logs its transactions to a transaction log. Setting this value determines the amount of transactions that are logged to one file. If there are issues with total file sizes, we might need to set this number less than the default (100,000) [10].",
    "There are other solutions to log rotation and cleanup that we might consider beyond Log4j. For example, logrotate is a helpful tool that enables options such as log rota- tion and compression of logs files.",
    "Log file maintenance is an important administration duty. However, there are other tasks that we need to consider as we start to roll out a new Kafka cluster. One of these tasks is making sure that clients can connect to our brokers."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Firewalls<sec><sec>Advertised listeners": [
    "Depending on our network configurations, we might need to serve clients that exist inside the network or those out of the network where the Kafka brokers are set up [3]. Kafka brokers can listen on multiple ports. For example, the default for a plain text port is 9092. An SSL port at 9093 can also be set up on that same host. Both of these ports might need to be open, depending on how clients connect to our brokers.",
    "In addition, ZooKeeper includes port 2181 for client connections. Port 2888 is used by follower ZooKeeper nodes to connect to the leader ZooKeeper node, and port 3888 is also used between ZooKeeper nodes to communicate [11]. If connecting remotely for JMX or other Kafka services (such as the REST Proxy), remember to account for any exposure of that port to other environments or users. In general, if we use any command line tools that require a port on the end of the hostname for Zoo- Keeper or Kafka servers, we need to make sure that these ports can be reached, espe- cially if a firewall is in place."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Metrics": [
    "One error when connecting that often appears like a firewall issue is using the lis- teners and advertised.listeners properties. Clients need to use the correct host- name, if given, to connect, so it will need to be a reachable hostname, however the rules are set up. For example, let\u2019s look at listeners versus advertised.listeners where those values might not be the same.",
    "Let\u2019s imagine we are connecting to a broker and can get a connection when the cli- ent starts, but not when it attempts to consume messages. How is this behavior that appears inconsistent possible? Remember that when a client starts, it connects to any broker to get metadata about which broker to connect to. The initial connection from the client uses the information that is located in the Kafka listeners configuration. What it gives back to the client to connect to next is the data in Kafka\u2019s advertised",
    ".listeners [12]. This makes it likely that the client will connect to a different host to do its work.",
    "Figure 9.3 shows how the client uses one hostname for the first connection attempt, then uses a different hostname on its second connection. This second host- name was given to the client from its initial call as the new location to connect to.",
    "An important setting to look at is inter.broker.listener.name, which determines how the brokers connect across the cluster to each other [12]. If the brokers cannot reach each other, replicas fail and the cluster will not be in a good state, to say the least! For an excellent explanation of advertised listeners, check out the article by Robin Mof- fatt, \u201cKafka Listeners \u2013 Explained,\u201d if you want to dig into more details [12]. Figure 9.3 was inspired by Robin Moffatt's diagrams on that site as well [12]."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Metrics<sec><sec>JMX console": [
    "In chapter 6, we looked at an example of setting up a way to see some JMX metrics from our application. The ability to see those metrics is the first step. Let\u2019s take a peek at finding some that are likely to highlight areas of concern."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tracing option": [
    "It is possible to use a GUI to explore the exposed metrics and get an idea of what is avail- able. VisualVM (https://visualvm.github.io/) is one example. Looking at the available JMX metrics can help us discover points of interest in which we might choose to add",
    "Scenario 1: no advertised listeners. Producer client starts and requests metadata from bootstrap server.",
    "Scenario 2: advertised listeners with URL resolved by both networks. Producer client requests metadata.",
    "Successful connection",
    "Bootstrap server",
    "Returns public advertised listener resolvable by both networks.",
    "Figure 9.3  Kafka\u2019s advertised listeners compared to listeners",
    "Successful connection",
    "alerts. When installing VisualVM, be sure to go through the additional step of installing the MBeans Browser.",
    "As noted in chapter 6, we must have JMX_PORT defined for each broker we want to connect to. This can be done with the environment variable in the terminal like so:",
    "export JMX_PORT=49999 [13]. Make sure that you correctly scope it to be separate for each broker as well as each ZooKeeper node.",
    "KAFKA_JMX_OPTS is also another option to look at for connecting remotely to Kafka brokers. Make sure to note the correct port and hostname. Listing 9.10 shows an example that sets KAFKA_JMX_OPTS with various arguments [13]. It uses port 49999 and the localhost as the hostname. In the listing, the other parameters allow us to con- nect without SSL and to not have to authenticate.",
    "Listing 9.10  Kafka JMX options",
    "Sets the hostname for",
    "Exposes this port for JMX",
    "KAFKA_JMX_OPTS=\"-Djava.rmi.server.hostname=127.0.0.1",
    "-Dcom.sun.management.jmxremote.local.only=false",
    "-Dcom.sun.management.jmxremote.rmi.port=49999",
    "-Dcom.sun.management.jmxremote.authenticate=false",
    "-Dcom.sun.management.jmxremote.ssl=false\"",
    "the localhost RMI server",
    "Allows remote connections Turns off authentication",
    "and SSL checks",
    "Let\u2019s take a look at a key broker metric and how to locate the value we need with the help of figure 9.4, which shows how to use a small MBeans representation to see the value of UnderReplicatedPartitions. Using a name such as",
    "kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions",
    "we can drill down what looks like a folder structure starting with kafka.server.",
    "We can drill down to find out if topic kinaction_alert",
    "has underreplicated partitions, for example.",
    "RequestQueueSize",
    "Figure 9.4 UnderReplicated- Partitions and RequestQueueSize locations",
    "Continuing on, we can then find the type ReplicaManager with the name attribute UnderReplicatedPartitions. RequestQueueSize is also shown in figure 9.4 as another example of finding a value [14]. Now that you know how to browse to specific values, let\u2019s go into detail about some of the most important things to look at on our servers.",
    "If you use Confluent Control Center or Confluent Cloud, most of these metrics are used in the built-in monitoring. The Confluent Platform suggests setting alerts on the following top three values to start with: UnderMinIsrPartitionCount, Under- ReplicatedPartitions, UnderMinIsr [14].",
    "Let\u2019s dig into a different monitoring option in the next section by looking at how we might leverage interceptors."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tracing option<sec><sec>Producer logic": [
    "The built-in metrics that we looked at so far can give us a great snapshot of current health, but what if we want to trace a single message through the system? What can we use to see a produced message and its consumed status? Let\u2019s talk about a simple but straightforward model that might work for our requirements.",
    "Let\u2019s say that we have a producer in which each event has a unique ID. Because each message is important, we do not want to miss any of these events. With one cli- ent, the business logic runs as normal and consumes the messages from the topic. In this case, it makes sense to log the ID of the event that was processed to a database or flat file. A separate consumer, let\u2019s call it an auditing consumer in this instance, fetches the data from the same topic and makes sure that there are no IDs missing from the processed entries of the first application. Though this process can work well, it does require adding logic to our application, and so it might not be the best choice. Figure 9.5 shows a different approach using Kafka interceptors. In practice, the interceptor that we define is a way to add logic to the producer, consumer, or both by hooking into the normal flow of our clients, intercepting the record, and adding our custom data before it moves along its normal path. Our changes to the clients are con-",
    "figuration-driven and help keep our specific logic out of the clients for the most part. Let\u2019s revisit the concept of interceptors that we touched on briefly in chapter 4, when introducing what producer interceptors could do for our messages. By adding an interceptor on both the producer and consumer clients that we are using, we can",
    "Producer client",
    "Sends initial message",
    "kinactionTraceId",
    "added at this step\tKafka",
    "Adds header trace ID to each record and then logs to standard out (stdout)",
    "kinactionTraceId read at this step",
    "Called before the consumer poll method. You look for and log your trace ID header to standard out (stdout).",
    "Consumes your record, header and all!",
    "Figure 9.5  Interceptors for tracing",
    "separate the monitoring logic from the application logic. The crosscutting concern of monitoring can, hopefully, be more encapsulated by this approach."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tracing option<sec><sec>Consumer logic": [
    "It is also interesting to note that we can have more than one interceptor, so we don\u2019t have to include all of our logic in one class; we can add and remove others later. The order in which we list the classes is important as that is the order in which the logic runs. The first interceptor gets the record from the producer client. If the interceptor modifies the record, other interceptors in the chain after the change would not see the same exact record as the first interceptor received [15].",
    "Let\u2019s start with looking at the Java interface ProducerInterceptor. We\u2019ll add this new interceptor to our Alert producer that we used in chapter 4. We will create a new class called AlertProducerMetricsInterceptor to add logic around alerts being pro- duced, as in listing 9.11. Implementing the interface, ProducerInterceptor, allows us to hook into the producer\u2019s interceptor lifecycle. The logic in the onSend method is called by the send() method from the normal producer client we have used so far [15]. In the listing, we\u2019ll also add a header called kinactionTraceId. Using a unique ID helps to confirm on the consumption side that we are seeing the same message at the end of its life cycle that was produced in the beginning of this step.",
    "Listing 9.11  AlertProducerMetricsInterceptor example",
    "Implements Producer-",
    "public class AlertProducerMetricsInterceptor implements ProducerInterceptor<Alert, String> {",
    "Interceptor to hook into the interceptor lifecycle",
    "final static Logger log = LoggerFactory.getLogger(AlertProducerMetricsInterceptor.class);",
    "public ProducerRecord<Alert, String> onSend(ProducerRecord<Alert, String> record) {",
    "Headers headers = record.headers();",
    "The producer client send",
    "method calls onSend.",
    "String kinactionTraceId = UUID.randomUUID().toString(); headers.add(\"kinactionTraceId\",",
    "kinactionTraceId.getBytes());",
    "Adds a custom header",
    "to the record to carry the generated ID across Kafka",
    "log.info(\"kinaction_info Created kinactionTraceId: {}\", kinactionTraceId);",
    "return record;",
    "Returns the modified record that includes our new header",
    "public void onAcknowledgement(",
    "RecordMetadata metadata, Exception exception)",
    "if (exception != null) {",
    "Calls onAcknowledgement when a record is acknowledged or an error occurs",
    "log.info(\"kinaction_error \" + exception.getMessage());",
    "log.info(\"kinaction_info topic = {}, offset = {}\", metadata.topic(), metadata.offset());",
    "// rest of the code omitted",
    "We also have to modify our existing AlertProducer class to register the new intercep- tor. We need to add the property interceptor.classes to the producer configura- tion with a value of the full package name of our new class: AlertProducer- MetricsInterceptor. Although we used the property name for clarity, remember that we can use the constant provided by the ProducerConfig class. In this case, we would use ProducerConfig.INTERCEPTOR_CLASSES_CONFIG [15]. The following listing shows this required modification.",
    "Listing 9.12  AlertProducer with interceptor configuration",
    "Properties kaProperties = new Properties();",
    "kaProperties.put(\"interceptor.classes\", AlertProducerMetricsInterceptor.class.getName());",
    "Producer<Alert, String> producer =",
    "new KafkaProducer<Alert, String>(kaProperties);",
    "Sets our interceptors (the value can be 1 or a comma-separated list).",
    "Overall, in this example, we have one interceptor that logs a unique ID for each pro- duced message. We add this ID as a header to the record so that when a consumer pulls this message, a corresponding consumer interceptor logs the ID that it has pro- cessed. The goal is to provide our own end-to-end monitoring that is outside of Kafka. By parsing the application logs, we will see messages like the following listing shows, which came from our AlertProducerMetricsInterceptor class.",
    "Listing 9.13  The alert interceptor output",
    "kinaction_info Created kinactionTraceId: 603a8922-9fb5-442a-a1fa-403f2a6a875d",
    "kinaction_info topic = kinaction_alert, offset ="
  ],
  "Part 2  Applying Kafka<sec><sec><sec>Tracing option<sec><sec>Overriding clients": [
    "The producer interceptor adds our logged value.",
    "Now that we have completed setting up an interceptor for sending a message, we need to see how to implement similar logic on the consumer end of our system. We want to validate that we can see the same header value that we added with the producer inter- ceptor on the consumption end. The following listing shows an implementation of ConsumerInterceptor to help retrieve this header [16].",
    "Listing 9.14  AlertConsumerMetricsInterceptor example",
    "public class AlertConsumerMetricsInterceptor implements ConsumerInterceptor<Alert, String> {",
    "public ConsumerRecords<Alert, String> onConsume(ConsumerRecords<Alert, String> records) {",
    "if (records.isEmpty()) { return records;",
    "Implements Consumer- Interceptor so Kafka recognizes our interceptor",
    "for (ConsumerRecord<Alert, String> record : records) {",
    "Logs the custom",
    "Headers headers = record.headers(); for (Header header : headers) {",
    "if (\"kinactionTraceId\".equals( header.key())) {",
    "Loops through each",
    "record\u2019s headers",
    "header to standard output",
    "log.info(\"KinactionTraceId is: \" + new String(header.value()));",
    "return records;",
    "Returns the records to continue with callers from our interceptor",
    "In a fashion similar to our producer, in this listing, we used a consumer-specific inter- face, ConsumerInterceptor, to make our new interceptor. We looped through all the records and their headers to find any that had our custom kinactionTraceId as the key and sent them to standard output. We also modified our existing AlertConsumer class to register our new interceptor. The property name interceptor.classes needs to be added to the consumer configuration with a value of the full package name of our new class: AlertConsumerMetricsInterceptor. The following listing shows this required step.",
    "Listing 9.15  AlertConsumer with interceptor configuration",
    "public class AlertConsumer {",
    "Properties kaProperties = new Properties();",
    "kaProperties.put(\"group.id\",",
    "\"kinaction_alertinterceptor\"); kaProperties.put(\"interceptor.classes\",",
    "AlertConsumerMetricsInterceptor.class.getName());",
    "Uses a new group.id to ensure starting with our current offsets (and not one from a previous group.id)",
    "Required property name to add our custom interceptor and class value",
    "We can include a comma-separated list if we have more than one class we need to use [16]. Although we used the property name for clarity, remember that we can use the constant provided by the ConsumerConfig class. In this case, we would use Consumer- Config.INTERCEPTOR_CLASSES_CONFIG [16]. Although we can see the usage of an intercep-tor on both ends of our flow, there is also another way to add functionality to client code\u2014overriding clients."
  ],
  "Part 2  Applying Kafka<sec><sec><sec>General monitoring tools": [
    "If we control the source code for clients that other developers will use, we can subclass an existing client or create our own that implements the Kafka producer/consumer interfaces. At the time of writing, the Brave project (https://github.com/openzipkin/ brave) has an example of one such client that works with tracing data.",
    "For those not familiar with Brave, it is a library meant to help add instrumentation for distributed tracing. It has the ability to send this data to something, for example, like a Zipkin server (https://zipkin.io/), which can handle the collection and search of this data. If interested, please take a peek at the TracingConsumer class (http:// mng.bz/6mAo) for a real-world example of adding functionality to clients with Kafka. We can decorate both the producer and consumer clients to enable tracing (or any custom logic), but we\u2019ll focus on the consumer client in the following stub example. The code in listing 9.16 is a section of pseudo code to add custom logic to the normal Kafka consumer flow. Developers wanting to consume messages with the custom logic can use an instance of KInActionCustomConsumer, which includes a reference to a reg- ular consumer client named normalKafkaConsumer (in the custom consumer client itself) in this listing. The custom logic is added to provide needed behavior while still interacting with the traditional client. Your developers work with your consumer,",
    "which handles the normal client behind the scenes.",
    "Listing 9.16  Custom consumer client",
    "final class KInActionCustomConsumer<K, V> implements Consumer<K, V> {",
    "final Consumer<K, V> normalKafkaConsumer;",
    "public ConsumerRecords<K, V> poll( final Duration timeout)",
    "Uses the normal Kafka consumer client in our custom consumer",
    "Consumers still call the interface",
    "methods they are used to.",
    "//Custom logic here",
    "// Normal Kafka consumer used as normal return normalKafkaConsumer.poll(timeout);",
    "Adds our custom logic where needed",
    "Uses the normal Kafka consumer client to provide its normal duties",
    "This listing only shows a comment indicating where your logic would go, but your users are abstracted from using the normal client methods if desired while still running any custom code such as checking for duplicate data submissions or logging tracing data from headers. The added behavior is not getting in the way of the normal client."
  ],
  "Part 3  Going further": [
    "1  \u201cClass AdminClient.\u201d Confluent documentation (n.d.). https://docs.confluent",
    ".io/5.3.1/clients/javadocs/index.html?org/apache/kafka/clients/admin/  AdminClient.html (accessed November 17, 2020).",
    "2 \u201ckcat.\u201d GitHub. https://github.com/edenhill/kcat/#readme (accessed August 25, 2021).",
    "3 \u201cKafka Security & the Confluent Platform.\u201d Confluent documentation (n.d.). https://docs.confluent.io/2.0.1/kafka/platform-security.html#kafka-security-  the-confluent-platform (accessed August 25, 2021).",
    "4 \u201cConfluent REST APIs: Overview: Features.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/kafka-rest/index.html#features (accessed February 20, 2019).",
    "5 \u201cREST Proxy Quick Start.\u201d Confluent documentation (n.d.). https://docs.con- fluent.io/platform/current/kafka-rest/quickstart.html (accessed February 22, 2019).",
    "6 \u201cUsing Confluent Platform systemd Service Unit Files.\u201d Confluent documenta- tion (n.d.). https://docs.confluent.io/platform/current/installation/scripted- install.html#overview (accessed January 15, 2021).",
    "7 \u201cClass RollingFileAppender.\u201d Apache Software Foundation (n.d.). https:// logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/RollingFileAppender",
    ".html (accessed April 22, 2020).",
    "8 log4j.properties. Apache Kafka GitHub (March 26, 2020). https://github",
    ".com/apache/kafka/blob/99b9b3e84f4e98c3f07714e1de6a139a004cbc5b/ config/log4j.properties (accessed June 17, 2020).",
    "9  \u201cRunning ZooKeeper in Production.\u201d Confluent documentation (n.d.). https:/",
    "/docs.confluent.io/platform/current/zookeeper/deployment.html#running- zk-in-production (accessed July 23, 2021).",
    "10 \u201cZooKeeper Administrator\u2019s Guide.\u201d Apache Software Foundation (n.d.). https://zookeeper.apache.org/doc/r3.4.5/zookeeperAdmin.html (accessed June 10, 2020).",
    "11 \u201cZooKeeper Getting Started Guide.\u201d Apache Software Foundation (n.d.). https://zookeeper.apache.org/doc/r3.1.2/zookeeperStarted.html (accessed August 19, 2020).",
    "12  R. Moffatt. \u201cKafka Listeners \u2013 Explained.\u201d Confluent blog (July 1, 2019). https:/",
    "/www.confluent.io/blog/kafka-listeners-explained/ (accessed June 11, 2020).",
    "13 \u201cKafka Monitoring and Metrics Using JMX.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/docker/operations/ monitoring.html (accessed June 12, 2020).",
    "14 \u201cMonitoring Kafka: Broker Metrics.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/5.4.0/kafka/monitoring.html#broker-metrics (accessed May 1, 2020).",
    "15 \u201cInterface ProducerInterceptor.\u201d Apache Software Foundation (n.d.). https:// kafka.apache.org/27/javadoc/org/apache/kafka/clients/producer/Producer Interceptor.html (accessed June 1, 2020).",
    "16 \u201cInterface ConsumerInterceptor.\u201d Apache Software Foundation (n.d.). https:// kafka.apache.org/27/javadoc/org/apache/kafka/clients/consumer/Consumer Interceptor.html (accessed June 1, 2020).",
    "17 \u201cMonitoring.\u201d Apache Software Foundation (n.d.). https://kafka.apache.org/ documentation/#monitoring (accessed May 1, 2020).",
    "18 Yahoo CMAK README.md. GitHub (March 5, 2020). https://github.com/",
    "yahoo/CMAK/blob/master/README.md (accessed July 20, 2021).",
    "19 README.md. LinkedIn Cruise Control for Apache Kafka GitHub (June 30, 2021). https://github.com/linkedin/cruise-control/blob/migrate_to_kafka_2",
    "_4/README.md (acces-sed July 21, 2021)."
  ],
  "Part 3  Going further<sec><sec><sec>Security basics": [
    "art 3 focuses on how to further our use of Kafka beyond what part 2 covered as the core pieces of Kafka. In this part, we go further than just having a Kafka cluster that we can read and write data to. We add more security, data schemas, and look at other Kafka products.",
    "In chapter 10, we look at strengthening a Kafka cluster by using SSL, ACLs, and options like quotas.",
    "In chapter 11, we dig into the Schema Registry and how it is used to help data evolve in compatible ways.",
    "In chapter 12, we look at Kafka Streams and ksqlDB.",
    "These pieces are all part of the Kafka ecosystem and are higher levels of abstrac- tion built on the core subjects you studied in part 2. At the end of this part, you\u2019ll be ready to dig into even more advanced Kafka topics on your own and, even better, you\u2019ll be able to use Kafka in your day-to-day workflow.",
    "This chapter focuses on keeping our data secured so that only those that need to read from or write to it have access. Because security is a huge area to cover, in this chapter, we will talk about some basic concepts to get a general background on the options we have in Kafka. Our goal in this chapter is not to set up security, but to learn some different options that you can talk with your security team on research- ing in the future and get familiar with the concepts. This will not be a complete guide to security in general, but sets the foundation for you. We will discuss practi- cal actions you can take in your own setup, and we will look at the client impact, as well as brokers and ZooKeeper, to make our cluster more secure.",
    "Your data might not need those protections we discuss, but knowing your data is key to deciding if you need the trade-offs of managing access. If you are handling",
    "anything related to personal information or financial data, like date of birth or credit card numbers, then you will likely want to look at most of the security options discussed in this chapter. However, if you are only handling generic information such as market- ing campaigns, or you are not tracking anything of a secure nature, then you might not need this protection. If this is the case, then your cluster would not need to introduce features like SSL. We start with an example of fictional data that we want to protect.",
    "Let\u2019s imagine that we have a goal to find the location of a prize by taking part in a treasure hunt. As a competition-wide exercise, we have two teams, and we do not want the other team to access our own team\u2019s work. Starting out, each team picks their own topic names and shares that name with their team members only. (Without knowing which topic name to write to and read from, your data is out of the view of the other team.) Each team begins by sending their clues to what they assume is their own private topic. Over time, members of the teams might start to wonder about the progress of the other team and whether they have any clues that the other team doesn\u2019t. This is when the trouble starts. Figure 10.1 shows the topic setup for Team Clueful and Team Clueless.",
    "At this point, nothing but team behavior is stopping the teams from reading each topic. Each team can read and write to each topic.",
    "Figure 10.1  Treasure hunt topics",
    "One tech-savvy competitor, who coincidentally has used Kafka before, reaches for his command line tools to find the topics (the other team\u2019s as well as his own). After get- ting a list of topics, the competitor now knows his rival\u2019s topic. Let\u2019s say that this team member of Team Clueless looks at Team Clueful\u2019s topic, --topic kinaction",
    "_clueful_secrets. With great happiness, all it took was a consumer console com- mand to list all the data that Team Clueful has been working on so far in the competi- tion! But the bad actor does not stop there.",
    "In order to throw Team Clueful off the trail, the actor also writes false information into the channel. Now Team Clueful has bad data in their topic, which is hindering their clue-solving progress! Because they are not sure who really wrote the messages on their topic, Team Clueful now has to determine which are the false messages and,",
    "in doing so, will lose valuable time that they could be using to work on figuring out the grand-prize location.",
    "How could we avoid the situation Team Clueful finds itself in? Is there a way that only those clients that have permission would be able to read from or write to our top- ics? There are two parts to our solution. The first part is how to encrypt our data. The next is how to find out who a person is in our system; not only who they are, but also making sure that the claimed identity of the user is verified. Once we verify a user, we need to know what they are permitted do in our system. We will dive deeper into these topics as we look at a few solutions provided with Kafka."
  ],
  "Part 3  Going further<sec><sec><sec>Security basics<sec><sec>Encryption with SSL": [
    "In regard to computer application security, you will likely encounter encryption, authentication, and authorization at some point in your work. Let\u2019s take a closer look at this terminology (see http://mng.bz/o802 for more detail of the following terms if needed).",
    "Encryption does not mean that others might not see your messages, but that if they do, they will not be able to derive the original content that you are protecting. Many people will think of how they are encouraged to use a site that is secure (HTTPS) for online shopping on a Wi-Fi\u00ae network. Later, we are going to enable SSL (Secure Sock- ets Layer) for our communication, not between a website and our computer, but between our clients and brokers! As a general note, as we work through this chapter, the label \u201cSSL\u201d is the property name you will see in our examples and explanations even though TLS is the newer protocol version [1].",
    "Moving along, let\u2019s talk about authentication. To verify the identity of a user or an application, we need to have a way to authenticate that user: authentication is the pro- cess of proving that a user or application is indeed who they claim to be. If you wanted to sign up for a library card, for example, does the library issue a card to anyone with- out making sure the user is who they say they are? In most cases, the library would con- firm the person\u2019s name and address with something like a government-issued ID and a utility bill. This process is intended to ensure that someone cannot easily claim another identity to use for their own purposes. If someone claims your identity to bor- row books and never returns them, sending the fines your way, you can easily see a drawback of not confirming the user\u2019s claim.",
    "Authorization, on the other hand, focuses on what the user can do. Continuing with our library example, a card issued to an adult might provide different permissions than if it was given to a user considered to be a child. And access to online publica- tions might be limited to only terminals inside the library for each cardholder."
  ],
  "Part 3  Going further<sec><sec><sec>Security basics<sec><sec>SSL between brokers and clients": [
    "So far, all of our brokers in this book have supported plaintext [1]. In effect, there has been no authentication or encryption over the network. Knowing this, it might make sense to review one of the broker server configuration values. If you look at any of your current server.properties files (see appendix A for your setup location of the",
    "config/server0.properties file, for example), you will find an entry like listeners = PLAINTEXT:localhost//:9092. That listener is, in effect, providing a mapping of a protocol to a specific port on the broker. Because brokers support multiple ports, this entry allows us to keep the PLAINTEXT port up and running, so we can test adding SSL or other protocols on a different port. Having two ports helps to make our transition smoother when we shift away from plaintext [2]. Figure 10.2 shows an example of using plaintext versus SSL.",
    "Figure 10.2  Plaintext vs. SSL",
    "At this point, we are starting with a cluster without any security baked in. (Luckily, we can add various pieces to our cluster as we harden it against other teams.) Setting up SSL between the brokers in our cluster and our clients is one place to start [1]. No extra servers or directories are needed. No client coding changes are required, as the changes are configuration driven.",
    "We don\u2019t know how advanced other users are when it comes to listening to our traffic on the same Wi-Fi network with security tools, so we know that we might not want to send plaintext from our brokers to our clients. Although the setup in the fol- lowing section is needed for Kafka security, readers who have set up SSL or HTTPS in the past (and especially with Java) will find this approach similar to other client/ server trust arrangements."
  ],
  "Part 3  Going further<sec><sec><sec>Security basics<sec><sec>SSL between brokers": [
    "In our previous examples of writing clients and connecting to Kafka, we have not used SSL for connections. However, now we are going to look at turning it on for the",
    "communication between our clients and our cluster to encrypt our network traffic with SSL. Let\u2019s walk through the process and see what we are going to need to accom- plish in order to get our cluster updated with this feature.",
    "NOTE The commands in this chapter are specific and will not work the same on all operating systems (or even across different server domain names listed for broker setup) without modification. The important thing is to follow along with the general concepts. Moreover, other tools (like OpenSSL\u00ae) can be switched out, so your setup and commands might be different. But once you get the concepts, head to Confluent\u2019s site at http://mng.bz/nrza for even more resources and guides. Confluent\u2019s documents that provided direction for any examples are referenced throughout this chapter and should be refer- enced to help you actually implement the topics we only cover at a high level in order to introduce the following concepts.",
    "WARNING A security professional should be consulted for the correct way to set up your own environment. Our commands are meant as a guide for get- ting familiar and for learning, not as a production level of security. This is not a complete guide. Use it at your own risk!",
    "One of our first steps is to create a key and certificate for our brokers [3]. Because you should already have Java on your machine, one option is to use the keytool utility, which is part of the Java installation. The keytool application manages a keystore of keys and trusted certificates [4]. The important part to note is the storage. In this chap- ter, the term broker0 is included in some filenames to identify one specific broker, not one that is meant for every broker. It might be good to think of a keystore as a data- base where our JVM programs can look up this information for our processes when needed [4]. At this point, we are also going to generate a key for our brokers as in the following listing [3]. Note that manning.com is used as an example in the following listings and is not intended to be used for readers following along.",
    "Listing 10.1  SSL key generation for a broker",
    "keytool -genkey -noprompt \\",
    "-alias localhost \\",
    "-dname \"CN=ka.manning.com,OU=TEST,O=TREASURE,L=Bend,S=Or,C=US\" \\",
    "-keystore kafka.broker0.keystore.jks \\",
    "-keyalg RSA \\",
    "Names the keystore that holds our newly generated key",
    "-storepass changeTreasure \\",
    "-keypass changeTreasure \\",
    "-validity 999",
    "Uses a password so that the store cannot be changed without it",
    "After running this command, we will have created a new key and stored it in the key- store file kafka.broker0.keystore.jks. Because we have a key that (in a way) identifies our broker, we need something to signal that we don\u2019t have just any certificate issued by a random user. One way to verify our certificates is by signing them with a CA (certificate",
    "authority). You might have heard of CAs offered by Let\u2019s Encrypt\u00ae (https://letsen crypt.org/) or GoDaddy\u00ae (https://www.godaddy.com/), to name a few sources. The role of a CA is to act as a trusted authority that certifies the ownership and identity of a public key [3]. In our examples, however, we are going to be our own CA to avoid any need of verifying our identity by a third party. Our next step is to create our own CA, as the following listing shows [3].",
    "Listing 10.2  Creating our own certificate authority",
    "openssl req -new -x509 \\",
    "-keyout cakey.crt -out ca.crt \\",
    "-days 999 \\",
    "Creates a new CA, then produces key and certificate files",
    "-subj '/CN=localhost/OU=TEST/O=TREASURE/L=Bend/S=Or/C=US' \\",
    "-passin pass:changeTreasure -passout pass:changeTreasure",
    "This generated CA is now something that we want to let our clients know that they should trust. Similar to the term keystore, we will use a truststore to hold this new information [3].",
    "Because we generated our CA in listing 10.2, we can use it to sign our certificates for our brokers that we have already made. First, we export the certificate that we gen- erated in listing 10.2 for each broker from the keystore, sign that with our new CA, and then import both the CA certificate and signed certificate back into the keystore [3]. Confluent also provides a shell script that can be used to help automate similar commands (see http://mng.bz/v497) [3]. Check out the rest of our commands in the source code for the book in the section for this chapter.",
    "NOTE While running the commands in these listings, your operating system or tool version may have a different prompt than that passed. It will likely have a user prompt appear after running your command. Our examples try to avoid these prompts.",
    "As part of our changes, we also need to update the server.properties configuration file on each broker, as the following listing shows [3]. Note that this listing only shows broker0 and only part of the file.",
    "Listing 10.3  Broker server properties changes",
    "listeners=PLAINTEXT://localhost:9092,",
    "\u27a5 SSL://localhost:9093 ssl.truststore.location=",
    "\u27a5 /opt/kafkainaction/private/kafka",
    "\u27a5 .broker0.truststore.jks",
    "ssl.truststore.password=changeTreasure",
    "ssl.keystore.location=",
    "Adds the SSL broker port, leaving the older PLAINTEXT port",
    "Provides the truststore location and password for our broker",
    "Provides the keystore location",
    "\u27a5 /opt/kafkainaction/kafka.broker0.keystore.jks ssl.keystore.password=changeTreasure ssl.key.password=changeTreasure",
    "and password for our broker",
    "Changes are also needed for our clients. For example, we set the value security",
    ".protocol=SSL, as well as the truststore location and password in a file called custom",
    "-ssl.properties. This helps set the protocol used for SSL as well as points to our truststore [3].",
    "While testing these changes, we can also have multiple listeners set up for our bro- ker. This also helps clients migrate over time, as both ports can serve traffic before we drop the older PLAINTEXT port for our clients [3]. The kinaction-ssl.properties file helps our clients provide the information needed to interact with the broker that is now becoming more secured!",
    "Listing 10.4  Using SSL configuration for command line clients",
    "bin/kafka-console-producer.sh --bootstrap-server localhost:9093 \\",
    "--topic kinaction_test_ssl \\",
    "--producer.config kinaction-ssl.properties",
    "bin/kafka-console-consumer.sh --bootstrap-server localhost:9093 \\",
    "--topic kinaction_test_ssl \\",
    "Lets our producer know about the SSL details",
    "--consumer.config kinaction-ssl.properties",
    "Uses our SSL configuration for consumers",
    "One of the nicest features is that we can use the same configuration for both produc- ers and consumers. As you look at the contents of this configuration file, one issue that might spring to mind is the use of passwords in these files. The most straightfor- ward option is to make sure that you are aware of the permissions around this file. Limiting the ability to read as well as the ownership of the file is important to note before placing this configuration on your filesystem. As always, consult your security experts for better options that might be available for your environment."
  ],
  "Part 3  Going further<sec><sec><sec>Kerberos and the Simple Authentication and Security Layer (SASL)": [
    "Another detail to research since we also have our brokers talking to each other is that we might want to decide if we need to use SSL for those interactions. We can use security.inter.broker.protocol = SSL in the server properties if we do not want to continue using plaintext for communications between brokers and consider a port change as well. More details can be found at http://mng.bz/4KBw [5]."
  ],
  "Part 3  Going further<sec><sec><sec>Authorization in Kafka": [
    "If you have a security team that already has a Kerberos server, you likely have some security experts to ask for help. When we first started working with Kafka, it was with a part of a suite of big data tools that mostly used Kerberos. Kerberos is often found in organizations as a method to provide single sign-on (SSO) that is secure.",
    "If you have a Kerberos server set up already, you need to work with a user with access to that Kerberos environment to create a principal for each broker and also for each user (or application ID) that will access the cluster. Because this setup might be too involved for local testing, follow along with this discussion to see the format of Java",
    "Authentication and Authorization Service (JAAS) files, which is a common file type for brokers and clients. There are great resources at http://mng.bz/QqxG if you want to gain more details [6].",
    "JAAS files, with keytab file information, help to provide Kafka with the principal and credentials that we will use. A keytab will likely be a separate file that has the principal and encrypted keys. We can use this file to authenticate to the Kafka brokers without requiring a password [7]. However, it is important to note that you need to treat your keytab file with the same security and care that you would for any credential.",
    "To get our brokers set up, let\u2019s look at some server property changes we\u2019ll need to make and an example JAAS configuration. To start, each broker will need its own key- tab file. Our JAAS file will help our brokers find the keytab\u2019s location on our server, as well as declare the principal to use [7]. The following listing shows an example JAAS file brokers would use on startup.",
    "Listing 10.5  Broker SASL JAAS file",
    "KafkaServer {",
    "keyTab=\"/opt/kafkainaction/kafka_server0.keytab\" principal=\"kafka/kafka0.ka.manning.com@MANNING.COM\";",
    "Sets up the Kafka broker JAAS file",
    "We are going to add another port to test SASL_SSL before we remove the older ports [7]. The following listing shows this change. Depending on what port you used to connect to your brokers, the protocol is either PLAINTEXT, SSL, or SASL_SSL in this example.",
    "Listing 10.6  Changing the broker SASL properties",
    "listeners=PLAINTEXT://localhost:9092,SSL://localhost:9093,",
    "\u27a5 SASL_SSL://localhost:9094",
    "Adds the SASL_SSL broker port, leaving the older ports",
    "The setup for a client is similar [7]. A JAAS file is needed, as the following listing shows.",
    "Listing 10.7  Client SASL JAAS file",
    "KafkaClient {",
    "keyTab=\"/opt/kafkainaction/kafkaclient.keytab\" principal=\"kafkaclient@MANNING.COM\";",
    "Adds the client SASL JAAS file entry",
    "We also need to update client configuration for the SASL values [3]. The client file is similar to our kinaction-ssl.properties file used earlier, but this one defines the SASL",
    "_SSL protocol. After testing that things are not broken on port 9092 or 9093, we can",
    "use our new configuration by validating the same result as before when we use our new SASL_SSL protocol."
  ],
  "Part 3  Going further<sec><sec><sec>Authorization in Kafka<sec><sec>Access control lists (ACLs)": [
    "Now that we have seen how to use authentication with Kafka, let\u2019s take a look at how we can start using that information to enable user access. For this discussion, we\u2019ll start with access control lists."
  ],
  "Part 3  Going further<sec><sec><sec>Authorization in Kafka<sec><sec>Role-based access control (RBAC)": [
    "As a quick review, authorization is the process that controls what a user can do. One way to enable authorization is with access control lists (ACLs). Although most Linux users are familiar with permissions on a file they can control with a chmod command (such as read, write, and execute), one drawback is that the permissions might not be flexible enough for our needs. ACLs can provide permissions for multiple individuals and groups as well as more types of permissions, and they are often used when we need different levels of access for a shared folder [8]. One example is a permission to let a user edit a file but not allow the same user to delete it (delete is a separate per- mission altogether). Figure 10.3 shows Franz\u2019s access to the resources for our hypo- thetical team for our treasure hunt.",
    "Principal is allowed or denied operation on resources.",
    "User Franz is allowed read/write on resource topic",
    "kinaction_clueful_secrets.",
    "Figure 10.3 Access control lists (ACLs)",
    "Kafka designed their authorizer to be pluggable, which allows users to make their own logic if desired [8]. Kafka has a SimpleAclAuthorizer class that we will use in our example.",
    "Listing 10.8 shows adding the authorizer class and superuser Franz to the broker\u2019s server.properties file in order to use ACLs. An important item to note is that once we configure an authorizer, we need to set ACLs, or only those considered superusers will have access to any resources [8].",
    "Listing 10.8  ACL authorizer and superusers",
    "authorizer.class.name=",
    "\u27a5 kafka.security.auth.SimpleAclAuthorizer",
    "Every broker configuration should include the SimpleAclAuthorizer.",
    "super.users=User:Franz",
    "Adds a superuser that can access all",
    "resources with or without ACLs",
    "Let\u2019s see how to grant access to Team Clueful so that only that team produces and con- sumes from their own topic, kinaction_clueful_secrets. For brevity, we use two users in our example team, Franz and Hemingway. Because we have already created the keytabs for the users, we know the principal information that we need. As you may notice in the following listing, the operation Read allows consumers the ability to get data from the topic [8]. The second operation, Write, allows the same principals to produce data into the topic.",
    "Listing 10.9  Kafka ACLs to read and write to a topic",
    "bin/kafka-acls.sh --authorizer-properties \\",
    "--bootstrap-server localhost:9094 --add \\",
    "--allow-principal User:Franz \\",
    "--allow-principal User:Hemingway \\",
    "--operation Read --operation Write \\",
    "--topic kinaction_clueful_secrets",
    "Identifies two users to grant permissions",
    "Allows the named principals to both read from and write to the specific topic",
    "The kafka-acls.sh CLI tool is included with the other Kafka scripts in our installa- tion and lets us add, delete, or list current ACLs [8]."
  ],
  "Part 3  Going further<sec><sec><sec>ZooKeeper": [
    "Role-based access control (RBAC) is an option that the Confluent Platform supports. RBAC is a way to control access based on roles [9]. Users are assigned to their role according to their needs (such as a job duty, for example). Instead of granting every user permissions, with RBAC, you manage the privileges assigned to predefined roles [9]. Figure 10.4 shows how adding a user to a role gives them a new permission assignment.",
    "Permissions",
    "User added. Nothing changes besides a user added to the role.",
    "Figure 10.4",
    "Role-based access control (RBAC)",
    "For our treasure hunting teams, it might make sense to have a specific role per team. This might mirror how a team from marketing would have a role versus a team from accounting. If some user changes departments, their role would be reassigned and not their individual permissions. Because this is a newer option, which may change as it matures and which is geared to the Confluent Platform environment, this is men- tioned for awareness. We will not dig further into it here."
  ],
  "Part 3  Going further<sec><sec><sec>ZooKeeper<sec><sec>Kerberos setup": [
    "Part of securing Kafka is looking at how we can secure all parts of our cluster, includ- ing ZooKeeper. If we protect the brokers but not the system that holds that security- related data, it is possible for those with knowledge to update security values without much effort. To help protect our metadata, we will need to set the value zookeeper",
    ".set.acl to true per broker, as shown in the following listing [10].",
    "Listing 10.10  ZooKeeper ACLs",
    "zookeeper.set.acl=true"
  ],
  "Part 3  Going further<sec><sec><sec>Quotas": [
    "Every broker configuration includes this ZooKeeper value.",
    "Making sure that ZooKeeper works with Kerberos requires a variety of configuration changes. For one, in the zookeeper.properties configuration file, we want to add those values that let ZooKeeper know that SASL should be used for clients and which pro- vider to use. Refer to http://mng.bz/Xr0v for more details if needed [10]. While we were busy looking at the other options for setup so far in this chapter, some users on our treasure hunt system were still up to no good. Let\u2019s see if we can dig into the sub- ject of quotas to help with that."
  ],
  "Part 3  Going further<sec><sec><sec>Quotas<sec><sec>Network bandwidth quota": [
    "Let\u2019s say that some users of our web application don\u2019t have any issues with requesting data repeatedly. Although this is often a good thing for end users that want to use a ser- vice as much as they want without their progress being limited, our cluster may need some protection from users who might use that to their advantage. In our example, because we made it so the data was accessed by members of our team only, some users on the opposing team thought of a new way to prevent members of our team from working successfully. In effect, they are trying to use a distributed denial-of-service (DDoS) attack against our system [11]!",
    "A targeted attack against our cluster can overwhelm our brokers and their sur- rounding infrastructure. In practice, the other team is requesting reads from our top- ics over and over while reading from the beginning of the topics each time they request data. We can use quotas to prevent this behavior. One detail that\u2019s important to know is that quotas are defined on a per-broker basis [11]. The cluster does not look across each broker to calculate a total, so a per-broker definition is needed. Fig- ure 10.5 shows an example of using a request percentage quota.",
    "Clients all with Client IDs from kinaction_clueless_secrets would get delays after too many fetches.",
    "Constant polling",
    "Each broker\u2019s quotas are treated separate from other brokers.",
    "Figure 10.5  Quotas",
    "To set our own custom quotas, we need to know how to identify who to limit and the limit we want to set. Whether we have security or not impacts what options we have for defining who we are limiting. Without security, we are able to use the client.id prop- erty. With security enabled, we can also add the user and any user and client.id combinations as well [11]. There are a couple of types of quotas that we can look at defining for our clients: network bandwidth and request rate quotas. Let\u2019s take a look at the network bandwidth option first."
  ],
  "Part 3  Going further<sec><sec><sec>Quotas<sec><sec>Request rate quotas": [
    "Network bandwidth is measured by the number of bytes per second [12]. In our example, we want to make sure that each client is respecting the network and not flooding it to prevent others from using it. Each user in our competition uses a client ID that is specific to their team for any producer or consumer requests from their cli- ents. In the following listing, we\u2019ll limit the clients using the client ID kinaction",
    "_clueful by setting a producer_byte_rate and a consumer_byte_rate [13].",
    "Listing 10.11  Creating a network bandwidth quota for client kinaction_clueful",
    "bin/kafka-configs.sh --bootstrap-server localhost:9094 --alter \\",
    "--add-config 'producer_byte_rate=1048576,",
    "\u27a5 consumer_byte_rate=5242880' \\",
    "--entity-type clients --entity-name kinaction_clueful",
    "Names the entity for a client with the client.id kinaction_clueful",
    "Limits producers to 1 MB per second and consumers to 5 MB per second",
    "We used the add-config parameter to set both the producer and consumer rate. The entity-name applies the rule to our specific kinaction_clueful clients. As is often the case, we might need to list our current quotas as well as delete them if they are no longer needed. All of these commands can be completed by sending different argu- ments to the kafka-configs.sh script, as the following listing shows [13].",
    "Listing 10.12  Listing and deleting a quota for client kinaction_clueful",
    "bin/kafka-configs.sh --bootstrap-server localhost:9094 \\",
    "--describe \\",
    "--entity-type clients --entity-name kinaction_clueful",
    "Lists the existing configuration of our client.id",
    "bin/kafka-configs.sh --bootstrap-server localhost:9094 --alter \\",
    "--delete-config",
    "\u27a5 'producer_byte_rate,consumer_byte_rate' \\",
    "--entity-type clients --entity-name kinaction_clueful",
    "Uses delete-config to remove those we just added",
    "The --describe command helps us get a look at the existing configuration. We can then use that information to decide if we need to modify or even delete the configura- tion by using the delete-config parameter.",
    "As we start to add quotas, we might end up with more than one quota applied to a client. We need to be aware of the precedence in which various quotas are applied. Although it might seem like the most restrictive setting (the lowest bytes allowed) would be the highest for quotas, that is not always the case. The following is the order in which quotas are applied with the highest precedence listed at the top [14]:",
    "User- and client.id-provided quotas",
    "User quotas",
    "client.id quotas",
    "For example, if a user named Franz has a user-quota limit of 10 MB and a client.id limit of 1 MB, the consumer he uses would be allowed 10 MB per second due to the user-defined quota having higher precedence."
  ],
  "Part 3  Going further<sec><sec><sec>Data at rest": [
    "The other quota to examine is request rate. Why the need for a second quota? Although a DDoS attack is often thought of as a network issue, clients making lots of connec- tions could still overwhelm the broker by making CPU-intensive requests. Consumer",
    "clients that poll continuously with a setting of fetch.max.wait.ms=0 are also a con- cern that can be addressed with request rate quotas, as shown in figure 10.5 [15].",
    "To set this quota, we use the same entity types and add-config options as we did with our other quotas [13]. The biggest difference is setting the configuration for request_percentage. You\u2019ll find a formula that uses the number of I/O threads and the number of network threads at http://mng.bz/J6Yz [16]. In the following listing, we set a request percentage of 100 for our example [13].",
    "Listing 10.13  Creating a network bandwidth quota for client kinaction_clueful",
    "bin/kafka-configs.sh --bootstrap-server localhost:9094 --alter \\",
    "--add-config 'request_percentage=100' \\",
    "--entity-type clients --entity-name kinaction_clueful",
    "Names the entity for our client.id kinaction_clueful",
    "Allows producers a request rate quota of 100%",
    "Using quotas is a good way to protect our cluster. Even better, it lets us react to clients that suddenly might start putting a strain on our brokers."
  ],
  "Part 3  Going further<sec><sec><sec>Data at rest<sec><sec>Managed options": [
    "Another thing to consider is whether you need to encrypt the data that Kafka writes to disk. By default, Kafka does not encrypt the events it adds to its logs. There have been a couple of Kafka Improvement Proposals (KIPs) that have looked at this feature, but at the time of publication, you will still need to make sure you have a strategy that meets your requirements. Depending on your business needs, you might want to only encrypt specific topics or even specific topics with unique keys."
  ],
  "Part 3  Going further<sec><sec><sec>Summary": [
    "If you like a more interactive way to gather information and a great place to search for or ask questions, visit the Confluent Community page (https://www.confluent.io/ community/). You\u2019ll find a Slack group with channels focusing on Kafka\u2019s specific parts, such as clients, Connect, and many other Kafka topics. The number of detailed questions that others have posted (and that you can post) shows the breadth of expe- riences users are willing to explore and share. There is also a community forum where you can introduce yourself and meet other vibrant members.",
    "Throughout this chapter, you have expanded your knowledge to learn about the further abstractions of KStreams and ksqlDB and how these relate to your core knowledge of Kafka. As the Kafka ecosystem evolves and changes, or even adds new products, we are confident that Kafka\u2019s foundations presented here will help you understand what is going on internally. Good luck on your continuing Kafka learnings!"
  ],
  "Part 3  Going further<sec><sec><sec>References": [
    "Kafka has many features that you can use for simple use cases or all the way up to being the major system of an enterprise.",
    "Schemas help version our data changes.",
    "The Schema Registry, a Confluent offering apart from Kafka, provides a way to work with Kafka-related schemas.",
    "As schemas change, compatibility rules help users know whether the changes are backward, forward, or fully compatible.",
    "If schemas are not an option, different topics can be used to handle different versions of data."
  ],
  "Part 3  Going further<sec><sec><sec>A proposed Kafka maturity model": [
    "1 \u201cEncryption and Authentication with SSL.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/kafka/authentication_ssl.html (accessed June 10, 2020).",
    "2 \u201cAdding security to a running cluster.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/kafka/incremental-security-upgrade.html #adding-security-to-a-running-cluster (accessed August 20, 2021).",
    "3 \u201cSecurity Tutorial.\u201d Confluent documentation (n.d.). https://docs.confluent",
    ".io/platform/current/security/security_tutorial.html (accessed June 10, 2020).",
    "4 keytool. Oracle Java documentation (n.d.). https://docs.oracle.com/javase/8/ docs/technotes/tools/unix/keytool.html (accessed August 20, 2021).",
    "5 \u201cDocumentation: Incorporating Security Features in a Running Cluster.\u201d Apache Software Foundation (n.d.). http://kafka.apache.org/24/documentation.html #security_rolling_upgrade (accessed June 1, 2020).",
    "6 V. A. Brennen. \u201cAn Overview of a Kerberos Infrastructure.\u201d Kerberos Infrastruc- ture HOWTO. https://tldp.org/HOWTO/Kerberos-Infrastructure-HOWTO/ overview.html (accessed July, 22, 2021).",
    "7  \u201cConfiguring GSSAP.\u201d Confluent documentation (n.d.). https://docs.confluent",
    ".io/platform/current/kafka/authentication_sasl/authentication_sasl_gssapi",
    ".html (accessed June 10, 2020).",
    "8 \u201cAuthorization using ACLs.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/kafka/authorization.html (accessed June 10, 2020).",
    "9 \u201cAuthorization using Role-Based Access.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/security/rbac/index.html (acces- sed June 10, 2020).",
    "10  \u201cZooKeeper Security.\u201d Confluent documentation (n.d.). https://docs.confluent",
    ".io/platform/current/security/zk-security.html (accessed June 10, 2020).",
    "11  \u201cQuotas.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform",
    "/current/kafka/design.html#quotas (accessed August 21, 2021).",
    "12  \u201cNetwork Bandwidth Quotas.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/kafka/design.html#network-bandwidth-quotas  (accessed August 21, 2021).",
    "13 \u201cSetting quotas.\u201d Apache Software Foundation (n.d.). https://kafka.apache",
    ".org/documentation/#quotas (accessed June 15, 2020).",
    "14 \u201cQuota Configuration.\u201d Confluent documentation (n.d.). https://docs.conflu ent.io/platform/current/kafka/design.html#quota-configuration (accessed August 21, 2021).",
    "15 KIP-124 \u201cRequest rate quotas.\u201d Wiki for Apache Kafka. Apache Software Foun- dation (March 30, 2017). https://cwiki.apache.org/confluence/display/",
    "KAFKA/KIP-124+-+Request+rate+quotas (accessed June 1, 2020).",
    "16 \u201cRequest Rate Quotas.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/kafka/design.html#request-rate-quotas (accessed August 21, 2021).",
    "17 \u201cAmazon MSK features.\u201d Amazon Managed Streaming for Apache Kafka (n.d). https://aws.amazon.com/msk/features/ (accessed July 23, 2021).",
    "18 \u201cSecrets Management.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/security/secrets.html (accessed August 21, 2021).",
    "As you have discovered the various ways to use Apache Kafka, it might be an inter- esting experiment to think about how you view Kafka the more you utilize it. As enterprises (or even tools) grow, they can sometimes be modeled with maturity levels. Martin Fowler provides a great explanation for this at https://martinfowler.com/ bliki/MaturityModel.html [1]. Fowler also has a good example that explains the Richardson Maturity Model, which looks at REST [2]. For even further reference, the original talk, \u201cJustice Will Take Us Millions Of Intricate Moves: Act Three: The",
    "Maturity Heuristic\u201d by Leonard Richardson can be found at https://www.crummy",
    ".com/writing/speaking/2008-QCon/act3.html.1"
  ],
  "Part 3  Going further<sec><sec><sec>A proposed Kafka maturity model<sec><sec>Level 0": [
    "In the following sections, we focus our discussion on maturity levels specific to Kafka. For a comparison, check out the Confluent white paper titled, \u201cFive Stages to Stream- ing Platform Adoption,\u201d which presents a different perspective that encompasses five stages of their streaming maturity model with distinct criteria for each stage [3]. Let\u2019s look at our first level (of course, as programmers we start with level 0).",
    "We use this exercise with a maturity model so that we can think about how Kafka can be a powerful tool for one application or even evolve into the foundation for all of your enterprise\u2019s applications rather than as a simple message broker. The following levels aren\u2019t meant to be a step-by-step required path, but rather a way to think about how you might start and then progress with Kafka. These steps are debatable, of course, but we simply offer an example path."
  ],
  "Part 3  Going further<sec><sec><sec>A proposed Kafka maturity model<sec><sec>Level 1": [
    "At this level, we use Kafka as an enterprise service bus (ESB) or publish/subscribe (pub/sub) system. Events provide asynchronous communication between applica- tions, whether we are replacing a different message broker like RabbitMQ or just start- ing with this pattern.",
    "One example use case is a user submitting a text document to be converted into a PDF. Once a user submits a document, the application stores the document and then sends a message to a Kafka topic. A Kafka consumer then reads the message to deter- mine which documents need to be converted into a PDF. In this example, the drive might be offloaded to work with a backend system that a user knows will not send a response right away. Figure 11.1 shows this message bus in action.",
    "PDF generation at a later time",
    "Figure 11.1",
    "Level 0 example",
    "1 The act3.html website text is licensed under the Creative Commons License at https://creativecommons.org/ licenses/by-sa/2.0/legalcode.",
    "This level alone brings us the benefit of allowing us to decouple a system so that a fail- ure of our frontend text submission system does not impact our backend system. Also, we don\u2019t need to rely on both to maintain successful simultaneous operations."
  ],
  "Part 3  Going further<sec><sec><sec>A proposed Kafka maturity model<sec><sec>Level 2": [
    "Batch processing can still be present in areas of our enterprise, but most data pro- duced is now brought into Kafka. Whether with extract, transform, load (ETL) or change data capture (CDC) processes, Kafka starts to gather events from more and more systems in our enterprise. Level 1 allows us to have an operational, real-time data flow and gives us the ability to feed data quickly into analytical systems.",
    "An example of this might be a vendor database that holds customer information. We do not want our marketing folks to run complex queries that could slow down our pro- duction traffic. In this case, we can use Kafka Connect to write the data from database tables into Kafka topics that we can use on our terms. Figure 11.2 shows Kafka Connect capturing data from a relational database and moving that data into a Kafka topic.",
    "Vendor database\tKafka connect",
    "Serving customer operational traffic",
    "Pulls database changes into Kafka",
    "Other consumers",
    "Figure 11.2  Level 1 example"
  ],
  "Part 3  Going further<sec><sec><sec>A proposed Kafka maturity model<sec><sec>Level 3": [
    "We realize that data will change over time and that schemas are needed. Although our producers and consumers might be decoupled, they still need a way to understand the data itself. For this, we\u2019ll take advantage of schemas and a schema registry. And even though it would have been ideal to start with schemas, the reality is that this need often presents itself a couple of application changes later, after initial deployments.",
    "One example for this level is changing the data structure of an event to receive orders from our processing system. New data is added, but the new fields are optional, and this works fine because our schema registry is configured to support backward compatibility. Figure 11.3 shows our consumer\u2019s need for schemas. We will look more into these details as we progress through this chapter.",
    "Same Kafka topic",
    "Order\tConsumer",
    "Added, but optional",
    "Consumers need to know how to handle both message versions.",
    "Figure 11.3  Level 2 example"
  ],
  "Part 3  Going further<sec><sec><sec>The Schema Registry": [
    "Everything is an event stream that is infinite (never ending). Kafka is the system of our enterprise for our event-based applications. In other words, we don\u2019t have customers waiting for recommendations or status reports that used to be produced by an over- night batch-processing run. Customers are alerted in milliseconds of a change to their account when an event happens, not in minutes. Instead of pulling data from other data sources, applications pull data directly from your cluster. User-facing applications can derive state and materialized views to customers depending on the needs of our core Kafka infrastructure."
  ],
  "Part 3  Going further<sec><sec><sec>The Schema Registry<sec><sec>Installing the Confluent Schema Registry": [
    "As part of our work in this chapter, we will focus on level 2, looking at how we can plan for data to change over time. Now that we have become good at sending data into and out of Kafka, and despite a small mention of schemas in chapter 3, we left out some important details. Let\u2019s dive into what the Confluent Schema Registry provides for us. The Confluent Schema Registry stores our named schemas and allows us to main- tain multiple versions [4]. This is somewhat similar to the Docker Registry in pur- pose, which stores and distributes Docker images. Why is this storage needed? Producers and consumers are not tied together, but they still need a way to discover the schema involved in the data from all clients. Also, by having a remotely hosted registry, users do not have to run their copy locally or attempt to build their own,",
    "based on a list of schemas.",
    "While schemas can provide a sort of interface for applications, we can also use them to prevent breaking changes [4]. Why should we care about data that is moving fast through our system? Kafka\u2019s storage and retention capabilities allow consumers to go back to process older messages. These messages might be from months ago (or longer), and our consumers need to handle these various data versions.",
    "For Kafka, we can use the Confluent Schema Registry. Confluent provides an excellent option to consider as we look into how to take advantage of schemas. If you",
    "installed Kafka via the Confluent Platform before this chapter, you should have all the tools available to explore further. If not, we discuss installing and setting up this regis- try in the following sections."
  ],
  "Part 3  Going further<sec><sec><sec>The Schema Registry<sec><sec>Registry configuration": [
    "The Confluent Schema Registry is a community software offering as part of the Con- fluent Platform [5]. The Schema Registry lives outside of Kafka Brokers, but itself uses Kafka as its storage layer with the topic name _schemas [6]. It is vital not to delete this topic accidentally!",
    "Schema Registry nodes",
    "Figure 11.4  Schema Registry infrastructure",
    "When thinking about production usage, the Schema Registry should be hosted on a server separate from our brokers, as figure 11.4 shows [6]. Because we deal with a dis- tributed system and have learned to expect failures, we can provide multiple registry instances. And because all nodes can handle lookup requests from clients and route write requests to the primary node, the clients of the registry do not have to maintain a list of specific nodes."
  ],
  "Part 3  Going further<sec><sec><sec>Schema features": [
    "Similar to the other components of Kafka, you can set several configuration parame- ters in a file. If you have installed Kafka, you\u2019ll see the defaults located in the etc/ schema-registry/schema-registry.properties file. For the registry to be successful, it needs to know which topic to store its schemas in and how to work with your specific Kafka cluster.",
    "In listing 11.1, we use ZooKeeper to help complete the election of the primary node. It\u2019s important to note that because only the primary node writes to the Kafka",
    "topic. If your team is trying to move away from ZooKeeper dependencies, you can also use a Kafka-based primary election (using the configuration kafkastore.bootstrap",
    ".servers) [7].",
    "Listing 11.1  Schema Registry configuration",
    "listeners=http://localhost:8081 kafkastore.connection.url=localhost:2181 kafkastore.topic=_schemas",
    "registry at 8081\tPoints to our ZooKeeper server",
    "Uses the default topic for",
    "We can flip this debug flag to get or remove extra error information.",
    "schema storage, but we can change that if needed",
    "Let\u2019s go ahead and start the Schema Registry. We want to make sure that our Zoo- Keeper and Kafka brokers are already started for our examples. After confirming that they are up and running, we can use the command line to run the starting script for the registry, as the following listing shows [8].",
    "Listing 11.2  Starting the Schema Registry",
    "Runs the startup script in",
    "bin/schema-registry-start.sh \\",
    "the install\u2019s bin directory",
    "./etc/schema-registry/schema-registry.properties",
    "Takes in a properties file that we can modify",
    "We can check that the process is still running or use jps to verify this because it is a Java application, just like the brokers and ZooKeeper. Now that we have the registry running, we need to look at how to use the system\u2019s components. Because we now have a place to store our data format in the registry, let\u2019s revisit a schema that we used in chapter 3."
  ],
  "Part 3  Going further<sec><sec><sec>Schema features<sec><sec>REST API": [
    "The Confluent Schema Registry contains the following important components. One is a REST API (and the underlying application) for storing and fetching schemas. The second is client libraries for retrieving and managing local schemas. In the following sections, we\u2019ll look a bit deeper into each of these two components, starting with the REST API."
  ],
  "Part 3  Going further<sec><sec><sec>Schema features<sec><sec>Client library": [
    "The REST API helps us manage the following resources: schemas, subjects, compatibility, and config [9]. Of these resources, \u201csubjects\u201d might need some explanation. We can create, retrieve, and delete versions and the subjects themselves. Let\u2019s look at a topic and its related subject for an application using a topic named kinaction_schematest. In our schema registry, we will have a subject called kinaction_schematest-value because we are using the default behavior of basing the name on our current topic",
    "name. If we were using a schema for the message key as well, we would also have a sub- ject called kinaction_schematest-key. Notice that the key and value are treated as different subjects [10]. Why is this? It ensures that we can version and change our schemas independently because the key and value are serialized separately.",
    "To confirm the registry is started and to see it in action, let\u2019s submit a GET against the REST API using a tool like curl [9]. In the following listing, we list the current configuration like the compatibility level.",
    "Listing 11.3  Getting the Schema Registry configuration",
    "curl -X GET http://localhost:8081/config",
    "Lists all the configs in the Registry using REST",
    "Also, we need to add a Content-Type header for our REST interactions with the Schema Registry. In any following examples, like listing 11.7, we will use application/ vnd.schemaregistry.v1+json [9]. As with the schemas themselves, we\u2019re planning for API changes by declaring which API version we\u2019ll use. This helps ensure that our clients are using the intended version.",
    "While the REST API is great for administrators of the subjects and schemas, the cli- ent library is where most developers will spend their time interacting with the Registry."
  ],
  "Part 3  Going further<sec><sec><sec>Compatibility rules": [
    "Let\u2019s drill into the producer client\u2019s interaction with the Schema Registry. Think back to our example in chapter 3 with a producer that is configured to use an Avro serial- izer for our messages. We should already have a registry started locally, so now we need to configure our producer client to use it (listing 11.4). With our use case from chap- ter 3, we created a schema for an Alert object that is the value of our message. The value.serializer property needs to be set to use the KafkaAvroSerializer in our case. This class serializes the custom object using the Registry.",
    "Listing 11.4  Producer using Avro serialization",
    "kaProperties.put(\"key.serializer\",",
    "\u27a5 \"org.apache.kafka.common.serialization.LongSerializer\"); kaProperties.put(\"value.serializer\",",
    "\u27a5 \"io.confluent.kafka.serializers.KafkaAvroSerializer\");",
    "kaProperties.put(\"schema.registry.url\",",
    "Sends Alert as a value and uses KafkaAvroSerializer",
    "\u27a5 \"http://localhost:8081\");",
    "Producer<Long, Alert> producer =",
    "new KafkaProducer<Long, Alert>(kaProperties); Alert alert = new Alert(); alert.setSensorId(12345L);",
    "Points to the URL of our registry containing a versioned history of our schemas to help with schema validation and evolution",
    "alert.setTime(Calendar.getInstance().getTimeInMillis()); alert.setStatus(alert_status.Critical);",
    "log.info(\"kinaction_info = {}, alert.toString()); ProducerRecord<Long, Alert> producerRecord =",
    "\u27a5 new ProducerRecord<Long, Alert>(",
    "\"kinaction_schematest\", alert.getSensorId(), alert",
    "producer.send(producerRecord);",
    "NOTE Because we use the default TopicNameStrategy, the Schema Registry registers the subject kinaction_schematest-value with our schema for Alert. To use a different strategy, the producer client could set either of the following configurations to override the value and key strategies: value",
    ".subject.name.strategy and key.subject.name.strategy [10]. In this case, we could have used an override to use an underscore to keep our topic name from having a mix of dashes and underscores.",
    "On the consumer side, once the client has successfully found the schema, it can now understand the records it reads. Let\u2019s look at using the same schema we produced for a topic and retrieve it with a consumer to see if we can get that value back without error, as the following listing exhibits [11].",
    "Listing 11.5  Consumer using Avro deserialization",
    "kaProperties.put(\"key.deserializer\",",
    "\u27a5 \"org.apache.kafka.common.serialization.LongDeserializer\");",
    "kaProperties.put(\"value.deserializer\",",
    "\u27a5 \"io.confluent.kafka.serializers.KafkaAvroDeserializer\"); kaProperties.put(\"schema.registry.url\",",
    "Uses KafkaAvroDeserializer in a consumer config",
    "\u27a5 \"http://localhost:8081\");",
    "KafkaConsumer<Long, Alert> consumer =",
    "Points to the URL of our registry",
    "\u27a5 new KafkaConsumer<Long, Alert>(kaProperties);",
    "consumer.subscribe(List.of(\"kinaction_schematest\")); while (keepConsuming) {",
    "ConsumerRecords<Long, Alert> records =",
    "\u27a5 consumer.poll(Duration.ofMillis(250));",
    "for (ConsumerRecord<Long, Alert> record : records) { log.info(\"kinaction_info Alert Content = {},",
    "\u27a5 record.value().toString());",
    "Subscribes to the same topic where we produced our schema messages",
    "So far, we have worked on only one version of a schema with our producer and con- sumer clients. However, planning for data changes can save you a lot of headaches. Next, we\u2019ll look at the rules that will help us think about the changes we can make and their impact on our clients."
  ],
  "Part 3  Going further<sec><sec><sec>Compatibility rules<sec><sec>Validating schema modifications": [
    "One important thing to decide on is what compatibility strategy we plan to support. The compatibility rules in this section are here to help direct our schemas as they change over time. While it may seem like a large number of available types, it is nice to know that, in general, those marked as transitive follow the same rules as those without that suffix. The non-transitive types are only checked against the last version of the schema, whereas transitive types are checked against all previous versions [12]. Here is a list of types noted by Confluent: BACKWARD (the default type), BACKWARD_TRANSITIVE, FORWARD, FORWARD_TRANSITIVE, FULL, FULL_TRANSITIVE, and NONE [12].",
    "Let\u2019s look at what the BACKWARD type implies for our applications. Backward-com- patible changes might involve adding non-required fields or removing fields [12]. Another critical aspect to consider when choosing the compatibility type is the order in which we want clients to change. For example, we will likely want our consumer cli- ents to upgrade first for the BACKWARD type [12]. Consumers will need to know how to read the messages before new variations are produced.",
    "On the reverse end of the types, forward-compatible changes are the opposite of backward. With the FORWARD type, we can add new fields and, opposite of the way we updated for the BACKWARD type, we will likely want to update our producer clients first [12].",
    "Let\u2019s look at how we can change our schema for Alert to maintain backward com- patibility. The following listing shows the addition of a new field, recovery_details, with a default value of Analyst recovery needed to account for messages that do not include a value for the new field.",
    "Listing 11.6  Alert schema change",
    "{\"name\": \"Alert\",",
    "\"fields\": [",
    "{\"name\": \"sensor_id\", \"type\": \"long\",",
    "\"doc\":\"The unique id that identifies the sensor\"},",
    "{\"name\": \"recovery_details\", \"type\": \"string\", \"default\": \"Analyst recovery needed\"}",
    "Creates a new field (recovery_details) in this instance",
    "Any older messages with version 1 of the schema will have a default value populated for the field added later. This will be read by a consumer using Schema Registry ver- sion 2 [12]."
  ],
  "Part 3  Going further<sec><sec><sec>Alternative to a schema registry": [
    "If we have tests that exercise our API endpoints or even Swagger (https://swagger.io/), it is important to think about how we can automate testing changes to our schemas. To check and validate our schema changes, we have a couple of options:",
    "Use the REST API compatibility resource endpoints",
    "Use a Maven plugin for JVM-based applications",
    "Let\u2019s look at an example REST call that will help us check our compatibility for a schema change. Listing 11.7 shows how this is done [13]. As a side note, before check- ing compatibility, we need to already have a copy of our older schema in the registry. If one is not present and the call fails, check out the source code with this book for an example.",
    "Listing 11.7  Checking compatibility with the Schema Registry REST API",
    "curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" \\",
    "--data '{ \"schema\": \"{ \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"Alert\\\",",
    "\u27a5 \\\"fields\\\": [{ \\\"name\\\": \\\"notafield\\\", \\\"type\\\": \\\"long\\\" } ]}\" }' \\ http://localhost:8081/compatibility/subjects/kinaction_schematest-value/",
    "\u27a5 versions/latest",
    "{\"is_compatible\":false}",
    "Gives a compatible",
    "result as a Boolean",
    "Passes the schema content on the command line",
    "We can also use a Maven plugin if we are willing to use Maven and are already on a JVM-based platform [14]. The following listing shows part of the pom.xml entry needed for this approach, and the complete file can be found in the chapter\u2019s source code.",
    "Listing 11.8  Checking compatibility with the Schema Registry Maven plugin",
    "<groupId>io.confluent</groupId>",
    "<artifactId>",
    "kafka-schema-registry-maven-plugin",
    "</artifactId>",
    "<configuration>",
    "<schemaRegistryUrls>",
    "<param>http://localhost:8081</param>",
    "</schemaRegistryUrls>",
    "<kinaction_schematest-value> src/main/avro/alert_v2.avsc",
    "</kinaction_schematest-value>",
    "</subjects>",
    "<goal>test-compatibility</goal>",
    "</configuration>",
    "Coordinates that Maven needs to download this plugin",
    "The URL to our Schema Registry",
    "Lists the subjects to validate our schemas in the provided file path",
    "We can invoke the Maven goal with mvn",
    "schema-registry:test-compatibility.",
    "In essence, it takes the schemas located in your file path and connects to the Schema Registry to check against the schemas already stored there."
  ],
  "Part 3  Going further<sec><sec><sec>Kafka Streams": [
    "1 M. Fowler. \u201cMaturity Model.\u201d (August 26, 2014). https://martinfowler.com/",
    "bliki/MaturityModel.html (accessed June 15, 2021).",
    "2 M. Fowler. \u201cRichardson Maturity Model.\u201d (March 18, 2010). https://martin fowler.com/articles/richardsonMaturityModel.html (accessed June 15, 2021).",
    "3 L. Hedderly. \u201cFive Stages to Streaming Platform Adoption.\u201d Confluent white paper (2018). https://www.confluent.io/resources/5-stages-streaming-platform",
    "-adoption/ (accessed January 15, 2020).",
    "4 \u201cSchema Registry Overview.\u201d Confluent documentation (n.d.). https://docs",
    ".confluent.io/platform/current/schema-registry/index.html (accessed July 15, 2020).",
    "5 \u201cConfluent Platform Licenses: Community License.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/installation/license.html# community-license (accessed August 21, 2021).",
    "6 \u201cRunning Schema Registry in Production.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/schema-registry/installation/ deployment.html#schema-registry-prod (accessed April 25, 2019).",
    "7 \u201cSchema Registry Configuration Options.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/schema-registry/installation/con fig.html#schemaregistry-config (accessed August 22, 2021).",
    "8 \u201cSchema Registry and Confluent Cloud.\u201d Confluent documentation (n.d.). https://docs.confluent.io/cloud/current/cp-component/schema-reg-cloud- config.html (accessed August 22, 2021).",
    "9 \u201cSchema Registry API Reference.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/schema-registry/develop/api.html (accessed July 15, 2020).",
    "10 \u201cFormats, Serializers, and Deserializers.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/schema-registry/serdes-develop/ index.html (accessed April 25, 2019).",
    "11 \u201cOn-Premises Schema Registry Tutorial.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/schema-registry/schema_registry",
    "_onprem_tutorial.html (accessed April 25, 2019).",
    "12 \u201cSchema Evolution and Compatibility.\u201d Confluent Platform. https://docs",
    ".confluent.io/current/schema-registry/avro.html#compatibility-types  (accessed June 1, 2020).",
    "13 \u201cSchema Registry API Usage Examples.\u201d Confluent documentation (n.d.). https://docs.confluent.io/platform/current/schema-registry/develop/using",
    ".html (accessed August 22, 2021).",
    "14 \u201cSchema Registry Maven Plugin.\u201d Confluent documentation (n.d.). https:// docs.confluent.io/platform/current/schema-registry/develop/maven-plugin",
    ".html (accessed July 16, 2020).",
    "So far on our path in learning about Kafka, we\u2019ve focused on the parts that help make a complete event-streaming platform, including the Kafka brokers, producer clients, and consumer clients. With this foundation, we can expand our toolset and understand the next layer of the Kafka ecosystem\u2014stream processing using Kafka Streams and ksqlDB. These technologies offer abstractions, APIs, and DSLs (domain-specific languages), based on the foundation that we have built on in the previous chapters.",
    "This chapter introduces a simple banking application that processes funds as they move in and out of the accounts. In our application, we will implement a Kafka Streams topology to process the transaction requests submitted to the transaction- request topic atomically.",
    "NOTE Our business requirement states that we must check whether the funds are sufficient for every request received before updating the account\u2019s bal- ance that\u2019s being processed. As per our requirements, our application can\u2019t process two transactions simultaneously for the same account, which could create a race condition in which we cannot guarantee we can enforce the bal- ance check before withdrawing funds.",
    "We will use Kafka\u2019s inter-partition ordering guarantees to implement serializable (ordered) processing of transactions for a particular account. We also have a data gen- erator program that writes simulated transaction requests to the Kafka topic with a key equal to the transaction\u2019s account number. We can, therefore, ensure all transactions will be processed by a single instance of our transaction service, no matter how many applications are concurrently running. Kafka Streams won\u2019t commit any message off- set until it completes our business logic of managing a transaction request.",
    "We introduce the Processor API by implementing a transformer component from Kafka Streams. This utility allows us to process events one by one while interacting with a state store, another element of Kafka Streams that helps us persist our account balance in a local instance of an embedded database, RocksDB. Finally, we will write a second stream processor to generate a detailed transaction statement enriched with account details. Rather than creating another Kafka Streams application, we will use ksqlDB to declare a stream processor that will enrich our transactional data in real time with our referential data coming from the account topic.",
    "This section aims to show how we can use an SQL-like query language to create",
    "stream processors (with functionality similar to Kafka Streams) without compiling and running any code. We\u2019ll dig into the Kafka Streams API\u2019s details after reviewing the concepts of stream-processing applications."
  ],
  "Part 3  Going further<sec><sec><sec>Kafka Streams<sec><sec>KStreams API DSL": [
    "In general, stream processing (or streaming) is a process or application you implement that deals with an uninterrupted flow of data and performs work as soon as that data arrives, as discussed in chapter 2. This application does not execute on a regular schedule or even query a database for data. Views can be created from the data, but we are not limited to a point-in-time view. Enter Kafka Streams!",
    "Kafka Streams is a library and not a standalone cluster [1]. Notice that this descrip- tion includes the word library. This alone can help us create stream processing for our applications. No other infrastructure is required besides the need to utilize an existing Kafka cluster [2]. The Kafka Streams library is a part of our JVM-based application.",
    "Not having additional components makes this API one that can be easily tested when starting with a new application. Though other frameworks might require more cluster management components, Kafka Streams applications can be built and deployed using any tool or platform that allows JVM-based applications to run.",
    "NOTE Our application won\u2019t run on the brokers of our cluster. For that rea- son, we will run our application outside the Kafka cluster. This approach",
    "guarantees the separation of concerns in resource management for Kafka brokers and stream processors.",
    "The Streams API performs per-record or per-message processing [3]. You won\u2019t want to wait for a batch to form or delay that work if you\u2019re concerned about your system reacting to events as soon as they are received.",
    "As we consider how to implement our applications, one of the first questions that comes to mind is choosing a producer/consumer client for the Kafka Streams library. Although the Producer API is excellent for taking precise control of how our data gets to Kafka and the Consumer API for consuming events, sometimes we might not want to implement every aspect of the stream-processing framework ourselves. Instead of using lower-level APIs for stream processing, we want to use an abstraction layer that allows us to work with our topics more efficiently.",
    "Kafka Streams might be a perfect option if our requirements include data transfor- mations with potentially complex logic consuming and producing data back into Kafka. Streams offer a choice between a functional DSL and the more imperative Pro- cessor API [2]. Let\u2019s take a first look at the Kafka Streams DSL."
  ],
  "Part 3  Going further<sec><sec><sec>Kafka Streams<sec><sec>KTable API": [
    "The first API that we\u2019re going to look at is the KStreams API. Kafka Streams is a data- processing system designed around the concept of a graph, one that doesn\u2019t have any cycles in it [2]. It has a starting node and an ending node, and data flows from the starting node to the ending node. Along the way, nodes (or processors) process and transform the data. Let\u2019s take a look at a scenario where we can model a data-process- ing process as a graph.",
    "We have an application that gets transactions from a payment system. At the begin- ning of our graph, we need a source for this data. Because we\u2019re using Kafka as a data source, a Kafka topic will be our starting point. This origin point is often referred to as a source processor (or source node). This starts the processing; there aren\u2019t any previous pro- cessors. Our first example, therefore, is an existing service that captures transactions from an external payment system and places transaction request events into a topic.",
    "NOTE We will simulate this behavior with a simple data generator application.",
    "A transaction request event is needed to update the balance for a particular account. The results of the transaction processor go into two Kafka topics: successful trans- actions land in transaction-success and unsuccessful transactions land in transaction-failure. Because this is the end of the road for our small application, we will create a pair of sink processors (or sink nodes) to write to our success or failure topics.",
    "NOTE Some processor nodes may not have a connection to sink nodes. In this case, those nodes create side effects elsewhere (e.g., printing information to the console or writing data to the state stores) and do not require sending data back to Kafka.",
    "Figure 12.1 shows a DAG (directed acyclic graph) representation of how data flows. Figure 12.2 shows you how this DAG maps out to the Kafka Streams topology.",
    "Figure 12.1  DAG (directed acyclic graph) of our stream-processing application",
    "\"transaction-request\"",
    "\"transaction-success\"\t\"transaction-failed\"",
    "Figure 12.2  Topology for your transaction-processing application",
    "Now that we have a map for a guide, let\u2019s look at what this application looks like with the DSL code. Unlike our earlier examples, when using this API, we don\u2019t need to reach a consumer directly to read our messages, but we can use a builder to start creat- ing our stream. Listing 12.1 shows the creation of a source processor.",
    "IMPORTANT At this point, we\u2019re defining our topology, but not invoking it, as processing hasn\u2019t started yet.",
    "In the listing, we use a StreamsBuilder object in order to create a stream from the Kafka topic transaction-request. Our data source is transaction-request and is the logical starting point for our processing.",
    "Listing 12.1  Source topic DSL definition",
    "The starting point for",
    "StreamsBuilder builder = new StreamsBuilder()",
    "building our topology",
    "KStream<String, Transaction> transactionStream = builder.stream(\"transaction-request\",",
    "Consumed.with(stringSerde, transactionRequestAvroSerde));",
    "Builds a KStream object for transaction-request to start our processing from this topic",
    "The next step is to add to our topology using the KStream that we created from the source processor. The following listing shows this task.",
    "Listing 12.2  Processor and sink topic definition",
    "Continues building our topology",
    "final KStream<String, TransactionResult> resultStream = transactionStream.transformValues(",
    "() -> new TransactionTransformer()",
    "using the stream created from the previous source processor",
    "resultStream",
    ".filter(TransactionProcessor::success)",
    ".to(this.transactionSuccessTopicName,",
    "Depending on the transaction success criteria, our sink processor writes to one of two topics: transaction- success or transaction-failed.",
    "Produced.with(Serdes.String(), transactionResultAvroSerde));",
    "resultStream",
    ".filterNot(TransactionProcessor::success)",
    ".to(this.transactionFailedTopicName, Produced.with(Serdes.String(), transactionResultAvroSerde));",
    "KafkaStreams kafkaStreams =",
    "\u27a5 new KafkaStreams(builder.build(), kaProperties); kafkaStreams.start();",
    "Passes our topology and configuration to create a KafkaStreams object",
    "kafkaStreams.close();",
    "Closes the stream to stop processing",
    "Starts our stream application, which continues in the same way as if we had consumer clients polled in an infinite loop",
    "Although we have only one processing node, which doesn\u2019t involve reading and writing data, it is easy to see how we could chain multiple nodes on our path. Looking over the code in listing 12.2, you might notice the lack of direct usage of the following:",
    "A consumer client to read from the source topic as in chapter 5",
    "A producer client to send our messages at the end of the flow as in chapter 4",
    "This layer of abstraction allows us to work on our logic rather than the details. Let\u2019s look at another practical example. Imagine we simply want to log transaction requests in the console without processing them. The following listing shows the reading of transaction events from the transaction-request topic.",
    "Listing 12.3  Transaction tracker KStream",
    "Sources our data from the topic",
    "KStream<String, Transaction> transactionStream builder.stream(\"transaction-request\",",
    "transaction-request and uses a custom Transaction object to hold our data",
    "Consumed.with(stringSerde, transactionRequestAvroSerde));",
    "transactionStream.print(Printed.<String, Transaction>toSysOut()",
    ".withLabel(\"transactions logger\"));",
    "KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), kaProperties);",
    "kafkaStreams.cleanUp(); kafkaStreams.start();",
    "Cleans up the local data store, ensuring that we run without the past state",
    "Prints the transactions as we get them, so we can more easily follow along with the example",
    "This flow is so simple that we just write out the transactions to the console, but we could have used an API call to send an SMS or email as well. Notice the added call to cleanup() before starting the application. This method provides a way to remove the local state stores for our application. Just remember to only do this before the start or after closing the application.",
    "Despite the ease of use of KStreams, they are not the only way we can process our data. The KTable API provides us with an alternative to always add events to our view by representing data as updates instead."
  ],
  "Part 3  Going further<sec><sec><sec>Kafka Streams<sec><sec>GlobalKTable API": [
    "Although a KStream can be thought of as event data always being appended to our log, a KTable allows us to think about a log-compacted topic [2]. In fact, we can also draw a parallel to a database table that deals with updates in place. Recall from work- ing with compacted topics in chapter 7 that our data needs to have a key for this to work. Without a key, the value to be updated won\u2019t really make practical sense. Run- ning the code in the following listing, we see that not every order event shows up. Instead, we see only the distinct orders.",
    "Listing 12.4  Transaction KTable",
    "StreamsBuilder builder = new StreamsBuilder(); KTable<String, Transaction> transactionStream =",
    "builder.stream(\"transaction-request\",",
    "StreamsBuilder.table() creates a KTable from the topic transaction-request.",
    "Consumed.with(stringSerde, transactionRequestAvroSerde), Materialized.as(\"latest-transactions\"));",
    "KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), kaProperties);",
    "KTable records materialize locally in the latest-transactions state store.",
    "What is familiar with this listing is the way we build the stream. We use a builder to cre- ate the steps and then, once it is defined, we call start. Until that moment, nothing processes in our application."
  ],
  "Part 3  Going further<sec><sec><sec>Kafka Streams<sec><sec>Processor API": [
    "Although similar to KTable, the GlobalKTable is populated with data from all parti- tions of a topic [2]. The foundational knowledge about topics and partitions pays off when understanding these abstractions, as shown in how the KafkaStreams instances consume each partition of a topic. Listing 12.5 is an example of using a join with a GlobalKTable. Imagine a stream that gets updated with details about a mailed pack- age for a customer. These events contain the customer ID, and we can then join on a customer table to find their associated email and send a message.",
    "Listing 12.5  Mailing notification GlobalKTable",
    "StreamsBuilder builder = new StreamsBuilder();",
    "final KStream<String, MailingNotif> notifiers = builder.stream(\"kinaction_mailingNotif\");",
    "final GlobalKTable<String, Customer> customers = builder.globalTable(\"kinaction_custinfo\");",
    "lists.join(customers,",
    "The notification stream listens for new messages about mailings to send to a customer.",
    "GlobalKTable holds a list of Customer information, including email.",
    "The join method matches",
    "(mailingNotifID, mailing) -> mailing.getCustomerId(), (mailing, customer) -> new Email(mailing, customer))",
    ".peek((key, email) -> emailService.sendMessage(email));",
    "the customer that needs to be notified with an email.",
    "KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), kaProperties); kafkaStreams.cleanUp();",
    "kafkaStreams.start();",
    "As shown in listing 12.5, we can build a new GlobalKTable using the method globalTable. Whereas a table that is not global might not consume all the input topic\u2019s data due to multiple partitions, the global table consumes all partitions for your running code [2].",
    "NOTE The idea of a global table is to make the data available to our applica- tion regardless of which partition it is mapped to.",
    "Even though the Streams DSL has been excellent for quick use cases, sometimes we might need more control as we send data along our logic paths. Developers can use the Processor API alone or with the Streams DSL to provide even more options."
  ],
  "Part 3  Going further<sec><sec><sec>Kafka Streams<sec><sec>Kafka Streams setup": [
    "It\u2019s important to note that when reviewing code for another streaming application or even looking at getting into lower abstraction levels in our own logic, we might run into examples from the Processor API. This is considered not as easy to use as the DSL",
    "discussed in the previous sections, but it gives us more options and power over our logic [2]. Let\u2019s look at an example in the following listing, where we create a topology and highlight the differences from our previous Streams applications.",
    "Listing 12.6  Processor API source",
    "import static org.apache.kafka.streams.Topology.AutoOffsetReset.LATEST; public static void main(String[] args) throws Exception {",
    "final Serde<String> stringSerde = Serdes.String(); Deserializer<String> stringDeserializer = stringSerde.deserializer(); Serializer<String> stringSerializer = stringSerde.serializer();",
    "Topology topology = new Topology(); topology = topology.addSource(LATEST,",
    "Creates our flow with the Topology object",
    "Sets the offset to LATEST",
    "Deserializes",
    "\"kinaction_source\", stringDeserializer, stringDeserializer, \"kinaction_source_topic\");",
    "Names the node that we can refer to in later steps",
    "Reads from this  Kafka topic",
    "Deserializes our value",
    "First, we build our graph using the Topology object [4]. Setting the offset to LATEST and listing our key and value deserializers should be familiar from when we set config- uration properties for our client consumers in chapter 5. In listing 12.6, we named the node kinaction_source, which reads from the topic kinaction_source_topic. Our next step is to add a processing node, as the following listing shows.",
    "Listing 12.7  Processor API processor node",
    "topology = topology.addProcessor(",
    "\"kinactionTestProcessor\", () -> new TestProcessor(), \"kinaction_source\");",
    "One or a list of nodes",
    "Names our new processor node",
    "Creates a processor instance from a ProcessorSupplier",
    "sends data to this node.",
    "Listing 12.7 shows that when we define a processing node, we give it a name (kinactionTestProcessor, in this case) and associate the logic with the step. We also list the nodes that will provide the data.",
    "To finish out our simple example, let\u2019s look at listing 12.8. It shows how we define two separate sinks to complete our topology. The sink is where we place our data at the end of processing. The topic name and the key and value serializers should be familiar from our earlier work with producer clients. As we did with the other parts of the topology, we define kinactionTestProcessor as one of the nodes from which we will get data in our flow.",
    "Listing 12.8  Processor API processor sink",
    "topology = topology.addSink(",
    "\"Kinaction-Destination1-Topic\", \"kinaction_destination1_topic\",",
    "sink node\tNames the output",
    "topic we plan to use",
    "Serializes our key",
    "stringSerializer, stringSerializer, \"kinactionTestProcessor\");",
    "topology = topology.addSink(",
    "Defines the node that feeds us data to write to the sink",
    "\"Kinaction-Destination2-Topic\", \"kinaction_destination2_topic\", stringSerializer,",
    "stringSerializer, \"kinactionTestProcessor\");",
    "Adds a second sink to our topology",
    "In our Processor code, we\u2019re going to show how we can use logic to direct the flow of our data. Our kinactionTestProcessor enables us to forward the flow, including the key and value, to the sink named Kinaction-Destination2-Topic. Although this is hardcoded in the following listing, we can use logic to determine when to send data to the second sink.",
    "Listing 12.9  Processor custom code",
    "public class KinactionTestProcessor",
    "extends AbstractProcessor<String, String> { @Override",
    "public void process(String key, String value) { context().forward(key, value,",
    "To.child(\"Kinaction-Destination2-Topic\"));",
    "Extends AbstractProcessor to implement the process method for our custom logic",
    "Hardcoded value, but we can also direct the forward with additional logic",
    "Even though it\u2019s easy to see that the code is more verbose than our DSL examples, the important thing is the control we now have in our logic that was not shown in our sim- ple flow with the DSL API. If we want to control the schedule of when processing occurs or even the commit time for results, we\u2019ll need to dig into more complex Pro- cessor API methods."
  ],
  "Part 3  Going further<sec><sec><sec>ksqlDB: An event-streaming database": [
    "While our example application only uses a single instance, streaming applications can scale by increasing the number of threads and deploying more than one instance. As with the number of instances of a consumer in the same consumer group, our applica- tion\u2019s parallelism is related to the number of partitions in its source topic [5]. For example, if our starting input topic has eight partitions, we would plan to scale to eight instances of our application. Unless we want to have an instance ready in case of failure, we won\u2019t have more instances because they won\u2019t take any traffic.",
    "When we think about our application\u2019s design, it is crucial to mention the process- ing guarantees that our use case requires. Kafka Streams supports at-least-once and exactly-once processing semantics.",
    "NOTE In version 2.6.0, exactly-once beta was introduced. This version enables higher throughput and scalability by attempting to reduce resource utilization [6].",
    "If your application logic depends on exactly-once semantics, having your Kafka Streams application within the walls of the Kafka ecosystem helps ensure this possibil- ity. As soon as you send data outside into external systems, you need to look at how they achieve any promised delivery options. Whereas the Streams API can treat retrieving topic data, updating the stores, and writing to another topic as one atomic operation, external systems cannot. System boundaries become significant when they impact your guarantees.",
    "As a reminder, with at-least-once delivery, it is crucial to note that although data should not be lost, you might have to prepare for the situation where your messages are processed more than once. At the time of writing, at-least-once delivery is the default mode, so make sure you\u2019re okay with addressing duplicate data in your application logic.",
    "Kafka Streams is designed with fault tolerance in mind. It does so in the ways that we have seen before in our Kafka cluster. The state stores in use are backed by a repli- cated Kafka topic that is partitioned. Due to Kafka\u2019s ability to retain messages and replay what happened before a failure, users can successfully continue without manu- ally recreating their state. If you\u2019re interested in continuing deeper into what Kafka Streams can offer, we recommend Kafka Streams in Action (https://www.manning.com/ books/kafka-streams-in-action) by William P. Bejeck Jr. (Manning, 2018) because it dives further into the details."
  ],
  "Part 3  Going further<sec><sec><sec>ksqlDB: An event-streaming database<sec><sec>Queries": [
    "ksqlDB (https://ksqldb.io) is an event-streaming database. This product was first intro- duced as KSQL, but the project underwent a name change in November 2019. Apache Kafka has developed various clients to help make our data work easier.",
    "ksqlDB exposes the power of Kafka to anyone who has ever used SQL. Suddenly, no Java or Scala code is needed to use the topics and data inside our clusters. Another primary driver is that as users worked with the entire application lifecycle, it was often the case that Kafka provided a part of the flow and not the whole architecture needed. Figure 12.3 shows an example of one way that we could utilize Kafka.",
    "Figure 12.3  Example Kafka application flow",
    "Moving data out of kinaction_alerttrend, for example",
    "Reporting UI",
    "Notice that to serve users, the data from Kafka is moved into an external data store. For example, imagine an application that triggers an order in an e-commerce system. An event is triggered for each stage of the order process and acts as a status for the purchaser to know what is happening with their order in a report.",
    "Before ksqlDB, it was often the case that the order events would be stored in Kafka (and processed",
    "and streams",
    "Reporting UI",
    "with Kafka Streams or Apache Spark) and then moved to the external system using the Kafka Connect API. The application would then read from that database the view created from the event stream to show the user as a point- in-time state. With the pull query and connector management fea-",
    "Figure 12.4  ksqlDB example Kafka application flow",
    "tures added to ksqlDB, developers gained a path to remain within the ecosystem to show users these mate-",
    "rialized views. Figure 12.4 shows a high-level overview of how the Kafka ecosystem can provide a more consolidated application without the need for external systems.We\u2019ll dig into the types of queries that ksqlDB supports, starting with the pull queries that we just introduced."
  ],
  "Part 3  Going further<sec><sec><sec>ksqlDB: An event-streaming database<sec><sec>Local development": [
    "Pull queries and push queries can help us build applications. Pull queries fit well when used in a synchronous flow like request-and-response patterns [7]. We can ask for the current state of the view that has been materialized by events that have arrived. The query returns a response and is considered completed. Most developers are familiar with this pattern and should know that the data is a point snapshot of their events when the query was retrieved.",
    "Push queries, on the other hand, can work well when used in asynchronous patterns [7]. In effect, we subscribe much like we did when using a consumer client. As new events arrive, our code can respond with the necessary actions."
  ],
  "Part 3  Going further<sec><sec><sec>ksqlDB: An event-streaming database<sec><sec>ksqlDB architecture": [
    "Although we\u2019ve tried to avoid bringing in extra technologies besides Kafka proper, the easiest way to go with ksqlDB local is with Confluent\u2019s Docker images. At https:// ksqldb.io/quickstart.html you can download images that include a complete Kafka setup or just the ksqldb-server and ksqldb-cli files.",
    "If you\u2019re using the Docker images, you can start those images with docker-compose up. Now you should be able to use ksqldb-cli to create an interactive session from your command terminal to your KSQL server. As users know, after you have your database",
    "server, you need to define your data. For more information on running Kafka and tools using Docker, see appendix A. The following listing shows the command we can run in order to leverage Docker to start an interactive ksqlDB session [8].",
    "Listing 12.10  Creating an interactive session with ksqlDB",
    "docker exec -it ksqldb-cli \\ ksql http://ksqldb-server:8088",
    "> SET 'auto.offset.reset'='earliest';",
    "Connects to the ksqlDB server to",
    "run commands from your terminal",
    "Sets the offset reset policy to earliest, letting ksqlDB process data already available in Kafka topics",
    "Next, let\u2019s look at an example of a situation where we can start to discover ksqlDB with an extension of our transaction processor. Using existing data from processed transac- tions, we\u2019ll learn how we can generate a statement report. The statement report includes extended (or enriched) information about the transaction\u2019s account. We will achieve this by joining successful transactions with account data. Let\u2019s start with creating a stream of a successful transactions from Kafka\u2019s topic.",
    "NOTE Because data was previously available in a Kafka topic from our Kafka Streams application, we may need to reset the offset with the command SET 'auto.offset.reset' = 'earliest'; so ksqlDB will be able to work with the existing data. We\u2019ll need to run this command before we execute the CREATE statement. Listing 12.11 shows our next step in the process, creating a stream for transaction success that reads from the topic transaction-success.",
    "Listing 12.11  Creating a stream for successful transactions",
    "CREATE STREAM TRANSACTION_SUCCESS (",
    "numkey string KEY,",
    "Tells ksqlDB about the record key",
    "transaction STRUCT<guid STRING, account STRING,",
    "amount DECIMAL(9, 2), type STRING, currency STRING, country STRING>,",
    "funds STRUCT<account STRING,",
    "balance DECIMAL(9, 2)>,",
    "success boolean,",
    "ksqlDB supports",
    "work with nested data.",
    "errorType STRING",
    "KAFKA_TOPIC='transaction-success',",
    "Using the KAFKA_TOPIC attribute of the WITH clause, specifies which topic to read from",
    "VALUE_FORMAT='avro');",
    "Integrates ksqlDB with schemas in Avro",
    "Because ksqlDB supports work with nested data, we used a nested type Transaction in the TransactionResult class in our Kafka Streams example. Using the STRUCT key- word, we defined a structure of a nested type. Additionally, ksqlDB integrates with the Confluent Schema Registry and natively supports schemas in Avro, Protobuf, JSON, and JSON-schema formats. Using this Schema Registry integration, ksqlDB can use schemas to infer or discover stream or table structures in many cases. This is a tremen- dous help for enabling effective collaboration between microservices, for example.",
    "As mentioned, we need to use comprehensive information about accounts. In con- trast to the history of successful transactions, we are not interested in a complete his- tory of account information changes. We just need to have a lookup of accounts by account ID. For that purpose, we can use TABLE in ksqlDB. The following listing shows how to do this.",
    "Listing 12.12  Creating a ksqlDB table",
    "Chooses the account number field",
    "CREATE TABLE ACCOUNT (number INT PRIMARY KEY)",
    "WITH (KAFKA_TOPIC = 'account', VALUE_FORMAT='avro');",
    "as a primary key for our table",
    "Using the Avro schema, ksqlDB learns about fields in the account table.",
    "The next step is to populate our table. Despite the SQL statement in listing 12.13 looking similar to SQL statements you may have run in the past, we want to draw your attention to a small but mighty difference. The use of EMIT CHANGES creates what we had previously discussed as a push query. Instead of returning to our command prompt, this stream runs in the background!",
    "Listing 12.13  A transaction statement stream with account information",
    "CREATE STREAM TRANSACTION_STATEMENT AS SELECT *",
    "FROM TRANSACTION_SUCCESS LEFT JOIN ACCOUNT",
    "ON TRANSACTION_SUCCESS.numkey = ACCOUNT.numkey EMIT CHANGES;",
    "To test our query, we need a new instance of the ksqldb-cli file to insert data into our stream to continue producing test transactions. The Kafka Streams application pro- cesses those transactions. In case of success, the Kafka Streams processor writes the result to a transaction-success topic, where it will be picked up by ksqlDB and used in TRANSACTION_SUCCESS and TRANSACTION_STATEMENT streams."
  ],
  "Part 3  Going further<sec><sec><sec>Going further": [
    "By using the Docker images, we glossed over the architecture that is part of ksqlDB. But it\u2019s important to know that unlike the Streams API, ksqlDB requires additional components to run. The main component is called the ksqlDB server [9]. This server is responsible for running the SQL queries submitted to it and getting data to and from our Kafka cluster. In addition to the query engine, a REST API also is provided. This API is used by the ksqldb-cli file that we used in the examples [9].",
    "Another item that we should consider is one of the deployment modes. Called headless, this mode prohibits developers from running queries through the command line interface [10]. To configure this mode, we can either start the ksqlDB server with the --queries-file command line argument or update the ksql-server.properties file",
    "[10]. Of course, this means that a query file is also required. The following listing shows how to start ksqlDB in headless mode [10].",
    "Listing 12.14  Starting ksqlDB in headless mode",
    "bin/ksql-server-start.sh \\",
    "etc/ksql/ksql-server.properties --queries-file kinaction.sql",
    "Starts ksqlDB in a non- interactive mode in which the CLI will not work",
    "Now that we have used Kafka Streams and ksqlDB, how do we know which one to reach for as we approach new tasks? Though not a read-eval-print loop (REPL) directly, running some quick prototype tests and trials with ksqldb-cli might be a great way to start new applications. Another key for ksqlDB is that users who are not run- ning Java or Scala (JVM languages) can find the Kafka Streams feature set available with this SQL option. Users looking to build microservices would likely find the Streams API a better fit."
  ],
  "Part 3  Going further<sec><sec><sec>Going further<sec><sec>Kafka Improvement Proposals (KIPs)": [
    "Even though we just introduced Kafka Streams and ksqlDB, there are still many more resources to help you continue your Kafka learning. The following sections look at a few of those resources."
  ],
  "Part 3  Going further<sec><sec><sec>Going further<sec><sec>Kafka projects you can explore": [
    "While it might not seem like the most exciting option, following Kafka Improvement Proposals (KIPs) is really one of the best ways to keep current with Kafka. Even though not everything that gets proposed is implemented, it is interesting to see what other users of Kafka think is worth exploring as use cases change over time.",
    "As we saw in chapter 5, KIP 392 (http://mng.bz/n2aV) was motivated by the need for users to fetch data when the partition leader was in a nonlocal data center. If Kafka existed only in on-premises data centers without separate data centers for disaster recovery, the proposal might not have gained acceptance. Reviewing these new KIPs allows everyone to understand the issues or features fellow Kafka users experience in their day-to-day life. KIPs are important enough to be addressed and discussed in key- notes such as the Kafka Summit 2019, where KIP 500 (http://mng.bz/8WvD) was pre- sented. This KIP deals with the replacement of ZooKeeper."
  ],
  "Part 3  Going further<sec><sec><sec>Going further<sec><sec>Community Slack channel": [
    "In addition to Kafka source code, searching GitHub or GitLab public repositories for real-world uses of Kafka can help you learn from those projects. Although not all code is equal in quality, hopefully, the previous chapters have given you enough informa- tion to understand how the required pieces fall into place. This book pointed out a couple of projects that use Kafka in some part to help power software, and these have made their source code publicly viewable on GitHub. One example was Apache Flume (https://github.com/apache/flume)."
  ]
}